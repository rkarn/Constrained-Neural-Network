{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# let's keep our keras backend tensorflow quiet\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "# for testing on CPU\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "# keras imports for the dataset and building our neural network\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape', (60000, 28, 28))\n",
      "('y_train shape', (60000,))\n",
      "('X_test shape', (10000, 28, 28))\n",
      "('y_test shape', (10000,))\n",
      "('Train matrix shape', (60000, 784))\n",
      "('Test matrix shape', (10000, 784))\n"
     ]
    }
   ],
   "source": [
    "# let's print the shape before we reshape and normalize\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n",
    "\n",
    "# building the input vector from the 28x28 pixels\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# print the final input shape ready for training\n",
    "print(\"Train matrix shape\", X_train.shape)\n",
    "print(\"Test matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape before one-hot encoding: ', (60000,))\n",
      "('Shape after one-hot encoding: ', (60000, 10))\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for neuron with input 2\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 2.2759 - acc: 0.0984 - val_loss: 2.1994 - val_acc: 0.0980\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 2.1231 - acc: 0.0987 - val_loss: 2.0739 - val_acc: 0.0980\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 2.0604 - acc: 0.0987 - val_loss: 2.0484 - val_acc: 0.0980\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 2.0438 - acc: 0.0987 - val_loss: 2.0381 - val_acc: 0.0980\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 2.0357 - acc: 0.0987 - val_loss: 2.0318 - val_acc: 0.0980\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 2.0302 - acc: 0.0987 - val_loss: 2.0270 - val_acc: 0.0980\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 2.0264 - acc: 0.0987 - val_loss: 2.0237 - val_acc: 0.0980\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 2.0232 - acc: 0.0987 - val_loss: 2.0216 - val_acc: 0.0980\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 2.0208 - acc: 0.0987 - val_loss: 2.0195 - val_acc: 0.0980\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 2.0188 - acc: 0.0987 - val_loss: 2.0178 - val_acc: 0.0980\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 2.0170 - acc: 0.0987 - val_loss: 2.0157 - val_acc: 0.0980\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 2.0153 - acc: 0.0987 - val_loss: 2.0140 - val_acc: 0.0980\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 2.0137 - acc: 0.0987 - val_loss: 2.0131 - val_acc: 0.0980\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 2.0122 - acc: 0.0987 - val_loss: 2.0117 - val_acc: 0.0980\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 2.0108 - acc: 0.0987 - val_loss: 2.0109 - val_acc: 0.0980\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 2.0095 - acc: 0.0987 - val_loss: 2.0097 - val_acc: 0.0980\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 2.0079 - acc: 0.0987 - val_loss: 2.0081 - val_acc: 0.0980\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 2.0065 - acc: 0.0987 - val_loss: 2.0071 - val_acc: 0.0980\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 2.0051 - acc: 0.0987 - val_loss: 2.0056 - val_acc: 0.0980\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 2.0035 - acc: 0.0987 - val_loss: 2.0046 - val_acc: 0.0980\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 2.0020 - acc: 0.0987 - val_loss: 2.0035 - val_acc: 0.0980\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 2.0005 - acc: 0.0987 - val_loss: 2.0023 - val_acc: 0.0980\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 1.9991 - acc: 0.0987 - val_loss: 2.0012 - val_acc: 0.0980\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 1.9980 - acc: 0.0987 - val_loss: 2.0008 - val_acc: 0.0980\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 1.9968 - acc: 0.0987 - val_loss: 1.9996 - val_acc: 0.0980\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9957 - acc: 0.0987 - val_loss: 1.9993 - val_acc: 0.0980\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 1.9947 - acc: 0.0987 - val_loss: 1.9985 - val_acc: 0.0980\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9939 - acc: 0.0987 - val_loss: 1.9985 - val_acc: 0.0980\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9931 - acc: 0.0987 - val_loss: 1.9977 - val_acc: 0.0980\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9923 - acc: 0.0987 - val_loss: 1.9968 - val_acc: 0.0980\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9917 - acc: 0.0987 - val_loss: 1.9967 - val_acc: 0.0980\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9909 - acc: 0.0987 - val_loss: 1.9959 - val_acc: 0.0980\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9903 - acc: 0.0987 - val_loss: 1.9964 - val_acc: 0.0980\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9896 - acc: 0.0987 - val_loss: 1.9968 - val_acc: 0.0980\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9891 - acc: 0.0987 - val_loss: 1.9955 - val_acc: 0.0980\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 1.9885 - acc: 0.0987 - val_loss: 1.9958 - val_acc: 0.0980\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9880 - acc: 0.0987 - val_loss: 1.9954 - val_acc: 0.0980\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 1.9874 - acc: 0.0987 - val_loss: 1.9953 - val_acc: 0.0980\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9867 - acc: 0.0987 - val_loss: 1.9949 - val_acc: 0.0980\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9862 - acc: 0.0987 - val_loss: 1.9946 - val_acc: 0.0980\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9857 - acc: 0.0987 - val_loss: 1.9945 - val_acc: 0.0980\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9853 - acc: 0.0987 - val_loss: 1.9943 - val_acc: 0.0980\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9847 - acc: 0.0987 - val_loss: 1.9937 - val_acc: 0.0980\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9841 - acc: 0.0987 - val_loss: 1.9939 - val_acc: 0.0980\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9836 - acc: 0.0987 - val_loss: 1.9930 - val_acc: 0.0980\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9831 - acc: 0.0987 - val_loss: 1.9924 - val_acc: 0.0980\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9826 - acc: 0.0987 - val_loss: 1.9933 - val_acc: 0.0980\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9822 - acc: 0.0987 - val_loss: 1.9922 - val_acc: 0.0980\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.9817 - acc: 0.0987 - val_loss: 1.9919 - val_acc: 0.0980\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 1.9811 - acc: 0.0987 - val_loss: 1.9915 - val_acc: 0.0980\n",
      "Training for neuron with input 3\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 2.2626 - acc: 0.0991 - val_loss: 2.1552 - val_acc: 0.1020\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 2.0831 - acc: 0.0993 - val_loss: 2.0411 - val_acc: 0.0980\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 2.0073 - acc: 0.1205 - val_loss: 1.9496 - val_acc: 0.1946\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 1.8643 - acc: 0.2331 - val_loss: 1.7918 - val_acc: 0.2893\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 1.7548 - acc: 0.3124 - val_loss: 1.7149 - val_acc: 0.3341\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 1.6921 - acc: 0.3418 - val_loss: 1.6611 - val_acc: 0.3530\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 1.6433 - acc: 0.3579 - val_loss: 1.6121 - val_acc: 0.3621\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 1.5958 - acc: 0.3668 - val_loss: 1.5618 - val_acc: 0.3671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 1.5348 - acc: 0.3819 - val_loss: 1.4850 - val_acc: 0.4242\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 1.4322 - acc: 0.4324 - val_loss: 1.3548 - val_acc: 0.4457\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 1.2985 - acc: 0.5114 - val_loss: 1.2344 - val_acc: 0.5742\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 1.1815 - acc: 0.5952 - val_loss: 1.1324 - val_acc: 0.6030\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 1.0823 - acc: 0.6107 - val_loss: 1.0362 - val_acc: 0.6086\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 1.0134 - acc: 0.6254 - val_loss: 0.9846 - val_acc: 0.6345\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.9749 - acc: 0.6599 - val_loss: 0.9513 - val_acc: 0.6996\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.9430 - acc: 0.7035 - val_loss: 0.9137 - val_acc: 0.7152\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.9157 - acc: 0.7169 - val_loss: 0.8934 - val_acc: 0.7248\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8957 - acc: 0.7248 - val_loss: 0.8723 - val_acc: 0.7361\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8811 - acc: 0.7300 - val_loss: 0.8629 - val_acc: 0.7390\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8681 - acc: 0.7354 - val_loss: 0.8500 - val_acc: 0.7443\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.8579 - acc: 0.7409 - val_loss: 0.8444 - val_acc: 0.7462\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8490 - acc: 0.7437 - val_loss: 0.8334 - val_acc: 0.7519\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8417 - acc: 0.7455 - val_loss: 0.8262 - val_acc: 0.7572\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8339 - acc: 0.7500 - val_loss: 0.8199 - val_acc: 0.7564\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8274 - acc: 0.7523 - val_loss: 0.8199 - val_acc: 0.7599\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8217 - acc: 0.7551 - val_loss: 0.8153 - val_acc: 0.7658\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8160 - acc: 0.7582 - val_loss: 0.8063 - val_acc: 0.7659\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8112 - acc: 0.7600 - val_loss: 0.8002 - val_acc: 0.7687\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8065 - acc: 0.7631 - val_loss: 0.7977 - val_acc: 0.7724\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8018 - acc: 0.7649 - val_loss: 0.7940 - val_acc: 0.7743\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7979 - acc: 0.7667 - val_loss: 0.7912 - val_acc: 0.7749\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7940 - acc: 0.7678 - val_loss: 0.7866 - val_acc: 0.7749\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7911 - acc: 0.7681 - val_loss: 0.7887 - val_acc: 0.7757\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7878 - acc: 0.7704 - val_loss: 0.7806 - val_acc: 0.7747\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7844 - acc: 0.7719 - val_loss: 0.7804 - val_acc: 0.7772\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7816 - acc: 0.7722 - val_loss: 0.7767 - val_acc: 0.7804\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7790 - acc: 0.7740 - val_loss: 0.7762 - val_acc: 0.7791\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7767 - acc: 0.7739 - val_loss: 0.7750 - val_acc: 0.7811\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7747 - acc: 0.7743 - val_loss: 0.7715 - val_acc: 0.7800\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7723 - acc: 0.7752 - val_loss: 0.7735 - val_acc: 0.7791\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7697 - acc: 0.7759 - val_loss: 0.7741 - val_acc: 0.7799\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7681 - acc: 0.7762 - val_loss: 0.7662 - val_acc: 0.7801\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7655 - acc: 0.7760 - val_loss: 0.7654 - val_acc: 0.7835\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7639 - acc: 0.7768 - val_loss: 0.7658 - val_acc: 0.7820\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.7620 - acc: 0.7771 - val_loss: 0.7631 - val_acc: 0.7826\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7606 - acc: 0.7770 - val_loss: 0.7611 - val_acc: 0.7856\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7585 - acc: 0.7780 - val_loss: 0.7602 - val_acc: 0.7847\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7569 - acc: 0.7781 - val_loss: 0.7580 - val_acc: 0.7869\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.7557 - acc: 0.7789 - val_loss: 0.7591 - val_acc: 0.7874\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7542 - acc: 0.7791 - val_loss: 0.7599 - val_acc: 0.7844\n",
      "Training for neuron with input 4\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 2.2029 - acc: 0.1762 - val_loss: 1.9401 - val_acc: 0.2323\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 1.7579 - acc: 0.2626 - val_loss: 1.6357 - val_acc: 0.3343\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 1.5633 - acc: 0.3703 - val_loss: 1.4848 - val_acc: 0.4160\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 1.3685 - acc: 0.4783 - val_loss: 1.2185 - val_acc: 0.5197\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 1.0874 - acc: 0.6435 - val_loss: 0.9705 - val_acc: 0.6866\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.8981 - acc: 0.7168 - val_loss: 0.8274 - val_acc: 0.7437\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.7961 - acc: 0.7535 - val_loss: 0.7516 - val_acc: 0.7665\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.7409 - acc: 0.7720 - val_loss: 0.7105 - val_acc: 0.7812\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.7066 - acc: 0.7835 - val_loss: 0.6809 - val_acc: 0.7936\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.6808 - acc: 0.7941 - val_loss: 0.6611 - val_acc: 0.8005\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.6604 - acc: 0.8012 - val_loss: 0.6415 - val_acc: 0.8071\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.6440 - acc: 0.8085 - val_loss: 0.6289 - val_acc: 0.8094\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.6304 - acc: 0.8137 - val_loss: 0.6172 - val_acc: 0.8142\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.6190 - acc: 0.8175 - val_loss: 0.6092 - val_acc: 0.8206\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.6097 - acc: 0.8212 - val_loss: 0.5991 - val_acc: 0.8221\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.6013 - acc: 0.8253 - val_loss: 0.5936 - val_acc: 0.8248\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.5941 - acc: 0.8276 - val_loss: 0.5861 - val_acc: 0.8297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5874 - acc: 0.8306 - val_loss: 0.5846 - val_acc: 0.8292\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5824 - acc: 0.8322 - val_loss: 0.5786 - val_acc: 0.8327\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5780 - acc: 0.8335 - val_loss: 0.5747 - val_acc: 0.8314\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5735 - acc: 0.8353 - val_loss: 0.5712 - val_acc: 0.8337\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5698 - acc: 0.8369 - val_loss: 0.5669 - val_acc: 0.8359\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5661 - acc: 0.8387 - val_loss: 0.5641 - val_acc: 0.8384\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5630 - acc: 0.8393 - val_loss: 0.5636 - val_acc: 0.8377\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5594 - acc: 0.8409 - val_loss: 0.5618 - val_acc: 0.8375\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5572 - acc: 0.8421 - val_loss: 0.5569 - val_acc: 0.8404\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5546 - acc: 0.8432 - val_loss: 0.5538 - val_acc: 0.8406\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5524 - acc: 0.8435 - val_loss: 0.5554 - val_acc: 0.8391\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5499 - acc: 0.8447 - val_loss: 0.5516 - val_acc: 0.8419\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5485 - acc: 0.8446 - val_loss: 0.5508 - val_acc: 0.8426\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5465 - acc: 0.8460 - val_loss: 0.5471 - val_acc: 0.8440\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5439 - acc: 0.8469 - val_loss: 0.5470 - val_acc: 0.8443\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5428 - acc: 0.8464 - val_loss: 0.5452 - val_acc: 0.8467\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5410 - acc: 0.8479 - val_loss: 0.5434 - val_acc: 0.8480\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5392 - acc: 0.8484 - val_loss: 0.5501 - val_acc: 0.8451\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5379 - acc: 0.8492 - val_loss: 0.5423 - val_acc: 0.8467\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5370 - acc: 0.8493 - val_loss: 0.5418 - val_acc: 0.8457\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5354 - acc: 0.8501 - val_loss: 0.5397 - val_acc: 0.8480\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5341 - acc: 0.8501 - val_loss: 0.5388 - val_acc: 0.8498\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5325 - acc: 0.8512 - val_loss: 0.5387 - val_acc: 0.8493\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5312 - acc: 0.8511 - val_loss: 0.5359 - val_acc: 0.8493\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5306 - acc: 0.8507 - val_loss: 0.5371 - val_acc: 0.8498\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5293 - acc: 0.8510 - val_loss: 0.5379 - val_acc: 0.8501\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5286 - acc: 0.8525 - val_loss: 0.5345 - val_acc: 0.8531\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5269 - acc: 0.8522 - val_loss: 0.5343 - val_acc: 0.8537\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5260 - acc: 0.8535 - val_loss: 0.5343 - val_acc: 0.8525\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5253 - acc: 0.8527 - val_loss: 0.5323 - val_acc: 0.8516\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.5234 - acc: 0.8536 - val_loss: 0.5300 - val_acc: 0.8543\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.5227 - acc: 0.8539 - val_loss: 0.5326 - val_acc: 0.8533\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.5220 - acc: 0.8544 - val_loss: 0.5310 - val_acc: 0.8547\n",
      "Training for neuron with input 5\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 2.1909 - acc: 0.1867 - val_loss: 1.9195 - val_acc: 0.2423\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.7564 - acc: 0.2718 - val_loss: 1.6456 - val_acc: 0.2955\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.5705 - acc: 0.3386 - val_loss: 1.4836 - val_acc: 0.3886\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.3105 - acc: 0.5192 - val_loss: 1.1430 - val_acc: 0.5945\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 1.0399 - acc: 0.6335 - val_loss: 0.9444 - val_acc: 0.6834\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.8760 - acc: 0.7059 - val_loss: 0.8062 - val_acc: 0.7316\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.7770 - acc: 0.7394 - val_loss: 0.7397 - val_acc: 0.7548\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.7278 - acc: 0.7596 - val_loss: 0.7069 - val_acc: 0.7685\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.7007 - acc: 0.7713 - val_loss: 0.6840 - val_acc: 0.7809\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6809 - acc: 0.7793 - val_loss: 0.6687 - val_acc: 0.7859\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6656 - acc: 0.7867 - val_loss: 0.6540 - val_acc: 0.7951\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6527 - acc: 0.7929 - val_loss: 0.6435 - val_acc: 0.8000\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6426 - acc: 0.7982 - val_loss: 0.6324 - val_acc: 0.8073\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6318 - acc: 0.8044 - val_loss: 0.6267 - val_acc: 0.8105\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6222 - acc: 0.8079 - val_loss: 0.6163 - val_acc: 0.8149\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.6133 - acc: 0.8117 - val_loss: 0.6101 - val_acc: 0.8141\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.6049 - acc: 0.8161 - val_loss: 0.5989 - val_acc: 0.8193\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.5958 - acc: 0.8195 - val_loss: 0.5891 - val_acc: 0.8242\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.5872 - acc: 0.8216 - val_loss: 0.5806 - val_acc: 0.8269\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.5785 - acc: 0.8254 - val_loss: 0.5704 - val_acc: 0.8317\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.5698 - acc: 0.8285 - val_loss: 0.5640 - val_acc: 0.8359\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.5606 - acc: 0.8322 - val_loss: 0.5531 - val_acc: 0.8394\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.5517 - acc: 0.8365 - val_loss: 0.5458 - val_acc: 0.8425\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.5431 - acc: 0.8394 - val_loss: 0.5383 - val_acc: 0.8474\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.5338 - acc: 0.8433 - val_loss: 0.5279 - val_acc: 0.8503\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.5251 - acc: 0.8460 - val_loss: 0.5214 - val_acc: 0.8518\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.5177 - acc: 0.8490 - val_loss: 0.5131 - val_acc: 0.8550\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5087 - acc: 0.8515 - val_loss: 0.5087 - val_acc: 0.8543\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.5015 - acc: 0.8540 - val_loss: 0.5024 - val_acc: 0.8560\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4951 - acc: 0.8565 - val_loss: 0.5010 - val_acc: 0.8571\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4885 - acc: 0.8586 - val_loss: 0.4927 - val_acc: 0.8602\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4830 - acc: 0.8602 - val_loss: 0.4874 - val_acc: 0.8620\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4782 - acc: 0.8628 - val_loss: 0.4862 - val_acc: 0.8619\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4728 - acc: 0.8631 - val_loss: 0.4829 - val_acc: 0.8644\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.4684 - acc: 0.8648 - val_loss: 0.4779 - val_acc: 0.8647\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4648 - acc: 0.8659 - val_loss: 0.4783 - val_acc: 0.8652\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4612 - acc: 0.8681 - val_loss: 0.4748 - val_acc: 0.8660\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4581 - acc: 0.8682 - val_loss: 0.4732 - val_acc: 0.8664\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4552 - acc: 0.8701 - val_loss: 0.4684 - val_acc: 0.8683\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4523 - acc: 0.8706 - val_loss: 0.4657 - val_acc: 0.8690\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4500 - acc: 0.8714 - val_loss: 0.4664 - val_acc: 0.8676\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4475 - acc: 0.8726 - val_loss: 0.4652 - val_acc: 0.8681\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4453 - acc: 0.8731 - val_loss: 0.4636 - val_acc: 0.8691\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4436 - acc: 0.8741 - val_loss: 0.4605 - val_acc: 0.8712\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4422 - acc: 0.8745 - val_loss: 0.4578 - val_acc: 0.8719\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4406 - acc: 0.8751 - val_loss: 0.4608 - val_acc: 0.8704\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4390 - acc: 0.8760 - val_loss: 0.4578 - val_acc: 0.8705\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4376 - acc: 0.8764 - val_loss: 0.4572 - val_acc: 0.8709\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4360 - acc: 0.8769 - val_loss: 0.4541 - val_acc: 0.8726\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4347 - acc: 0.8776 - val_loss: 0.4543 - val_acc: 0.8731\n",
      "Training for neuron with input 6\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 2.1950 - acc: 0.1887 - val_loss: 1.8619 - val_acc: 0.2906\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 1.4713 - acc: 0.4753 - val_loss: 1.1641 - val_acc: 0.5653\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 1.0434 - acc: 0.6547 - val_loss: 0.9284 - val_acc: 0.7062\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8790 - acc: 0.7183 - val_loss: 0.8174 - val_acc: 0.7358\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.7868 - acc: 0.7502 - val_loss: 0.7447 - val_acc: 0.7679\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.7177 - acc: 0.7773 - val_loss: 0.6823 - val_acc: 0.7938\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.6614 - acc: 0.7991 - val_loss: 0.6276 - val_acc: 0.8123\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.6099 - acc: 0.8181 - val_loss: 0.5719 - val_acc: 0.8318\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5565 - acc: 0.8394 - val_loss: 0.5172 - val_acc: 0.8516\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5126 - acc: 0.8539 - val_loss: 0.4791 - val_acc: 0.8636\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4814 - acc: 0.8635 - val_loss: 0.4532 - val_acc: 0.8720\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4582 - acc: 0.8710 - val_loss: 0.4330 - val_acc: 0.8770\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4409 - acc: 0.8752 - val_loss: 0.4180 - val_acc: 0.8829\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4259 - acc: 0.8795 - val_loss: 0.4114 - val_acc: 0.8823\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4141 - acc: 0.8824 - val_loss: 0.3977 - val_acc: 0.8872\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.4030 - acc: 0.8855 - val_loss: 0.3897 - val_acc: 0.8896\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3932 - acc: 0.8882 - val_loss: 0.3778 - val_acc: 0.8916\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3816 - acc: 0.8919 - val_loss: 0.3700 - val_acc: 0.8976\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3731 - acc: 0.8941 - val_loss: 0.3629 - val_acc: 0.8972\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3645 - acc: 0.8965 - val_loss: 0.3578 - val_acc: 0.9022\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3576 - acc: 0.8987 - val_loss: 0.3494 - val_acc: 0.9009\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3510 - acc: 0.9004 - val_loss: 0.3448 - val_acc: 0.9018\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3447 - acc: 0.9026 - val_loss: 0.3397 - val_acc: 0.9033\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3408 - acc: 0.9041 - val_loss: 0.3359 - val_acc: 0.9044\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3354 - acc: 0.9051 - val_loss: 0.3330 - val_acc: 0.9048\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3319 - acc: 0.9064 - val_loss: 0.3310 - val_acc: 0.9039\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3288 - acc: 0.9068 - val_loss: 0.3304 - val_acc: 0.9043\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3258 - acc: 0.9076 - val_loss: 0.3269 - val_acc: 0.9069\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3233 - acc: 0.9084 - val_loss: 0.3241 - val_acc: 0.9073\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3209 - acc: 0.9090 - val_loss: 0.3224 - val_acc: 0.9079\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3188 - acc: 0.9096 - val_loss: 0.3222 - val_acc: 0.9077\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3166 - acc: 0.9106 - val_loss: 0.3203 - val_acc: 0.9086\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3149 - acc: 0.9109 - val_loss: 0.3190 - val_acc: 0.9107\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.3134 - acc: 0.9114 - val_loss: 0.3176 - val_acc: 0.9092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3117 - acc: 0.9120 - val_loss: 0.3207 - val_acc: 0.9087\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3109 - acc: 0.9123 - val_loss: 0.3164 - val_acc: 0.9109\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3090 - acc: 0.9122 - val_loss: 0.3168 - val_acc: 0.9104\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3081 - acc: 0.9131 - val_loss: 0.3154 - val_acc: 0.9081\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3069 - acc: 0.9134 - val_loss: 0.3176 - val_acc: 0.9104\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3058 - acc: 0.9143 - val_loss: 0.3134 - val_acc: 0.9104\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3043 - acc: 0.9143 - val_loss: 0.3146 - val_acc: 0.9103\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3039 - acc: 0.9143 - val_loss: 0.3134 - val_acc: 0.9112\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3025 - acc: 0.9149 - val_loss: 0.3130 - val_acc: 0.9122\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3016 - acc: 0.9151 - val_loss: 0.3154 - val_acc: 0.9090\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3007 - acc: 0.9157 - val_loss: 0.3135 - val_acc: 0.9098\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3004 - acc: 0.9158 - val_loss: 0.3123 - val_acc: 0.9121\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2990 - acc: 0.9159 - val_loss: 0.3120 - val_acc: 0.9115\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2984 - acc: 0.9166 - val_loss: 0.3134 - val_acc: 0.9105\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2978 - acc: 0.9163 - val_loss: 0.3113 - val_acc: 0.9125\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2969 - acc: 0.9167 - val_loss: 0.3110 - val_acc: 0.9114\n",
      "Training for neuron with input 7\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 2.1541 - acc: 0.1217 - val_loss: 1.9252 - val_acc: 0.1985\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 1.7214 - acc: 0.3089 - val_loss: 1.5322 - val_acc: 0.3939\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 1.2891 - acc: 0.5248 - val_loss: 0.9760 - val_acc: 0.6619\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.8125 - acc: 0.7373 - val_loss: 0.6574 - val_acc: 0.8028\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.6087 - acc: 0.8176 - val_loss: 0.5422 - val_acc: 0.8397\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.5257 - acc: 0.8480 - val_loss: 0.4883 - val_acc: 0.8581\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4832 - acc: 0.8620 - val_loss: 0.4579 - val_acc: 0.8668\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4566 - acc: 0.8703 - val_loss: 0.4372 - val_acc: 0.8721\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4380 - acc: 0.8752 - val_loss: 0.4227 - val_acc: 0.8772\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4236 - acc: 0.8805 - val_loss: 0.4101 - val_acc: 0.8814\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4129 - acc: 0.8824 - val_loss: 0.4025 - val_acc: 0.8828\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4031 - acc: 0.8852 - val_loss: 0.3952 - val_acc: 0.8852\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3945 - acc: 0.8888 - val_loss: 0.3873 - val_acc: 0.8868\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3873 - acc: 0.8900 - val_loss: 0.3827 - val_acc: 0.8895\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3813 - acc: 0.8919 - val_loss: 0.3775 - val_acc: 0.8906\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3755 - acc: 0.8933 - val_loss: 0.3735 - val_acc: 0.8900\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3704 - acc: 0.8946 - val_loss: 0.3683 - val_acc: 0.8932\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3657 - acc: 0.8952 - val_loss: 0.3647 - val_acc: 0.8938\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.3610 - acc: 0.8970 - val_loss: 0.3609 - val_acc: 0.8939\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.3565 - acc: 0.8979 - val_loss: 0.3566 - val_acc: 0.8972\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3521 - acc: 0.8994 - val_loss: 0.3571 - val_acc: 0.8961\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3481 - acc: 0.9005 - val_loss: 0.3494 - val_acc: 0.8992\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3440 - acc: 0.9018 - val_loss: 0.3452 - val_acc: 0.8986\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3407 - acc: 0.9028 - val_loss: 0.3455 - val_acc: 0.8975\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3359 - acc: 0.9046 - val_loss: 0.3394 - val_acc: 0.8994\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3319 - acc: 0.9052 - val_loss: 0.3384 - val_acc: 0.9001\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3287 - acc: 0.9063 - val_loss: 0.3334 - val_acc: 0.9034\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3249 - acc: 0.9077 - val_loss: 0.3339 - val_acc: 0.9037\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3215 - acc: 0.9085 - val_loss: 0.3285 - val_acc: 0.9047\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3178 - acc: 0.9097 - val_loss: 0.3280 - val_acc: 0.9040\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.3150 - acc: 0.9103 - val_loss: 0.3261 - val_acc: 0.9037\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.3121 - acc: 0.9115 - val_loss: 0.3219 - val_acc: 0.9068\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3087 - acc: 0.9128 - val_loss: 0.3196 - val_acc: 0.9083\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3062 - acc: 0.9136 - val_loss: 0.3162 - val_acc: 0.9084\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3043 - acc: 0.9143 - val_loss: 0.3143 - val_acc: 0.9094\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.3015 - acc: 0.9152 - val_loss: 0.3142 - val_acc: 0.9100\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2986 - acc: 0.9159 - val_loss: 0.3118 - val_acc: 0.9109\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.2964 - acc: 0.9172 - val_loss: 0.3086 - val_acc: 0.9115\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.2936 - acc: 0.9181 - val_loss: 0.3091 - val_acc: 0.9110\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2910 - acc: 0.9185 - val_loss: 0.3036 - val_acc: 0.9138\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2885 - acc: 0.9197 - val_loss: 0.3022 - val_acc: 0.9147\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2862 - acc: 0.9203 - val_loss: 0.3040 - val_acc: 0.9130\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.2833 - acc: 0.9208 - val_loss: 0.2985 - val_acc: 0.9158\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.2814 - acc: 0.9215 - val_loss: 0.2963 - val_acc: 0.9152\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2787 - acc: 0.9219 - val_loss: 0.2930 - val_acc: 0.9166\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2769 - acc: 0.9228 - val_loss: 0.2915 - val_acc: 0.9157\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2747 - acc: 0.9235 - val_loss: 0.2941 - val_acc: 0.9163\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.2722 - acc: 0.9235 - val_loss: 0.2868 - val_acc: 0.9183\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2698 - acc: 0.9248 - val_loss: 0.2904 - val_acc: 0.9178\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.2677 - acc: 0.9250 - val_loss: 0.2853 - val_acc: 0.9190\n",
      "Training for neuron with input 8\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 2.1126 - acc: 0.2247 - val_loss: 1.6928 - val_acc: 0.3319\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 1.4001 - acc: 0.4747 - val_loss: 1.1703 - val_acc: 0.6160\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.9848 - acc: 0.6656 - val_loss: 0.8086 - val_acc: 0.7223\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.7239 - acc: 0.7612 - val_loss: 0.6334 - val_acc: 0.8019\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.6044 - acc: 0.8092 - val_loss: 0.5506 - val_acc: 0.8297\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.5311 - acc: 0.8372 - val_loss: 0.4899 - val_acc: 0.8534\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.4722 - acc: 0.8586 - val_loss: 0.4410 - val_acc: 0.8706\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.4275 - acc: 0.8759 - val_loss: 0.4057 - val_acc: 0.8804\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.3979 - acc: 0.8845 - val_loss: 0.3843 - val_acc: 0.8871\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3781 - acc: 0.8911 - val_loss: 0.3667 - val_acc: 0.8932\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3632 - acc: 0.8954 - val_loss: 0.3599 - val_acc: 0.8943\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3532 - acc: 0.8976 - val_loss: 0.3485 - val_acc: 0.8982\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3441 - acc: 0.9008 - val_loss: 0.3406 - val_acc: 0.9015\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3367 - acc: 0.9036 - val_loss: 0.3378 - val_acc: 0.9033\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3302 - acc: 0.9050 - val_loss: 0.3332 - val_acc: 0.9046\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3247 - acc: 0.9064 - val_loss: 0.3253 - val_acc: 0.9069\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.3198 - acc: 0.9088 - val_loss: 0.3219 - val_acc: 0.9066\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.3159 - acc: 0.9096 - val_loss: 0.3221 - val_acc: 0.9079\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.3118 - acc: 0.9101 - val_loss: 0.3141 - val_acc: 0.9099\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3082 - acc: 0.9126 - val_loss: 0.3147 - val_acc: 0.9084\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3053 - acc: 0.9128 - val_loss: 0.3118 - val_acc: 0.9114\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3022 - acc: 0.9141 - val_loss: 0.3099 - val_acc: 0.9111\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2999 - acc: 0.9149 - val_loss: 0.3085 - val_acc: 0.9120\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.2974 - acc: 0.9154 - val_loss: 0.3071 - val_acc: 0.9119\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.2954 - acc: 0.9160 - val_loss: 0.3040 - val_acc: 0.9135\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2929 - acc: 0.9169 - val_loss: 0.3011 - val_acc: 0.9152\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2909 - acc: 0.9175 - val_loss: 0.3022 - val_acc: 0.9141\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.2887 - acc: 0.9185 - val_loss: 0.3051 - val_acc: 0.9127\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2870 - acc: 0.9189 - val_loss: 0.3015 - val_acc: 0.9153\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2848 - acc: 0.9197 - val_loss: 0.2976 - val_acc: 0.9176\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2839 - acc: 0.9198 - val_loss: 0.2995 - val_acc: 0.9175\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2827 - acc: 0.9196 - val_loss: 0.2969 - val_acc: 0.9168\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2805 - acc: 0.9204 - val_loss: 0.2973 - val_acc: 0.9170\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2796 - acc: 0.9213 - val_loss: 0.2948 - val_acc: 0.9175\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2779 - acc: 0.9217 - val_loss: 0.2965 - val_acc: 0.9187\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2771 - acc: 0.9222 - val_loss: 0.2947 - val_acc: 0.9199\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2756 - acc: 0.9224 - val_loss: 0.2950 - val_acc: 0.9196\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2748 - acc: 0.9229 - val_loss: 0.2938 - val_acc: 0.9205\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2739 - acc: 0.9229 - val_loss: 0.2948 - val_acc: 0.9196\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2717 - acc: 0.9238 - val_loss: 0.2948 - val_acc: 0.9195\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2712 - acc: 0.9236 - val_loss: 0.2927 - val_acc: 0.9206\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2702 - acc: 0.9243 - val_loss: 0.2929 - val_acc: 0.9203\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2692 - acc: 0.9239 - val_loss: 0.2925 - val_acc: 0.9204\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.2681 - acc: 0.9242 - val_loss: 0.2904 - val_acc: 0.9200\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2672 - acc: 0.9246 - val_loss: 0.2919 - val_acc: 0.9207\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.2657 - acc: 0.9254 - val_loss: 0.2903 - val_acc: 0.9215\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2649 - acc: 0.9255 - val_loss: 0.2893 - val_acc: 0.9221\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2640 - acc: 0.9256 - val_loss: 0.2897 - val_acc: 0.9226\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2641 - acc: 0.9260 - val_loss: 0.2897 - val_acc: 0.9216\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.2625 - acc: 0.9265 - val_loss: 0.2885 - val_acc: 0.9220\n",
      "Training for neuron with input 9\n",
      "Train on 60000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 2.0817 - acc: 0.2533 - val_loss: 1.6540 - val_acc: 0.3540\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 1.3055 - acc: 0.5406 - val_loss: 0.9253 - val_acc: 0.7407\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.7371 - acc: 0.7778 - val_loss: 0.5850 - val_acc: 0.8233\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.5380 - acc: 0.8379 - val_loss: 0.4807 - val_acc: 0.8556\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.4644 - acc: 0.8635 - val_loss: 0.4336 - val_acc: 0.8715\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4270 - acc: 0.8759 - val_loss: 0.4077 - val_acc: 0.8800\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.4026 - acc: 0.8840 - val_loss: 0.3905 - val_acc: 0.8831\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3855 - acc: 0.8892 - val_loss: 0.3747 - val_acc: 0.8880\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3705 - acc: 0.8935 - val_loss: 0.3650 - val_acc: 0.8911\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3592 - acc: 0.8970 - val_loss: 0.3612 - val_acc: 0.8910\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3493 - acc: 0.9002 - val_loss: 0.3456 - val_acc: 0.8976\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3398 - acc: 0.9029 - val_loss: 0.3419 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3325 - acc: 0.9053 - val_loss: 0.3337 - val_acc: 0.9034\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.3246 - acc: 0.9082 - val_loss: 0.3242 - val_acc: 0.9057\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3183 - acc: 0.9100 - val_loss: 0.3211 - val_acc: 0.9081\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3122 - acc: 0.9113 - val_loss: 0.3141 - val_acc: 0.9088\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3066 - acc: 0.9131 - val_loss: 0.3101 - val_acc: 0.9101\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.3008 - acc: 0.9150 - val_loss: 0.3015 - val_acc: 0.9118\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2954 - acc: 0.9166 - val_loss: 0.2978 - val_acc: 0.9129\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2904 - acc: 0.9180 - val_loss: 0.2950 - val_acc: 0.9156\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2858 - acc: 0.9202 - val_loss: 0.2904 - val_acc: 0.9168\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2817 - acc: 0.9208 - val_loss: 0.2852 - val_acc: 0.9190\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2774 - acc: 0.9222 - val_loss: 0.2801 - val_acc: 0.9210\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2721 - acc: 0.9234 - val_loss: 0.2773 - val_acc: 0.9221\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2685 - acc: 0.9247 - val_loss: 0.2741 - val_acc: 0.9236\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2644 - acc: 0.9255 - val_loss: 0.2690 - val_acc: 0.9245\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2599 - acc: 0.9271 - val_loss: 0.2666 - val_acc: 0.9258\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2563 - acc: 0.9281 - val_loss: 0.2635 - val_acc: 0.9273\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2533 - acc: 0.9280 - val_loss: 0.2599 - val_acc: 0.9273\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2505 - acc: 0.9292 - val_loss: 0.2570 - val_acc: 0.9276\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2477 - acc: 0.9299 - val_loss: 0.2547 - val_acc: 0.9284\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2449 - acc: 0.9308 - val_loss: 0.2541 - val_acc: 0.9283\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2415 - acc: 0.9312 - val_loss: 0.2488 - val_acc: 0.9302\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2402 - acc: 0.9319 - val_loss: 0.2482 - val_acc: 0.9301\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2380 - acc: 0.9329 - val_loss: 0.2461 - val_acc: 0.9309\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2351 - acc: 0.9329 - val_loss: 0.2453 - val_acc: 0.9307\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2335 - acc: 0.9337 - val_loss: 0.2455 - val_acc: 0.9305\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2305 - acc: 0.9343 - val_loss: 0.2407 - val_acc: 0.9325\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2292 - acc: 0.9348 - val_loss: 0.2405 - val_acc: 0.9318\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2271 - acc: 0.9351 - val_loss: 0.2382 - val_acc: 0.9336\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2256 - acc: 0.9357 - val_loss: 0.2374 - val_acc: 0.9337\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2238 - acc: 0.9362 - val_loss: 0.2358 - val_acc: 0.9333\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2221 - acc: 0.9364 - val_loss: 0.2340 - val_acc: 0.9341\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2211 - acc: 0.9367 - val_loss: 0.2353 - val_acc: 0.9339\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2196 - acc: 0.9365 - val_loss: 0.2320 - val_acc: 0.9352\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2182 - acc: 0.9376 - val_loss: 0.2314 - val_acc: 0.9347\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2170 - acc: 0.9382 - val_loss: 0.2321 - val_acc: 0.9345\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2159 - acc: 0.9383 - val_loss: 0.2299 - val_acc: 0.9354\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2143 - acc: 0.9382 - val_loss: 0.2320 - val_acc: 0.9340\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2133 - acc: 0.9393 - val_loss: 0.2299 - val_acc: 0.9347\n"
     ]
    }
   ],
   "source": [
    "#Assigning the sparsity value to different layers. Sparsity: 1st hidden layer > 2nd hidden layer > output layer\n",
    "\n",
    "model_losses = []\n",
    "model_accs = []\n",
    "layerwise_sparsity = []\n",
    "model_sparsity = []\n",
    "num_neuron_ips = range(2,10, 1)\n",
    "for neuron_ip in num_neuron_ips:\n",
    "    print('Training for neuron with input {}'.format(neuron_ip))\n",
    "    class LossHistory(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "            self.batches = []\n",
    "            self.weight_save = []\n",
    "            self.model_weights = model.get_weights()\n",
    "            self.weight_masks = []\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i!=0 and i%2 == 0:\n",
    "                    self.random_mask = np.zeros(self.model_weights[i].shape)\n",
    "                    self.random_mask[np.random.randint(0, self.model_weights[i].shape[0], neuron_ip), :] = 1\n",
    "                    np.random.shuffle(self.random_mask)\n",
    "                    self.weight_masks.append(self.random_mask)\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i!=0 and i %2 == 0:\n",
    "                    self.model_weights[i] = np.multiply(self.model_weights[i], self.weight_masks[i/2 -1])\n",
    "                elif i!=0:\n",
    "                    self.model_weights[i] = np.zeros(self.model_weights[i].shape)\n",
    "            model.set_weights(self.model_weights)\n",
    "            self.weight_save.append(model.get_weights)     \n",
    "\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.batches.append(batch)\n",
    "            self.model_weights = model.get_weights()\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i!=0 and i %2 == 0:\n",
    "                    self.model_weights[i] = np.multiply(self.model_weights[i], self.weight_masks[i/2 -1])\n",
    "                elif i!=0:\n",
    "                    self.model_weights[i] = np.zeros(self.model_weights[i].shape)\n",
    "            model.set_weights(self.model_weights)\n",
    "            self.weight_save.append(model.get_weights) \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,),kernel_initializer='random_uniform', bias_initializer='zeros', activation='relu'))\n",
    "    model.add(Dense(256, kernel_initializer='random_uniform', bias_initializer='zeros', activation='relu'))\n",
    "    model.add(Dense(10, kernel_initializer='random_uniform', bias_initializer='zeros', activation='softmax'))\n",
    "\n",
    "    # compiling the sequential model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "    l_history=LossHistory()\n",
    "    history = model.fit(X_train, Y_train,\n",
    "              batch_size=600, epochs=50, verbose=1,\n",
    "              validation_data=(X_test, Y_test), callbacks = [es, l_history])\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, Y_test, verbose=2)\n",
    "    model_losses.append(loss_and_metrics[0])\n",
    "    model_accs.append(loss_and_metrics[1])\n",
    "    for i in range(len(model.get_weights())):\n",
    "        if i%2==0:\n",
    "            layerwise_sparsity.append(1 - float(np.count_nonzero(model.get_weights()[i]))/model.get_weights()[i].size)\n",
    "    model_sparsity.append(layerwise_sparsity)\n",
    "    layerwise_sparsity = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hazard. Layer 1, neuron 1 has connections 10.\n",
      "Hazard. Layer 1, neuron 2 has connections 10.\n",
      "Hazard. Layer 1, neuron 3 has connections 10.\n",
      "Hazard. Layer 1, neuron 4 has connections 10.\n",
      "Hazard. Layer 1, neuron 5 has connections 10.\n",
      "Hazard. Layer 1, neuron 6 has connections 10.\n",
      "Hazard. Layer 1, neuron 7 has connections 10.\n",
      "Hazard. Layer 1, neuron 8 has connections 10.\n",
      "Hazard. Layer 1, neuron 9 has connections 10.\n",
      "Hazard. Layer 1, neuron 10 has connections 10.\n",
      "Hazard. Layer 1, neuron 11 has connections 10.\n",
      "Hazard. Layer 1, neuron 12 has connections 10.\n",
      "Hazard. Layer 1, neuron 13 has connections 10.\n",
      "Hazard. Layer 1, neuron 14 has connections 10.\n",
      "Hazard. Layer 1, neuron 15 has connections 10.\n",
      "Hazard. Layer 1, neuron 16 has connections 10.\n",
      "Hazard. Layer 1, neuron 17 has connections 10.\n",
      "Hazard. Layer 1, neuron 18 has connections 10.\n",
      "Hazard. Layer 1, neuron 19 has connections 10.\n",
      "Hazard. Layer 1, neuron 20 has connections 10.\n",
      "Hazard. Layer 1, neuron 21 has connections 10.\n",
      "Hazard. Layer 1, neuron 22 has connections 10.\n",
      "Hazard. Layer 1, neuron 23 has connections 10.\n",
      "Hazard. Layer 1, neuron 24 has connections 10.\n",
      "Hazard. Layer 1, neuron 25 has connections 10.\n",
      "Hazard. Layer 1, neuron 26 has connections 10.\n",
      "Hazard. Layer 1, neuron 27 has connections 10.\n",
      "Hazard. Layer 1, neuron 28 has connections 10.\n",
      "Hazard. Layer 1, neuron 29 has connections 10.\n",
      "Hazard. Layer 1, neuron 30 has connections 10.\n",
      "Hazard. Layer 1, neuron 31 has connections 10.\n",
      "Hazard. Layer 1, neuron 32 has connections 10.\n",
      "Hazard. Layer 1, neuron 33 has connections 10.\n",
      "Hazard. Layer 1, neuron 34 has connections 10.\n",
      "Hazard. Layer 1, neuron 35 has connections 10.\n",
      "Hazard. Layer 1, neuron 36 has connections 10.\n",
      "Hazard. Layer 1, neuron 37 has connections 10.\n",
      "Hazard. Layer 1, neuron 38 has connections 10.\n",
      "Hazard. Layer 1, neuron 39 has connections 10.\n",
      "Hazard. Layer 1, neuron 40 has connections 10.\n",
      "Hazard. Layer 1, neuron 41 has connections 10.\n",
      "Hazard. Layer 1, neuron 42 has connections 10.\n",
      "Hazard. Layer 1, neuron 43 has connections 10.\n",
      "Hazard. Layer 1, neuron 44 has connections 10.\n",
      "Hazard. Layer 1, neuron 45 has connections 10.\n",
      "Hazard. Layer 1, neuron 46 has connections 10.\n",
      "Hazard. Layer 1, neuron 47 has connections 10.\n",
      "Hazard. Layer 1, neuron 48 has connections 10.\n",
      "Hazard. Layer 1, neuron 49 has connections 10.\n",
      "Hazard. Layer 1, neuron 50 has connections 10.\n",
      "Hazard. Layer 1, neuron 51 has connections 10.\n",
      "Hazard. Layer 1, neuron 52 has connections 10.\n",
      "Hazard. Layer 1, neuron 53 has connections 10.\n",
      "Hazard. Layer 1, neuron 54 has connections 10.\n",
      "Hazard. Layer 1, neuron 55 has connections 10.\n",
      "Hazard. Layer 1, neuron 56 has connections 10.\n",
      "Hazard. Layer 1, neuron 57 has connections 10.\n",
      "Hazard. Layer 1, neuron 58 has connections 10.\n",
      "Hazard. Layer 1, neuron 59 has connections 10.\n",
      "Hazard. Layer 1, neuron 60 has connections 10.\n",
      "Hazard. Layer 1, neuron 61 has connections 10.\n",
      "Hazard. Layer 1, neuron 62 has connections 10.\n",
      "Hazard. Layer 1, neuron 63 has connections 10.\n",
      "Hazard. Layer 1, neuron 64 has connections 10.\n",
      "Hazard. Layer 1, neuron 65 has connections 10.\n",
      "Hazard. Layer 1, neuron 66 has connections 10.\n",
      "Hazard. Layer 1, neuron 67 has connections 10.\n",
      "Hazard. Layer 1, neuron 68 has connections 10.\n",
      "Hazard. Layer 1, neuron 69 has connections 10.\n",
      "Hazard. Layer 1, neuron 70 has connections 10.\n",
      "Hazard. Layer 1, neuron 71 has connections 10.\n",
      "Hazard. Layer 1, neuron 72 has connections 10.\n",
      "Hazard. Layer 1, neuron 73 has connections 10.\n",
      "Hazard. Layer 1, neuron 74 has connections 10.\n",
      "Hazard. Layer 1, neuron 75 has connections 10.\n",
      "Hazard. Layer 1, neuron 76 has connections 10.\n",
      "Hazard. Layer 1, neuron 77 has connections 10.\n",
      "Hazard. Layer 1, neuron 78 has connections 10.\n",
      "Hazard. Layer 1, neuron 79 has connections 10.\n",
      "Hazard. Layer 1, neuron 80 has connections 10.\n",
      "Hazard. Layer 1, neuron 81 has connections 10.\n",
      "Hazard. Layer 1, neuron 82 has connections 10.\n",
      "Hazard. Layer 1, neuron 83 has connections 10.\n",
      "Hazard. Layer 1, neuron 84 has connections 10.\n",
      "Hazard. Layer 1, neuron 85 has connections 10.\n",
      "Hazard. Layer 1, neuron 86 has connections 10.\n",
      "Hazard. Layer 1, neuron 87 has connections 10.\n",
      "Hazard. Layer 1, neuron 88 has connections 10.\n",
      "Hazard. Layer 1, neuron 89 has connections 10.\n",
      "Hazard. Layer 1, neuron 90 has connections 10.\n",
      "Hazard. Layer 1, neuron 91 has connections 10.\n",
      "Hazard. Layer 1, neuron 92 has connections 10.\n",
      "Hazard. Layer 1, neuron 93 has connections 10.\n",
      "Hazard. Layer 1, neuron 94 has connections 10.\n",
      "Hazard. Layer 1, neuron 95 has connections 10.\n",
      "Hazard. Layer 1, neuron 96 has connections 10.\n",
      "Hazard. Layer 1, neuron 97 has connections 10.\n",
      "Hazard. Layer 1, neuron 98 has connections 10.\n",
      "Hazard. Layer 1, neuron 99 has connections 10.\n",
      "Hazard. Layer 1, neuron 100 has connections 10.\n",
      "Hazard. Layer 1, neuron 101 has connections 10.\n",
      "Hazard. Layer 1, neuron 102 has connections 10.\n",
      "Hazard. Layer 1, neuron 103 has connections 10.\n",
      "Hazard. Layer 1, neuron 104 has connections 10.\n",
      "Hazard. Layer 1, neuron 105 has connections 10.\n",
      "Hazard. Layer 1, neuron 106 has connections 10.\n",
      "Hazard. Layer 1, neuron 107 has connections 10.\n",
      "Hazard. Layer 1, neuron 108 has connections 10.\n",
      "Hazard. Layer 1, neuron 109 has connections 10.\n",
      "Hazard. Layer 1, neuron 110 has connections 10.\n",
      "Hazard. Layer 1, neuron 111 has connections 10.\n",
      "Hazard. Layer 1, neuron 112 has connections 10.\n",
      "Hazard. Layer 1, neuron 113 has connections 10.\n",
      "Hazard. Layer 1, neuron 114 has connections 10.\n",
      "Hazard. Layer 1, neuron 115 has connections 10.\n",
      "Hazard. Layer 1, neuron 116 has connections 10.\n",
      "Hazard. Layer 1, neuron 117 has connections 10.\n",
      "Hazard. Layer 1, neuron 118 has connections 10.\n",
      "Hazard. Layer 1, neuron 119 has connections 10.\n",
      "Hazard. Layer 1, neuron 120 has connections 10.\n",
      "Hazard. Layer 1, neuron 121 has connections 10.\n",
      "Hazard. Layer 1, neuron 122 has connections 10.\n",
      "Hazard. Layer 1, neuron 123 has connections 10.\n",
      "Hazard. Layer 1, neuron 124 has connections 10.\n",
      "Hazard. Layer 1, neuron 125 has connections 10.\n",
      "Hazard. Layer 1, neuron 126 has connections 10.\n",
      "Hazard. Layer 1, neuron 127 has connections 10.\n",
      "Hazard. Layer 1, neuron 128 has connections 10.\n",
      "Hazard. Layer 1, neuron 129 has connections 10.\n",
      "Hazard. Layer 1, neuron 130 has connections 10.\n",
      "Hazard. Layer 1, neuron 131 has connections 10.\n",
      "Hazard. Layer 1, neuron 132 has connections 10.\n",
      "Hazard. Layer 1, neuron 133 has connections 10.\n",
      "Hazard. Layer 1, neuron 134 has connections 10.\n",
      "Hazard. Layer 1, neuron 135 has connections 10.\n",
      "Hazard. Layer 1, neuron 136 has connections 10.\n",
      "Hazard. Layer 1, neuron 137 has connections 10.\n",
      "Hazard. Layer 1, neuron 138 has connections 10.\n",
      "Hazard. Layer 1, neuron 139 has connections 10.\n",
      "Hazard. Layer 1, neuron 140 has connections 10.\n",
      "Hazard. Layer 1, neuron 141 has connections 10.\n",
      "Hazard. Layer 1, neuron 142 has connections 10.\n",
      "Hazard. Layer 1, neuron 143 has connections 10.\n",
      "Hazard. Layer 1, neuron 144 has connections 10.\n",
      "Hazard. Layer 1, neuron 145 has connections 10.\n",
      "Hazard. Layer 1, neuron 146 has connections 10.\n",
      "Hazard. Layer 1, neuron 147 has connections 10.\n",
      "Hazard. Layer 1, neuron 148 has connections 10.\n",
      "Hazard. Layer 1, neuron 149 has connections 10.\n",
      "Hazard. Layer 1, neuron 150 has connections 10.\n",
      "Hazard. Layer 1, neuron 151 has connections 10.\n",
      "Hazard. Layer 1, neuron 152 has connections 10.\n",
      "Hazard. Layer 1, neuron 153 has connections 10.\n",
      "Hazard. Layer 1, neuron 154 has connections 10.\n",
      "Hazard. Layer 1, neuron 155 has connections 10.\n",
      "Hazard. Layer 1, neuron 156 has connections 10.\n",
      "Hazard. Layer 1, neuron 157 has connections 10.\n",
      "Hazard. Layer 1, neuron 158 has connections 10.\n",
      "Hazard. Layer 1, neuron 159 has connections 10.\n",
      "Hazard. Layer 1, neuron 160 has connections 10.\n",
      "Hazard. Layer 1, neuron 161 has connections 10.\n",
      "Hazard. Layer 1, neuron 162 has connections 10.\n",
      "Hazard. Layer 1, neuron 163 has connections 10.\n",
      "Hazard. Layer 1, neuron 164 has connections 10.\n",
      "Hazard. Layer 1, neuron 165 has connections 10.\n",
      "Hazard. Layer 1, neuron 166 has connections 10.\n",
      "Hazard. Layer 1, neuron 167 has connections 10.\n",
      "Hazard. Layer 1, neuron 168 has connections 10.\n",
      "Hazard. Layer 1, neuron 169 has connections 10.\n",
      "Hazard. Layer 1, neuron 170 has connections 10.\n",
      "Hazard. Layer 1, neuron 171 has connections 10.\n",
      "Hazard. Layer 1, neuron 172 has connections 10.\n",
      "Hazard. Layer 1, neuron 173 has connections 10.\n",
      "Hazard. Layer 1, neuron 174 has connections 10.\n",
      "Hazard. Layer 1, neuron 175 has connections 10.\n",
      "Hazard. Layer 1, neuron 176 has connections 10.\n",
      "Hazard. Layer 1, neuron 177 has connections 10.\n",
      "Hazard. Layer 1, neuron 178 has connections 10.\n",
      "Hazard. Layer 1, neuron 179 has connections 10.\n",
      "Hazard. Layer 1, neuron 180 has connections 10.\n",
      "Hazard. Layer 1, neuron 181 has connections 10.\n",
      "Hazard. Layer 1, neuron 182 has connections 10.\n",
      "Hazard. Layer 1, neuron 183 has connections 10.\n",
      "Hazard. Layer 1, neuron 184 has connections 10.\n",
      "Hazard. Layer 1, neuron 185 has connections 10.\n",
      "Hazard. Layer 1, neuron 186 has connections 10.\n",
      "Hazard. Layer 1, neuron 187 has connections 10.\n",
      "Hazard. Layer 1, neuron 188 has connections 10.\n",
      "Hazard. Layer 1, neuron 189 has connections 10.\n",
      "Hazard. Layer 1, neuron 190 has connections 10.\n",
      "Hazard. Layer 1, neuron 191 has connections 10.\n",
      "Hazard. Layer 1, neuron 192 has connections 10.\n",
      "Hazard. Layer 1, neuron 193 has connections 10.\n",
      "Hazard. Layer 1, neuron 194 has connections 10.\n",
      "Hazard. Layer 1, neuron 195 has connections 10.\n",
      "Hazard. Layer 1, neuron 196 has connections 10.\n",
      "Hazard. Layer 1, neuron 197 has connections 10.\n",
      "Hazard. Layer 1, neuron 198 has connections 10.\n",
      "Hazard. Layer 1, neuron 199 has connections 10.\n",
      "Hazard. Layer 1, neuron 200 has connections 10.\n",
      "Hazard. Layer 1, neuron 201 has connections 10.\n",
      "Hazard. Layer 1, neuron 202 has connections 10.\n",
      "Hazard. Layer 1, neuron 203 has connections 10.\n",
      "Hazard. Layer 1, neuron 204 has connections 10.\n",
      "Hazard. Layer 1, neuron 205 has connections 10.\n",
      "Hazard. Layer 1, neuron 206 has connections 10.\n",
      "Hazard. Layer 1, neuron 207 has connections 10.\n",
      "Hazard. Layer 1, neuron 208 has connections 10.\n",
      "Hazard. Layer 1, neuron 209 has connections 10.\n",
      "Hazard. Layer 1, neuron 210 has connections 10.\n",
      "Hazard. Layer 1, neuron 211 has connections 10.\n",
      "Hazard. Layer 1, neuron 212 has connections 10.\n",
      "Hazard. Layer 1, neuron 213 has connections 10.\n",
      "Hazard. Layer 1, neuron 214 has connections 10.\n",
      "Hazard. Layer 1, neuron 215 has connections 10.\n",
      "Hazard. Layer 1, neuron 216 has connections 10.\n",
      "Hazard. Layer 1, neuron 217 has connections 10.\n",
      "Hazard. Layer 1, neuron 218 has connections 10.\n",
      "Hazard. Layer 1, neuron 219 has connections 10.\n",
      "Hazard. Layer 1, neuron 220 has connections 10.\n",
      "Hazard. Layer 1, neuron 221 has connections 10.\n",
      "Hazard. Layer 1, neuron 222 has connections 10.\n",
      "Hazard. Layer 1, neuron 223 has connections 10.\n",
      "Hazard. Layer 1, neuron 224 has connections 10.\n",
      "Hazard. Layer 1, neuron 225 has connections 10.\n",
      "Hazard. Layer 1, neuron 226 has connections 10.\n",
      "Hazard. Layer 1, neuron 227 has connections 10.\n",
      "Hazard. Layer 1, neuron 228 has connections 10.\n",
      "Hazard. Layer 1, neuron 229 has connections 10.\n",
      "Hazard. Layer 1, neuron 230 has connections 10.\n",
      "Hazard. Layer 1, neuron 231 has connections 10.\n",
      "Hazard. Layer 1, neuron 232 has connections 10.\n",
      "Hazard. Layer 1, neuron 233 has connections 10.\n",
      "Hazard. Layer 1, neuron 234 has connections 10.\n",
      "Hazard. Layer 1, neuron 235 has connections 10.\n",
      "Hazard. Layer 1, neuron 236 has connections 10.\n",
      "Hazard. Layer 1, neuron 237 has connections 10.\n",
      "Hazard. Layer 1, neuron 238 has connections 10.\n",
      "Hazard. Layer 1, neuron 239 has connections 10.\n",
      "Hazard. Layer 1, neuron 240 has connections 10.\n",
      "Hazard. Layer 1, neuron 241 has connections 10.\n",
      "Hazard. Layer 1, neuron 242 has connections 10.\n",
      "Hazard. Layer 1, neuron 243 has connections 10.\n",
      "Hazard. Layer 1, neuron 244 has connections 10.\n",
      "Hazard. Layer 1, neuron 245 has connections 10.\n",
      "Hazard. Layer 1, neuron 246 has connections 10.\n",
      "Hazard. Layer 1, neuron 247 has connections 10.\n",
      "Hazard. Layer 1, neuron 248 has connections 10.\n",
      "Hazard. Layer 1, neuron 249 has connections 10.\n",
      "Hazard. Layer 1, neuron 250 has connections 10.\n",
      "Hazard. Layer 1, neuron 251 has connections 10.\n",
      "Hazard. Layer 1, neuron 252 has connections 10.\n",
      "Hazard. Layer 1, neuron 253 has connections 10.\n",
      "Hazard. Layer 1, neuron 254 has connections 10.\n",
      "Hazard. Layer 1, neuron 255 has connections 10.\n",
      "Hazard. Layer 1, neuron 256 has connections 10.\n",
      "Hazard. Layer 1, neuron 257 has connections 10.\n",
      "Hazard. Layer 1, neuron 258 has connections 10.\n",
      "Hazard. Layer 1, neuron 259 has connections 10.\n",
      "Hazard. Layer 1, neuron 260 has connections 10.\n",
      "Hazard. Layer 1, neuron 261 has connections 10.\n",
      "Hazard. Layer 1, neuron 262 has connections 10.\n",
      "Hazard. Layer 1, neuron 263 has connections 10.\n",
      "Hazard. Layer 1, neuron 264 has connections 10.\n",
      "Hazard. Layer 1, neuron 265 has connections 10.\n",
      "Hazard. Layer 1, neuron 266 has connections 10.\n",
      "Hazard. Layer 1, neuron 267 has connections 10.\n",
      "Hazard. Layer 1, neuron 268 has connections 10.\n",
      "Hazard. Layer 1, neuron 269 has connections 10.\n",
      "Hazard. Layer 1, neuron 270 has connections 10.\n",
      "Hazard. Layer 1, neuron 271 has connections 10.\n",
      "Hazard. Layer 1, neuron 272 has connections 10.\n",
      "Hazard. Layer 1, neuron 273 has connections 10.\n",
      "Hazard. Layer 1, neuron 274 has connections 10.\n",
      "Hazard. Layer 1, neuron 275 has connections 10.\n",
      "Hazard. Layer 1, neuron 276 has connections 10.\n",
      "Hazard. Layer 1, neuron 277 has connections 10.\n",
      "Hazard. Layer 1, neuron 278 has connections 10.\n",
      "Hazard. Layer 1, neuron 279 has connections 10.\n",
      "Hazard. Layer 1, neuron 280 has connections 10.\n",
      "Hazard. Layer 1, neuron 281 has connections 10.\n",
      "Hazard. Layer 1, neuron 282 has connections 10.\n",
      "Hazard. Layer 1, neuron 283 has connections 10.\n",
      "Hazard. Layer 1, neuron 284 has connections 10.\n",
      "Hazard. Layer 1, neuron 285 has connections 10.\n",
      "Hazard. Layer 1, neuron 286 has connections 10.\n",
      "Hazard. Layer 1, neuron 287 has connections 10.\n",
      "Hazard. Layer 1, neuron 288 has connections 10.\n",
      "Hazard. Layer 1, neuron 289 has connections 10.\n",
      "Hazard. Layer 1, neuron 290 has connections 10.\n",
      "Hazard. Layer 1, neuron 291 has connections 10.\n",
      "Hazard. Layer 1, neuron 292 has connections 10.\n",
      "Hazard. Layer 1, neuron 293 has connections 10.\n",
      "Hazard. Layer 1, neuron 294 has connections 10.\n",
      "Hazard. Layer 1, neuron 295 has connections 10.\n",
      "Hazard. Layer 1, neuron 296 has connections 10.\n",
      "Hazard. Layer 1, neuron 297 has connections 10.\n",
      "Hazard. Layer 1, neuron 298 has connections 10.\n",
      "Hazard. Layer 1, neuron 299 has connections 10.\n",
      "Hazard. Layer 1, neuron 300 has connections 10.\n",
      "Hazard. Layer 1, neuron 301 has connections 10.\n",
      "Hazard. Layer 1, neuron 302 has connections 10.\n",
      "Hazard. Layer 1, neuron 303 has connections 10.\n",
      "Hazard. Layer 1, neuron 304 has connections 10.\n",
      "Hazard. Layer 1, neuron 305 has connections 10.\n",
      "Hazard. Layer 1, neuron 306 has connections 10.\n",
      "Hazard. Layer 1, neuron 307 has connections 10.\n",
      "Hazard. Layer 1, neuron 308 has connections 10.\n",
      "Hazard. Layer 1, neuron 309 has connections 10.\n",
      "Hazard. Layer 1, neuron 310 has connections 10.\n",
      "Hazard. Layer 1, neuron 311 has connections 10.\n",
      "Hazard. Layer 1, neuron 312 has connections 10.\n",
      "Hazard. Layer 1, neuron 313 has connections 10.\n",
      "Hazard. Layer 1, neuron 314 has connections 10.\n",
      "Hazard. Layer 1, neuron 315 has connections 10.\n",
      "Hazard. Layer 1, neuron 316 has connections 10.\n",
      "Hazard. Layer 1, neuron 317 has connections 10.\n",
      "Hazard. Layer 1, neuron 318 has connections 10.\n",
      "Hazard. Layer 1, neuron 319 has connections 10.\n",
      "Hazard. Layer 1, neuron 320 has connections 10.\n",
      "Hazard. Layer 1, neuron 321 has connections 10.\n",
      "Hazard. Layer 1, neuron 322 has connections 10.\n",
      "Hazard. Layer 1, neuron 323 has connections 10.\n",
      "Hazard. Layer 1, neuron 324 has connections 10.\n",
      "Hazard. Layer 1, neuron 325 has connections 10.\n",
      "Hazard. Layer 1, neuron 326 has connections 10.\n",
      "Hazard. Layer 1, neuron 327 has connections 10.\n",
      "Hazard. Layer 1, neuron 328 has connections 10.\n",
      "Hazard. Layer 1, neuron 329 has connections 10.\n",
      "Hazard. Layer 1, neuron 330 has connections 10.\n",
      "Hazard. Layer 1, neuron 331 has connections 10.\n",
      "Hazard. Layer 1, neuron 332 has connections 10.\n",
      "Hazard. Layer 1, neuron 333 has connections 10.\n",
      "Hazard. Layer 1, neuron 334 has connections 10.\n",
      "Hazard. Layer 1, neuron 335 has connections 10.\n",
      "Hazard. Layer 1, neuron 336 has connections 10.\n",
      "Hazard. Layer 1, neuron 337 has connections 10.\n",
      "Hazard. Layer 1, neuron 338 has connections 10.\n",
      "Hazard. Layer 1, neuron 339 has connections 10.\n",
      "Hazard. Layer 1, neuron 340 has connections 10.\n",
      "Hazard. Layer 1, neuron 341 has connections 10.\n",
      "Hazard. Layer 1, neuron 342 has connections 10.\n",
      "Hazard. Layer 1, neuron 343 has connections 10.\n",
      "Hazard. Layer 1, neuron 344 has connections 10.\n",
      "Hazard. Layer 1, neuron 345 has connections 10.\n",
      "Hazard. Layer 1, neuron 346 has connections 10.\n",
      "Hazard. Layer 1, neuron 347 has connections 10.\n",
      "Hazard. Layer 1, neuron 348 has connections 10.\n",
      "Hazard. Layer 1, neuron 349 has connections 10.\n",
      "Hazard. Layer 1, neuron 350 has connections 10.\n",
      "Hazard. Layer 1, neuron 351 has connections 10.\n",
      "Hazard. Layer 1, neuron 352 has connections 10.\n",
      "Hazard. Layer 1, neuron 353 has connections 10.\n",
      "Hazard. Layer 1, neuron 354 has connections 10.\n",
      "Hazard. Layer 1, neuron 355 has connections 10.\n",
      "Hazard. Layer 1, neuron 356 has connections 10.\n",
      "Hazard. Layer 1, neuron 357 has connections 10.\n",
      "Hazard. Layer 1, neuron 358 has connections 10.\n",
      "Hazard. Layer 1, neuron 359 has connections 10.\n",
      "Hazard. Layer 1, neuron 360 has connections 10.\n",
      "Hazard. Layer 1, neuron 361 has connections 10.\n",
      "Hazard. Layer 1, neuron 362 has connections 10.\n",
      "Hazard. Layer 1, neuron 363 has connections 10.\n",
      "Hazard. Layer 1, neuron 364 has connections 10.\n",
      "Hazard. Layer 1, neuron 365 has connections 10.\n",
      "Hazard. Layer 1, neuron 366 has connections 10.\n",
      "Hazard. Layer 1, neuron 367 has connections 10.\n",
      "Hazard. Layer 1, neuron 368 has connections 10.\n",
      "Hazard. Layer 1, neuron 369 has connections 10.\n",
      "Hazard. Layer 1, neuron 370 has connections 10.\n",
      "Hazard. Layer 1, neuron 371 has connections 10.\n",
      "Hazard. Layer 1, neuron 372 has connections 10.\n",
      "Hazard. Layer 1, neuron 373 has connections 10.\n",
      "Hazard. Layer 1, neuron 374 has connections 10.\n",
      "Hazard. Layer 1, neuron 375 has connections 10.\n",
      "Hazard. Layer 1, neuron 376 has connections 10.\n",
      "Hazard. Layer 1, neuron 377 has connections 10.\n",
      "Hazard. Layer 1, neuron 378 has connections 10.\n",
      "Hazard. Layer 1, neuron 379 has connections 10.\n",
      "Hazard. Layer 1, neuron 380 has connections 10.\n",
      "Hazard. Layer 1, neuron 381 has connections 10.\n",
      "Hazard. Layer 1, neuron 382 has connections 10.\n",
      "Hazard. Layer 1, neuron 383 has connections 10.\n",
      "Hazard. Layer 1, neuron 384 has connections 10.\n",
      "Hazard. Layer 1, neuron 385 has connections 10.\n",
      "Hazard. Layer 1, neuron 386 has connections 10.\n",
      "Hazard. Layer 1, neuron 387 has connections 10.\n",
      "Hazard. Layer 1, neuron 388 has connections 10.\n",
      "Hazard. Layer 1, neuron 389 has connections 10.\n",
      "Hazard. Layer 1, neuron 390 has connections 10.\n",
      "Hazard. Layer 1, neuron 391 has connections 10.\n",
      "Hazard. Layer 1, neuron 392 has connections 10.\n",
      "Hazard. Layer 1, neuron 393 has connections 10.\n",
      "Hazard. Layer 1, neuron 394 has connections 10.\n",
      "Hazard. Layer 1, neuron 395 has connections 10.\n",
      "Hazard. Layer 1, neuron 396 has connections 10.\n",
      "Hazard. Layer 1, neuron 397 has connections 10.\n",
      "Hazard. Layer 1, neuron 398 has connections 10.\n",
      "Hazard. Layer 1, neuron 399 has connections 10.\n",
      "Hazard. Layer 1, neuron 400 has connections 10.\n",
      "Hazard. Layer 1, neuron 401 has connections 10.\n",
      "Hazard. Layer 1, neuron 402 has connections 10.\n",
      "Hazard. Layer 1, neuron 403 has connections 10.\n",
      "Hazard. Layer 1, neuron 404 has connections 10.\n",
      "Hazard. Layer 1, neuron 405 has connections 10.\n",
      "Hazard. Layer 1, neuron 406 has connections 10.\n",
      "Hazard. Layer 1, neuron 407 has connections 10.\n",
      "Hazard. Layer 1, neuron 408 has connections 10.\n",
      "Hazard. Layer 1, neuron 409 has connections 10.\n",
      "Hazard. Layer 1, neuron 410 has connections 10.\n",
      "Hazard. Layer 1, neuron 411 has connections 10.\n",
      "Hazard. Layer 1, neuron 412 has connections 10.\n",
      "Hazard. Layer 1, neuron 413 has connections 10.\n",
      "Hazard. Layer 1, neuron 414 has connections 10.\n",
      "Hazard. Layer 1, neuron 415 has connections 10.\n",
      "Hazard. Layer 1, neuron 416 has connections 10.\n",
      "Hazard. Layer 1, neuron 417 has connections 10.\n",
      "Hazard. Layer 1, neuron 418 has connections 10.\n",
      "Hazard. Layer 1, neuron 419 has connections 10.\n",
      "Hazard. Layer 1, neuron 420 has connections 10.\n",
      "Hazard. Layer 1, neuron 421 has connections 10.\n",
      "Hazard. Layer 1, neuron 422 has connections 10.\n",
      "Hazard. Layer 1, neuron 423 has connections 10.\n",
      "Hazard. Layer 1, neuron 424 has connections 10.\n",
      "Hazard. Layer 1, neuron 425 has connections 10.\n",
      "Hazard. Layer 1, neuron 426 has connections 10.\n",
      "Hazard. Layer 1, neuron 427 has connections 10.\n",
      "Hazard. Layer 1, neuron 428 has connections 10.\n",
      "Hazard. Layer 1, neuron 429 has connections 10.\n",
      "Hazard. Layer 1, neuron 430 has connections 10.\n",
      "Hazard. Layer 1, neuron 431 has connections 10.\n",
      "Hazard. Layer 1, neuron 432 has connections 10.\n",
      "Hazard. Layer 1, neuron 433 has connections 10.\n",
      "Hazard. Layer 1, neuron 434 has connections 10.\n",
      "Hazard. Layer 1, neuron 435 has connections 10.\n",
      "Hazard. Layer 1, neuron 436 has connections 10.\n",
      "Hazard. Layer 1, neuron 437 has connections 10.\n",
      "Hazard. Layer 1, neuron 438 has connections 10.\n",
      "Hazard. Layer 1, neuron 439 has connections 10.\n",
      "Hazard. Layer 1, neuron 440 has connections 10.\n",
      "Hazard. Layer 1, neuron 441 has connections 10.\n",
      "Hazard. Layer 1, neuron 442 has connections 10.\n",
      "Hazard. Layer 1, neuron 443 has connections 10.\n",
      "Hazard. Layer 1, neuron 444 has connections 10.\n",
      "Hazard. Layer 1, neuron 445 has connections 10.\n",
      "Hazard. Layer 1, neuron 446 has connections 10.\n",
      "Hazard. Layer 1, neuron 447 has connections 10.\n",
      "Hazard. Layer 1, neuron 448 has connections 10.\n",
      "Hazard. Layer 1, neuron 449 has connections 10.\n",
      "Hazard. Layer 1, neuron 450 has connections 10.\n",
      "Hazard. Layer 1, neuron 451 has connections 10.\n",
      "Hazard. Layer 1, neuron 452 has connections 10.\n",
      "Hazard. Layer 1, neuron 453 has connections 10.\n",
      "Hazard. Layer 1, neuron 454 has connections 10.\n",
      "Hazard. Layer 1, neuron 455 has connections 10.\n",
      "Hazard. Layer 1, neuron 456 has connections 10.\n",
      "Hazard. Layer 1, neuron 457 has connections 10.\n",
      "Hazard. Layer 1, neuron 458 has connections 10.\n",
      "Hazard. Layer 1, neuron 459 has connections 10.\n",
      "Hazard. Layer 1, neuron 460 has connections 10.\n",
      "Hazard. Layer 1, neuron 461 has connections 10.\n",
      "Hazard. Layer 1, neuron 462 has connections 10.\n",
      "Hazard. Layer 1, neuron 463 has connections 10.\n",
      "Hazard. Layer 1, neuron 464 has connections 10.\n",
      "Hazard. Layer 1, neuron 465 has connections 10.\n",
      "Hazard. Layer 1, neuron 466 has connections 10.\n",
      "Hazard. Layer 1, neuron 467 has connections 10.\n",
      "Hazard. Layer 1, neuron 468 has connections 10.\n",
      "Hazard. Layer 1, neuron 469 has connections 10.\n",
      "Hazard. Layer 1, neuron 470 has connections 10.\n",
      "Hazard. Layer 1, neuron 471 has connections 10.\n",
      "Hazard. Layer 1, neuron 472 has connections 10.\n",
      "Hazard. Layer 1, neuron 473 has connections 10.\n",
      "Hazard. Layer 1, neuron 474 has connections 10.\n",
      "Hazard. Layer 1, neuron 475 has connections 10.\n",
      "Hazard. Layer 1, neuron 476 has connections 10.\n",
      "Hazard. Layer 1, neuron 477 has connections 10.\n",
      "Hazard. Layer 1, neuron 478 has connections 10.\n",
      "Hazard. Layer 1, neuron 479 has connections 10.\n",
      "Hazard. Layer 1, neuron 480 has connections 10.\n",
      "Hazard. Layer 1, neuron 481 has connections 10.\n",
      "Hazard. Layer 1, neuron 482 has connections 10.\n",
      "Hazard. Layer 1, neuron 483 has connections 10.\n",
      "Hazard. Layer 1, neuron 484 has connections 10.\n",
      "Hazard. Layer 1, neuron 485 has connections 10.\n",
      "Hazard. Layer 1, neuron 486 has connections 10.\n",
      "Hazard. Layer 1, neuron 487 has connections 10.\n",
      "Hazard. Layer 1, neuron 488 has connections 10.\n",
      "Hazard. Layer 1, neuron 489 has connections 10.\n",
      "Hazard. Layer 1, neuron 490 has connections 10.\n",
      "Hazard. Layer 1, neuron 491 has connections 10.\n",
      "Hazard. Layer 1, neuron 492 has connections 10.\n",
      "Hazard. Layer 1, neuron 493 has connections 10.\n",
      "Hazard. Layer 1, neuron 494 has connections 10.\n",
      "Hazard. Layer 1, neuron 495 has connections 10.\n",
      "Hazard. Layer 1, neuron 496 has connections 10.\n",
      "Hazard. Layer 1, neuron 497 has connections 10.\n",
      "Hazard. Layer 1, neuron 498 has connections 10.\n",
      "Hazard. Layer 1, neuron 499 has connections 10.\n",
      "Hazard. Layer 1, neuron 500 has connections 10.\n",
      "Hazard. Layer 1, neuron 501 has connections 10.\n",
      "Hazard. Layer 1, neuron 502 has connections 10.\n",
      "Hazard. Layer 1, neuron 503 has connections 10.\n",
      "Hazard. Layer 1, neuron 504 has connections 10.\n",
      "Hazard. Layer 1, neuron 505 has connections 10.\n",
      "Hazard. Layer 1, neuron 506 has connections 10.\n",
      "Hazard. Layer 1, neuron 507 has connections 10.\n",
      "Hazard. Layer 1, neuron 508 has connections 10.\n",
      "Hazard. Layer 1, neuron 509 has connections 10.\n",
      "Hazard. Layer 1, neuron 510 has connections 10.\n",
      "Hazard. Layer 1, neuron 511 has connections 10.\n",
      "Hazard. Layer 1, neuron 512 has connections 10.\n",
      "Hazard. Layer 2, neuron 1 has connections 10.\n",
      "Hazard. Layer 2, neuron 2 has connections 10.\n",
      "Hazard. Layer 2, neuron 3 has connections 10.\n",
      "Hazard. Layer 2, neuron 4 has connections 10.\n",
      "Hazard. Layer 2, neuron 5 has connections 10.\n",
      "Hazard. Layer 2, neuron 6 has connections 10.\n",
      "Hazard. Layer 2, neuron 7 has connections 10.\n",
      "Hazard. Layer 2, neuron 8 has connections 10.\n",
      "Hazard. Layer 2, neuron 9 has connections 10.\n",
      "Hazard. Layer 2, neuron 10 has connections 10.\n",
      "Hazard. Layer 2, neuron 11 has connections 10.\n",
      "Hazard. Layer 2, neuron 12 has connections 10.\n",
      "Hazard. Layer 2, neuron 13 has connections 10.\n",
      "Hazard. Layer 2, neuron 14 has connections 10.\n",
      "Hazard. Layer 2, neuron 15 has connections 10.\n",
      "Hazard. Layer 2, neuron 16 has connections 10.\n",
      "Hazard. Layer 2, neuron 17 has connections 10.\n",
      "Hazard. Layer 2, neuron 18 has connections 10.\n",
      "Hazard. Layer 2, neuron 19 has connections 10.\n",
      "Hazard. Layer 2, neuron 20 has connections 10.\n",
      "Hazard. Layer 2, neuron 21 has connections 10.\n",
      "Hazard. Layer 2, neuron 22 has connections 10.\n",
      "Hazard. Layer 2, neuron 23 has connections 10.\n",
      "Hazard. Layer 2, neuron 24 has connections 10.\n",
      "Hazard. Layer 2, neuron 25 has connections 10.\n",
      "Hazard. Layer 2, neuron 26 has connections 10.\n",
      "Hazard. Layer 2, neuron 27 has connections 10.\n",
      "Hazard. Layer 2, neuron 28 has connections 10.\n",
      "Hazard. Layer 2, neuron 29 has connections 10.\n",
      "Hazard. Layer 2, neuron 30 has connections 10.\n",
      "Hazard. Layer 2, neuron 31 has connections 10.\n",
      "Hazard. Layer 2, neuron 32 has connections 10.\n",
      "Hazard. Layer 2, neuron 33 has connections 10.\n",
      "Hazard. Layer 2, neuron 34 has connections 10.\n",
      "Hazard. Layer 2, neuron 35 has connections 10.\n",
      "Hazard. Layer 2, neuron 36 has connections 10.\n",
      "Hazard. Layer 2, neuron 37 has connections 10.\n",
      "Hazard. Layer 2, neuron 38 has connections 10.\n",
      "Hazard. Layer 2, neuron 39 has connections 10.\n",
      "Hazard. Layer 2, neuron 40 has connections 10.\n",
      "Hazard. Layer 2, neuron 41 has connections 10.\n",
      "Hazard. Layer 2, neuron 42 has connections 10.\n",
      "Hazard. Layer 2, neuron 43 has connections 10.\n",
      "Hazard. Layer 2, neuron 44 has connections 10.\n",
      "Hazard. Layer 2, neuron 45 has connections 10.\n",
      "Hazard. Layer 2, neuron 46 has connections 10.\n",
      "Hazard. Layer 2, neuron 47 has connections 10.\n",
      "Hazard. Layer 2, neuron 48 has connections 10.\n",
      "Hazard. Layer 2, neuron 49 has connections 10.\n",
      "Hazard. Layer 2, neuron 50 has connections 10.\n",
      "Hazard. Layer 2, neuron 51 has connections 10.\n",
      "Hazard. Layer 2, neuron 52 has connections 10.\n",
      "Hazard. Layer 2, neuron 53 has connections 10.\n",
      "Hazard. Layer 2, neuron 54 has connections 10.\n",
      "Hazard. Layer 2, neuron 55 has connections 10.\n",
      "Hazard. Layer 2, neuron 56 has connections 10.\n",
      "Hazard. Layer 2, neuron 57 has connections 10.\n",
      "Hazard. Layer 2, neuron 58 has connections 10.\n",
      "Hazard. Layer 2, neuron 59 has connections 10.\n",
      "Hazard. Layer 2, neuron 60 has connections 10.\n",
      "Hazard. Layer 2, neuron 61 has connections 10.\n",
      "Hazard. Layer 2, neuron 62 has connections 10.\n",
      "Hazard. Layer 2, neuron 63 has connections 10.\n",
      "Hazard. Layer 2, neuron 64 has connections 10.\n",
      "Hazard. Layer 2, neuron 65 has connections 10.\n",
      "Hazard. Layer 2, neuron 66 has connections 10.\n",
      "Hazard. Layer 2, neuron 67 has connections 10.\n",
      "Hazard. Layer 2, neuron 68 has connections 10.\n",
      "Hazard. Layer 2, neuron 69 has connections 10.\n",
      "Hazard. Layer 2, neuron 70 has connections 10.\n",
      "Hazard. Layer 2, neuron 71 has connections 10.\n",
      "Hazard. Layer 2, neuron 72 has connections 10.\n",
      "Hazard. Layer 2, neuron 73 has connections 10.\n",
      "Hazard. Layer 2, neuron 74 has connections 10.\n",
      "Hazard. Layer 2, neuron 75 has connections 10.\n",
      "Hazard. Layer 2, neuron 76 has connections 10.\n",
      "Hazard. Layer 2, neuron 77 has connections 10.\n",
      "Hazard. Layer 2, neuron 78 has connections 10.\n",
      "Hazard. Layer 2, neuron 79 has connections 10.\n",
      "Hazard. Layer 2, neuron 80 has connections 10.\n",
      "Hazard. Layer 2, neuron 81 has connections 10.\n",
      "Hazard. Layer 2, neuron 82 has connections 10.\n",
      "Hazard. Layer 2, neuron 83 has connections 10.\n",
      "Hazard. Layer 2, neuron 84 has connections 10.\n",
      "Hazard. Layer 2, neuron 85 has connections 10.\n",
      "Hazard. Layer 2, neuron 86 has connections 10.\n",
      "Hazard. Layer 2, neuron 87 has connections 10.\n",
      "Hazard. Layer 2, neuron 88 has connections 10.\n",
      "Hazard. Layer 2, neuron 89 has connections 10.\n",
      "Hazard. Layer 2, neuron 90 has connections 10.\n",
      "Hazard. Layer 2, neuron 91 has connections 10.\n",
      "Hazard. Layer 2, neuron 92 has connections 10.\n",
      "Hazard. Layer 2, neuron 93 has connections 10.\n",
      "Hazard. Layer 2, neuron 94 has connections 10.\n",
      "Hazard. Layer 2, neuron 95 has connections 10.\n",
      "Hazard. Layer 2, neuron 96 has connections 10.\n",
      "Hazard. Layer 2, neuron 97 has connections 10.\n",
      "Hazard. Layer 2, neuron 98 has connections 10.\n",
      "Hazard. Layer 2, neuron 99 has connections 10.\n",
      "Hazard. Layer 2, neuron 100 has connections 10.\n",
      "Hazard. Layer 2, neuron 101 has connections 10.\n",
      "Hazard. Layer 2, neuron 102 has connections 10.\n",
      "Hazard. Layer 2, neuron 103 has connections 10.\n",
      "Hazard. Layer 2, neuron 104 has connections 10.\n",
      "Hazard. Layer 2, neuron 105 has connections 10.\n",
      "Hazard. Layer 2, neuron 106 has connections 10.\n",
      "Hazard. Layer 2, neuron 107 has connections 10.\n",
      "Hazard. Layer 2, neuron 108 has connections 10.\n",
      "Hazard. Layer 2, neuron 109 has connections 10.\n",
      "Hazard. Layer 2, neuron 110 has connections 10.\n",
      "Hazard. Layer 2, neuron 111 has connections 10.\n",
      "Hazard. Layer 2, neuron 112 has connections 10.\n",
      "Hazard. Layer 2, neuron 113 has connections 10.\n",
      "Hazard. Layer 2, neuron 114 has connections 10.\n",
      "Hazard. Layer 2, neuron 115 has connections 10.\n",
      "Hazard. Layer 2, neuron 116 has connections 10.\n",
      "Hazard. Layer 2, neuron 117 has connections 10.\n",
      "Hazard. Layer 2, neuron 118 has connections 10.\n",
      "Hazard. Layer 2, neuron 119 has connections 10.\n",
      "Hazard. Layer 2, neuron 120 has connections 10.\n",
      "Hazard. Layer 2, neuron 121 has connections 10.\n",
      "Hazard. Layer 2, neuron 122 has connections 10.\n",
      "Hazard. Layer 2, neuron 123 has connections 10.\n",
      "Hazard. Layer 2, neuron 124 has connections 10.\n",
      "Hazard. Layer 2, neuron 125 has connections 10.\n",
      "Hazard. Layer 2, neuron 126 has connections 10.\n",
      "Hazard. Layer 2, neuron 127 has connections 10.\n",
      "Hazard. Layer 2, neuron 128 has connections 10.\n",
      "Hazard. Layer 2, neuron 129 has connections 10.\n",
      "Hazard. Layer 2, neuron 130 has connections 10.\n",
      "Hazard. Layer 2, neuron 131 has connections 10.\n",
      "Hazard. Layer 2, neuron 132 has connections 10.\n",
      "Hazard. Layer 2, neuron 133 has connections 10.\n",
      "Hazard. Layer 2, neuron 134 has connections 10.\n",
      "Hazard. Layer 2, neuron 135 has connections 10.\n",
      "Hazard. Layer 2, neuron 136 has connections 10.\n",
      "Hazard. Layer 2, neuron 137 has connections 10.\n",
      "Hazard. Layer 2, neuron 138 has connections 10.\n",
      "Hazard. Layer 2, neuron 139 has connections 10.\n",
      "Hazard. Layer 2, neuron 140 has connections 10.\n",
      "Hazard. Layer 2, neuron 141 has connections 10.\n",
      "Hazard. Layer 2, neuron 142 has connections 10.\n",
      "Hazard. Layer 2, neuron 143 has connections 10.\n",
      "Hazard. Layer 2, neuron 144 has connections 10.\n",
      "Hazard. Layer 2, neuron 145 has connections 10.\n",
      "Hazard. Layer 2, neuron 146 has connections 10.\n",
      "Hazard. Layer 2, neuron 147 has connections 10.\n",
      "Hazard. Layer 2, neuron 148 has connections 10.\n",
      "Hazard. Layer 2, neuron 149 has connections 10.\n",
      "Hazard. Layer 2, neuron 150 has connections 10.\n",
      "Hazard. Layer 2, neuron 151 has connections 10.\n",
      "Hazard. Layer 2, neuron 152 has connections 10.\n",
      "Hazard. Layer 2, neuron 153 has connections 10.\n",
      "Hazard. Layer 2, neuron 154 has connections 10.\n",
      "Hazard. Layer 2, neuron 155 has connections 10.\n",
      "Hazard. Layer 2, neuron 156 has connections 10.\n",
      "Hazard. Layer 2, neuron 157 has connections 10.\n",
      "Hazard. Layer 2, neuron 158 has connections 10.\n",
      "Hazard. Layer 2, neuron 159 has connections 10.\n",
      "Hazard. Layer 2, neuron 160 has connections 10.\n",
      "Hazard. Layer 2, neuron 161 has connections 10.\n",
      "Hazard. Layer 2, neuron 162 has connections 10.\n",
      "Hazard. Layer 2, neuron 163 has connections 10.\n",
      "Hazard. Layer 2, neuron 164 has connections 10.\n",
      "Hazard. Layer 2, neuron 165 has connections 10.\n",
      "Hazard. Layer 2, neuron 166 has connections 10.\n",
      "Hazard. Layer 2, neuron 167 has connections 10.\n",
      "Hazard. Layer 2, neuron 168 has connections 10.\n",
      "Hazard. Layer 2, neuron 169 has connections 10.\n",
      "Hazard. Layer 2, neuron 170 has connections 10.\n",
      "Hazard. Layer 2, neuron 171 has connections 10.\n",
      "Hazard. Layer 2, neuron 172 has connections 10.\n",
      "Hazard. Layer 2, neuron 173 has connections 10.\n",
      "Hazard. Layer 2, neuron 174 has connections 10.\n",
      "Hazard. Layer 2, neuron 175 has connections 10.\n",
      "Hazard. Layer 2, neuron 176 has connections 10.\n",
      "Hazard. Layer 2, neuron 177 has connections 10.\n",
      "Hazard. Layer 2, neuron 178 has connections 10.\n",
      "Hazard. Layer 2, neuron 179 has connections 10.\n",
      "Hazard. Layer 2, neuron 180 has connections 10.\n",
      "Hazard. Layer 2, neuron 181 has connections 10.\n",
      "Hazard. Layer 2, neuron 182 has connections 10.\n",
      "Hazard. Layer 2, neuron 183 has connections 10.\n",
      "Hazard. Layer 2, neuron 184 has connections 10.\n",
      "Hazard. Layer 2, neuron 185 has connections 10.\n",
      "Hazard. Layer 2, neuron 186 has connections 10.\n",
      "Hazard. Layer 2, neuron 187 has connections 10.\n",
      "Hazard. Layer 2, neuron 188 has connections 10.\n",
      "Hazard. Layer 2, neuron 189 has connections 10.\n",
      "Hazard. Layer 2, neuron 190 has connections 10.\n",
      "Hazard. Layer 2, neuron 191 has connections 10.\n",
      "Hazard. Layer 2, neuron 192 has connections 10.\n",
      "Hazard. Layer 2, neuron 193 has connections 10.\n",
      "Hazard. Layer 2, neuron 194 has connections 10.\n",
      "Hazard. Layer 2, neuron 195 has connections 10.\n",
      "Hazard. Layer 2, neuron 196 has connections 10.\n",
      "Hazard. Layer 2, neuron 197 has connections 10.\n",
      "Hazard. Layer 2, neuron 198 has connections 10.\n",
      "Hazard. Layer 2, neuron 199 has connections 10.\n",
      "Hazard. Layer 2, neuron 200 has connections 10.\n",
      "Hazard. Layer 2, neuron 201 has connections 10.\n",
      "Hazard. Layer 2, neuron 202 has connections 10.\n",
      "Hazard. Layer 2, neuron 203 has connections 10.\n",
      "Hazard. Layer 2, neuron 204 has connections 10.\n",
      "Hazard. Layer 2, neuron 205 has connections 10.\n",
      "Hazard. Layer 2, neuron 206 has connections 10.\n",
      "Hazard. Layer 2, neuron 207 has connections 10.\n",
      "Hazard. Layer 2, neuron 208 has connections 10.\n",
      "Hazard. Layer 2, neuron 209 has connections 10.\n",
      "Hazard. Layer 2, neuron 210 has connections 10.\n",
      "Hazard. Layer 2, neuron 211 has connections 10.\n",
      "Hazard. Layer 2, neuron 212 has connections 10.\n",
      "Hazard. Layer 2, neuron 213 has connections 10.\n",
      "Hazard. Layer 2, neuron 214 has connections 10.\n",
      "Hazard. Layer 2, neuron 215 has connections 10.\n",
      "Hazard. Layer 2, neuron 216 has connections 10.\n",
      "Hazard. Layer 2, neuron 217 has connections 10.\n",
      "Hazard. Layer 2, neuron 218 has connections 10.\n",
      "Hazard. Layer 2, neuron 219 has connections 10.\n",
      "Hazard. Layer 2, neuron 220 has connections 10.\n",
      "Hazard. Layer 2, neuron 221 has connections 10.\n",
      "Hazard. Layer 2, neuron 222 has connections 10.\n",
      "Hazard. Layer 2, neuron 223 has connections 10.\n",
      "Hazard. Layer 2, neuron 224 has connections 10.\n",
      "Hazard. Layer 2, neuron 225 has connections 10.\n",
      "Hazard. Layer 2, neuron 226 has connections 10.\n",
      "Hazard. Layer 2, neuron 227 has connections 10.\n",
      "Hazard. Layer 2, neuron 228 has connections 10.\n",
      "Hazard. Layer 2, neuron 229 has connections 10.\n",
      "Hazard. Layer 2, neuron 230 has connections 10.\n",
      "Hazard. Layer 2, neuron 231 has connections 10.\n",
      "Hazard. Layer 2, neuron 232 has connections 10.\n",
      "Hazard. Layer 2, neuron 233 has connections 10.\n",
      "Hazard. Layer 2, neuron 234 has connections 10.\n",
      "Hazard. Layer 2, neuron 235 has connections 10.\n",
      "Hazard. Layer 2, neuron 236 has connections 10.\n",
      "Hazard. Layer 2, neuron 237 has connections 10.\n",
      "Hazard. Layer 2, neuron 238 has connections 10.\n",
      "Hazard. Layer 2, neuron 239 has connections 10.\n",
      "Hazard. Layer 2, neuron 240 has connections 10.\n",
      "Hazard. Layer 2, neuron 241 has connections 10.\n",
      "Hazard. Layer 2, neuron 242 has connections 10.\n",
      "Hazard. Layer 2, neuron 243 has connections 10.\n",
      "Hazard. Layer 2, neuron 244 has connections 10.\n",
      "Hazard. Layer 2, neuron 245 has connections 10.\n",
      "Hazard. Layer 2, neuron 246 has connections 10.\n",
      "Hazard. Layer 2, neuron 247 has connections 10.\n",
      "Hazard. Layer 2, neuron 248 has connections 10.\n",
      "Hazard. Layer 2, neuron 249 has connections 10.\n",
      "Hazard. Layer 2, neuron 250 has connections 10.\n",
      "Hazard. Layer 2, neuron 251 has connections 10.\n",
      "Hazard. Layer 2, neuron 252 has connections 10.\n",
      "Hazard. Layer 2, neuron 253 has connections 10.\n",
      "Hazard. Layer 2, neuron 254 has connections 10.\n",
      "Hazard. Layer 2, neuron 255 has connections 10.\n",
      "Hazard. Layer 2, neuron 256 has connections 10.\n",
      "Hazard. Layer 3, neuron 1 has connections 9.\n",
      "Hazard. Layer 3, neuron 2 has connections 9.\n",
      "Hazard. Layer 3, neuron 3 has connections 9.\n",
      "Hazard. Layer 3, neuron 4 has connections 9.\n",
      "Hazard. Layer 3, neuron 5 has connections 9.\n",
      "Hazard. Layer 3, neuron 6 has connections 9.\n",
      "Hazard. Layer 3, neuron 7 has connections 9.\n",
      "Hazard. Layer 3, neuron 8 has connections 9.\n",
      "Hazard. Layer 3, neuron 9 has connections 9.\n",
      "Hazard. Layer 3, neuron 10 has connections 9.\n",
      "Hazard. Layer 1, neuron 1 has connections 10.\n",
      "Hazard. Layer 1, neuron 2 has connections 10.\n",
      "Hazard. Layer 1, neuron 3 has connections 10.\n",
      "Hazard. Layer 1, neuron 4 has connections 10.\n",
      "Hazard. Layer 1, neuron 5 has connections 10.\n",
      "Hazard. Layer 1, neuron 6 has connections 10.\n",
      "Hazard. Layer 1, neuron 7 has connections 10.\n",
      "Hazard. Layer 1, neuron 8 has connections 10.\n",
      "Hazard. Layer 1, neuron 9 has connections 10.\n",
      "Hazard. Layer 1, neuron 10 has connections 10.\n",
      "Hazard. Layer 1, neuron 11 has connections 10.\n",
      "Hazard. Layer 1, neuron 12 has connections 10.\n",
      "Hazard. Layer 1, neuron 13 has connections 10.\n",
      "Hazard. Layer 1, neuron 14 has connections 10.\n",
      "Hazard. Layer 1, neuron 15 has connections 10.\n",
      "Hazard. Layer 1, neuron 16 has connections 10.\n",
      "Hazard. Layer 1, neuron 17 has connections 10.\n",
      "Hazard. Layer 1, neuron 18 has connections 10.\n",
      "Hazard. Layer 1, neuron 19 has connections 10.\n",
      "Hazard. Layer 1, neuron 20 has connections 10.\n",
      "Hazard. Layer 1, neuron 21 has connections 10.\n",
      "Hazard. Layer 1, neuron 22 has connections 10.\n",
      "Hazard. Layer 1, neuron 23 has connections 10.\n",
      "Hazard. Layer 1, neuron 24 has connections 10.\n",
      "Hazard. Layer 1, neuron 25 has connections 10.\n",
      "Hazard. Layer 1, neuron 26 has connections 10.\n",
      "Hazard. Layer 1, neuron 27 has connections 10.\n",
      "Hazard. Layer 1, neuron 28 has connections 10.\n",
      "Hazard. Layer 1, neuron 29 has connections 10.\n",
      "Hazard. Layer 1, neuron 30 has connections 10.\n",
      "Hazard. Layer 1, neuron 31 has connections 10.\n",
      "Hazard. Layer 1, neuron 32 has connections 10.\n",
      "Hazard. Layer 1, neuron 33 has connections 10.\n",
      "Hazard. Layer 1, neuron 34 has connections 10.\n",
      "Hazard. Layer 1, neuron 35 has connections 10.\n",
      "Hazard. Layer 1, neuron 36 has connections 10.\n",
      "Hazard. Layer 1, neuron 37 has connections 10.\n",
      "Hazard. Layer 1, neuron 38 has connections 10.\n",
      "Hazard. Layer 1, neuron 39 has connections 10.\n",
      "Hazard. Layer 1, neuron 40 has connections 10.\n",
      "Hazard. Layer 1, neuron 41 has connections 10.\n",
      "Hazard. Layer 1, neuron 42 has connections 10.\n",
      "Hazard. Layer 1, neuron 43 has connections 10.\n",
      "Hazard. Layer 1, neuron 44 has connections 10.\n",
      "Hazard. Layer 1, neuron 45 has connections 10.\n",
      "Hazard. Layer 1, neuron 46 has connections 10.\n",
      "Hazard. Layer 1, neuron 47 has connections 10.\n",
      "Hazard. Layer 1, neuron 48 has connections 10.\n",
      "Hazard. Layer 1, neuron 49 has connections 10.\n",
      "Hazard. Layer 1, neuron 50 has connections 10.\n",
      "Hazard. Layer 1, neuron 51 has connections 10.\n",
      "Hazard. Layer 1, neuron 52 has connections 10.\n",
      "Hazard. Layer 1, neuron 53 has connections 10.\n",
      "Hazard. Layer 1, neuron 54 has connections 10.\n",
      "Hazard. Layer 1, neuron 55 has connections 10.\n",
      "Hazard. Layer 1, neuron 56 has connections 10.\n",
      "Hazard. Layer 1, neuron 57 has connections 10.\n",
      "Hazard. Layer 1, neuron 58 has connections 10.\n",
      "Hazard. Layer 1, neuron 59 has connections 10.\n",
      "Hazard. Layer 1, neuron 60 has connections 10.\n",
      "Hazard. Layer 1, neuron 61 has connections 10.\n",
      "Hazard. Layer 1, neuron 62 has connections 10.\n",
      "Hazard. Layer 1, neuron 63 has connections 10.\n",
      "Hazard. Layer 1, neuron 64 has connections 10.\n",
      "Hazard. Layer 1, neuron 65 has connections 10.\n",
      "Hazard. Layer 1, neuron 66 has connections 10.\n",
      "Hazard. Layer 1, neuron 67 has connections 10.\n",
      "Hazard. Layer 1, neuron 68 has connections 10.\n",
      "Hazard. Layer 1, neuron 69 has connections 10.\n",
      "Hazard. Layer 1, neuron 70 has connections 10.\n",
      "Hazard. Layer 1, neuron 71 has connections 10.\n",
      "Hazard. Layer 1, neuron 72 has connections 10.\n",
      "Hazard. Layer 1, neuron 73 has connections 10.\n",
      "Hazard. Layer 1, neuron 74 has connections 10.\n",
      "Hazard. Layer 1, neuron 75 has connections 10.\n",
      "Hazard. Layer 1, neuron 76 has connections 10.\n",
      "Hazard. Layer 1, neuron 77 has connections 10.\n",
      "Hazard. Layer 1, neuron 78 has connections 10.\n",
      "Hazard. Layer 1, neuron 79 has connections 10.\n",
      "Hazard. Layer 1, neuron 80 has connections 10.\n",
      "Hazard. Layer 1, neuron 81 has connections 10.\n",
      "Hazard. Layer 1, neuron 82 has connections 10.\n",
      "Hazard. Layer 1, neuron 83 has connections 10.\n",
      "Hazard. Layer 1, neuron 84 has connections 10.\n",
      "Hazard. Layer 1, neuron 85 has connections 10.\n",
      "Hazard. Layer 1, neuron 86 has connections 10.\n",
      "Hazard. Layer 1, neuron 87 has connections 10.\n",
      "Hazard. Layer 1, neuron 88 has connections 10.\n",
      "Hazard. Layer 1, neuron 89 has connections 10.\n",
      "Hazard. Layer 1, neuron 90 has connections 10.\n",
      "Hazard. Layer 1, neuron 91 has connections 10.\n",
      "Hazard. Layer 1, neuron 92 has connections 10.\n",
      "Hazard. Layer 1, neuron 93 has connections 10.\n",
      "Hazard. Layer 1, neuron 94 has connections 10.\n",
      "Hazard. Layer 1, neuron 95 has connections 10.\n",
      "Hazard. Layer 1, neuron 96 has connections 10.\n",
      "Hazard. Layer 1, neuron 97 has connections 10.\n",
      "Hazard. Layer 1, neuron 98 has connections 10.\n",
      "Hazard. Layer 1, neuron 99 has connections 10.\n",
      "Hazard. Layer 1, neuron 100 has connections 10.\n",
      "Hazard. Layer 1, neuron 101 has connections 10.\n",
      "Hazard. Layer 1, neuron 102 has connections 10.\n",
      "Hazard. Layer 1, neuron 103 has connections 10.\n",
      "Hazard. Layer 1, neuron 104 has connections 10.\n",
      "Hazard. Layer 1, neuron 105 has connections 10.\n",
      "Hazard. Layer 1, neuron 106 has connections 10.\n",
      "Hazard. Layer 1, neuron 107 has connections 10.\n",
      "Hazard. Layer 1, neuron 108 has connections 10.\n",
      "Hazard. Layer 1, neuron 109 has connections 10.\n",
      "Hazard. Layer 1, neuron 110 has connections 10.\n",
      "Hazard. Layer 1, neuron 111 has connections 10.\n",
      "Hazard. Layer 1, neuron 112 has connections 10.\n",
      "Hazard. Layer 1, neuron 113 has connections 10.\n",
      "Hazard. Layer 1, neuron 114 has connections 10.\n",
      "Hazard. Layer 1, neuron 115 has connections 10.\n",
      "Hazard. Layer 1, neuron 116 has connections 10.\n",
      "Hazard. Layer 1, neuron 117 has connections 10.\n",
      "Hazard. Layer 1, neuron 118 has connections 10.\n",
      "Hazard. Layer 1, neuron 119 has connections 10.\n",
      "Hazard. Layer 1, neuron 120 has connections 10.\n",
      "Hazard. Layer 1, neuron 121 has connections 10.\n",
      "Hazard. Layer 1, neuron 122 has connections 10.\n",
      "Hazard. Layer 1, neuron 123 has connections 10.\n",
      "Hazard. Layer 1, neuron 124 has connections 10.\n",
      "Hazard. Layer 1, neuron 125 has connections 10.\n",
      "Hazard. Layer 1, neuron 126 has connections 10.\n",
      "Hazard. Layer 1, neuron 127 has connections 10.\n",
      "Hazard. Layer 1, neuron 128 has connections 10.\n",
      "Hazard. Layer 1, neuron 129 has connections 10.\n",
      "Hazard. Layer 1, neuron 130 has connections 10.\n",
      "Hazard. Layer 1, neuron 131 has connections 10.\n",
      "Hazard. Layer 1, neuron 132 has connections 10.\n",
      "Hazard. Layer 1, neuron 133 has connections 10.\n",
      "Hazard. Layer 1, neuron 134 has connections 10.\n",
      "Hazard. Layer 1, neuron 135 has connections 10.\n",
      "Hazard. Layer 1, neuron 136 has connections 10.\n",
      "Hazard. Layer 1, neuron 137 has connections 10.\n",
      "Hazard. Layer 1, neuron 138 has connections 10.\n",
      "Hazard. Layer 1, neuron 139 has connections 10.\n",
      "Hazard. Layer 1, neuron 140 has connections 10.\n",
      "Hazard. Layer 1, neuron 141 has connections 10.\n",
      "Hazard. Layer 1, neuron 142 has connections 10.\n",
      "Hazard. Layer 1, neuron 143 has connections 10.\n",
      "Hazard. Layer 1, neuron 144 has connections 10.\n",
      "Hazard. Layer 1, neuron 145 has connections 10.\n",
      "Hazard. Layer 1, neuron 146 has connections 10.\n",
      "Hazard. Layer 1, neuron 147 has connections 10.\n",
      "Hazard. Layer 1, neuron 148 has connections 10.\n",
      "Hazard. Layer 1, neuron 149 has connections 10.\n",
      "Hazard. Layer 1, neuron 150 has connections 10.\n",
      "Hazard. Layer 1, neuron 151 has connections 10.\n",
      "Hazard. Layer 1, neuron 152 has connections 10.\n",
      "Hazard. Layer 1, neuron 153 has connections 10.\n",
      "Hazard. Layer 1, neuron 154 has connections 10.\n",
      "Hazard. Layer 1, neuron 155 has connections 10.\n",
      "Hazard. Layer 1, neuron 156 has connections 10.\n",
      "Hazard. Layer 1, neuron 157 has connections 10.\n",
      "Hazard. Layer 1, neuron 158 has connections 10.\n",
      "Hazard. Layer 1, neuron 159 has connections 10.\n",
      "Hazard. Layer 1, neuron 160 has connections 10.\n",
      "Hazard. Layer 1, neuron 161 has connections 10.\n",
      "Hazard. Layer 1, neuron 162 has connections 10.\n",
      "Hazard. Layer 1, neuron 163 has connections 10.\n",
      "Hazard. Layer 1, neuron 164 has connections 10.\n",
      "Hazard. Layer 1, neuron 165 has connections 10.\n",
      "Hazard. Layer 1, neuron 166 has connections 10.\n",
      "Hazard. Layer 1, neuron 167 has connections 10.\n",
      "Hazard. Layer 1, neuron 168 has connections 10.\n",
      "Hazard. Layer 1, neuron 169 has connections 10.\n",
      "Hazard. Layer 1, neuron 170 has connections 10.\n",
      "Hazard. Layer 1, neuron 171 has connections 10.\n",
      "Hazard. Layer 1, neuron 172 has connections 10.\n",
      "Hazard. Layer 1, neuron 173 has connections 10.\n",
      "Hazard. Layer 1, neuron 174 has connections 10.\n",
      "Hazard. Layer 1, neuron 175 has connections 10.\n",
      "Hazard. Layer 1, neuron 176 has connections 10.\n",
      "Hazard. Layer 1, neuron 177 has connections 10.\n",
      "Hazard. Layer 1, neuron 178 has connections 10.\n",
      "Hazard. Layer 1, neuron 179 has connections 10.\n",
      "Hazard. Layer 1, neuron 180 has connections 10.\n",
      "Hazard. Layer 1, neuron 181 has connections 10.\n",
      "Hazard. Layer 1, neuron 182 has connections 10.\n",
      "Hazard. Layer 1, neuron 183 has connections 10.\n",
      "Hazard. Layer 1, neuron 184 has connections 10.\n",
      "Hazard. Layer 1, neuron 185 has connections 10.\n",
      "Hazard. Layer 1, neuron 186 has connections 10.\n",
      "Hazard. Layer 1, neuron 187 has connections 10.\n",
      "Hazard. Layer 1, neuron 188 has connections 10.\n",
      "Hazard. Layer 1, neuron 189 has connections 10.\n",
      "Hazard. Layer 1, neuron 190 has connections 10.\n",
      "Hazard. Layer 1, neuron 191 has connections 10.\n",
      "Hazard. Layer 1, neuron 192 has connections 10.\n",
      "Hazard. Layer 1, neuron 193 has connections 10.\n",
      "Hazard. Layer 1, neuron 194 has connections 10.\n",
      "Hazard. Layer 1, neuron 195 has connections 10.\n",
      "Hazard. Layer 1, neuron 196 has connections 10.\n",
      "Hazard. Layer 1, neuron 197 has connections 10.\n",
      "Hazard. Layer 1, neuron 198 has connections 10.\n",
      "Hazard. Layer 1, neuron 199 has connections 10.\n",
      "Hazard. Layer 1, neuron 200 has connections 10.\n",
      "Hazard. Layer 1, neuron 201 has connections 10.\n",
      "Hazard. Layer 1, neuron 202 has connections 10.\n",
      "Hazard. Layer 1, neuron 203 has connections 10.\n",
      "Hazard. Layer 1, neuron 204 has connections 10.\n",
      "Hazard. Layer 1, neuron 205 has connections 10.\n",
      "Hazard. Layer 1, neuron 206 has connections 10.\n",
      "Hazard. Layer 1, neuron 207 has connections 10.\n",
      "Hazard. Layer 1, neuron 208 has connections 10.\n",
      "Hazard. Layer 1, neuron 209 has connections 10.\n",
      "Hazard. Layer 1, neuron 210 has connections 10.\n",
      "Hazard. Layer 1, neuron 211 has connections 10.\n",
      "Hazard. Layer 1, neuron 212 has connections 10.\n",
      "Hazard. Layer 1, neuron 213 has connections 10.\n",
      "Hazard. Layer 1, neuron 214 has connections 10.\n",
      "Hazard. Layer 1, neuron 215 has connections 10.\n",
      "Hazard. Layer 1, neuron 216 has connections 10.\n",
      "Hazard. Layer 1, neuron 217 has connections 10.\n",
      "Hazard. Layer 1, neuron 218 has connections 10.\n",
      "Hazard. Layer 1, neuron 219 has connections 10.\n",
      "Hazard. Layer 1, neuron 220 has connections 10.\n",
      "Hazard. Layer 1, neuron 221 has connections 10.\n",
      "Hazard. Layer 1, neuron 222 has connections 10.\n",
      "Hazard. Layer 1, neuron 223 has connections 10.\n",
      "Hazard. Layer 1, neuron 224 has connections 10.\n",
      "Hazard. Layer 1, neuron 225 has connections 10.\n",
      "Hazard. Layer 1, neuron 226 has connections 10.\n",
      "Hazard. Layer 1, neuron 227 has connections 10.\n",
      "Hazard. Layer 1, neuron 228 has connections 10.\n",
      "Hazard. Layer 1, neuron 229 has connections 10.\n",
      "Hazard. Layer 1, neuron 230 has connections 10.\n",
      "Hazard. Layer 1, neuron 231 has connections 10.\n",
      "Hazard. Layer 1, neuron 232 has connections 10.\n",
      "Hazard. Layer 1, neuron 233 has connections 10.\n",
      "Hazard. Layer 1, neuron 234 has connections 10.\n",
      "Hazard. Layer 1, neuron 235 has connections 10.\n",
      "Hazard. Layer 1, neuron 236 has connections 10.\n",
      "Hazard. Layer 1, neuron 237 has connections 10.\n",
      "Hazard. Layer 1, neuron 238 has connections 10.\n",
      "Hazard. Layer 1, neuron 239 has connections 10.\n",
      "Hazard. Layer 1, neuron 240 has connections 10.\n",
      "Hazard. Layer 1, neuron 241 has connections 10.\n",
      "Hazard. Layer 1, neuron 242 has connections 10.\n",
      "Hazard. Layer 1, neuron 243 has connections 10.\n",
      "Hazard. Layer 1, neuron 244 has connections 10.\n",
      "Hazard. Layer 1, neuron 245 has connections 10.\n",
      "Hazard. Layer 1, neuron 246 has connections 10.\n",
      "Hazard. Layer 1, neuron 247 has connections 10.\n",
      "Hazard. Layer 1, neuron 248 has connections 10.\n",
      "Hazard. Layer 1, neuron 249 has connections 10.\n",
      "Hazard. Layer 1, neuron 250 has connections 10.\n",
      "Hazard. Layer 1, neuron 251 has connections 10.\n",
      "Hazard. Layer 1, neuron 252 has connections 10.\n",
      "Hazard. Layer 1, neuron 253 has connections 10.\n",
      "Hazard. Layer 1, neuron 254 has connections 10.\n",
      "Hazard. Layer 1, neuron 255 has connections 10.\n",
      "Hazard. Layer 1, neuron 256 has connections 10.\n",
      "Hazard. Layer 1, neuron 257 has connections 10.\n",
      "Hazard. Layer 1, neuron 258 has connections 10.\n",
      "Hazard. Layer 1, neuron 259 has connections 10.\n",
      "Hazard. Layer 1, neuron 260 has connections 10.\n",
      "Hazard. Layer 1, neuron 261 has connections 10.\n",
      "Hazard. Layer 1, neuron 262 has connections 10.\n",
      "Hazard. Layer 1, neuron 263 has connections 10.\n",
      "Hazard. Layer 1, neuron 264 has connections 10.\n",
      "Hazard. Layer 1, neuron 265 has connections 10.\n",
      "Hazard. Layer 1, neuron 266 has connections 10.\n",
      "Hazard. Layer 1, neuron 267 has connections 10.\n",
      "Hazard. Layer 1, neuron 268 has connections 10.\n",
      "Hazard. Layer 1, neuron 269 has connections 10.\n",
      "Hazard. Layer 1, neuron 270 has connections 10.\n",
      "Hazard. Layer 1, neuron 271 has connections 10.\n",
      "Hazard. Layer 1, neuron 272 has connections 10.\n",
      "Hazard. Layer 1, neuron 273 has connections 10.\n",
      "Hazard. Layer 1, neuron 274 has connections 10.\n",
      "Hazard. Layer 1, neuron 275 has connections 10.\n",
      "Hazard. Layer 1, neuron 276 has connections 10.\n",
      "Hazard. Layer 1, neuron 277 has connections 10.\n",
      "Hazard. Layer 1, neuron 278 has connections 10.\n",
      "Hazard. Layer 1, neuron 279 has connections 10.\n",
      "Hazard. Layer 1, neuron 280 has connections 10.\n",
      "Hazard. Layer 1, neuron 281 has connections 10.\n",
      "Hazard. Layer 1, neuron 282 has connections 10.\n",
      "Hazard. Layer 1, neuron 283 has connections 10.\n",
      "Hazard. Layer 1, neuron 284 has connections 10.\n",
      "Hazard. Layer 1, neuron 285 has connections 10.\n",
      "Hazard. Layer 1, neuron 286 has connections 10.\n",
      "Hazard. Layer 1, neuron 287 has connections 10.\n",
      "Hazard. Layer 1, neuron 288 has connections 10.\n",
      "Hazard. Layer 1, neuron 289 has connections 10.\n",
      "Hazard. Layer 1, neuron 290 has connections 10.\n",
      "Hazard. Layer 1, neuron 291 has connections 10.\n",
      "Hazard. Layer 1, neuron 292 has connections 10.\n",
      "Hazard. Layer 1, neuron 293 has connections 10.\n",
      "Hazard. Layer 1, neuron 294 has connections 10.\n",
      "Hazard. Layer 1, neuron 295 has connections 10.\n",
      "Hazard. Layer 1, neuron 296 has connections 10.\n",
      "Hazard. Layer 1, neuron 297 has connections 10.\n",
      "Hazard. Layer 1, neuron 298 has connections 10.\n",
      "Hazard. Layer 1, neuron 299 has connections 10.\n",
      "Hazard. Layer 1, neuron 300 has connections 10.\n",
      "Hazard. Layer 1, neuron 301 has connections 10.\n",
      "Hazard. Layer 1, neuron 302 has connections 10.\n",
      "Hazard. Layer 1, neuron 303 has connections 10.\n",
      "Hazard. Layer 1, neuron 304 has connections 10.\n",
      "Hazard. Layer 1, neuron 305 has connections 10.\n",
      "Hazard. Layer 1, neuron 306 has connections 10.\n",
      "Hazard. Layer 1, neuron 307 has connections 10.\n",
      "Hazard. Layer 1, neuron 308 has connections 10.\n",
      "Hazard. Layer 1, neuron 309 has connections 10.\n",
      "Hazard. Layer 1, neuron 310 has connections 10.\n",
      "Hazard. Layer 1, neuron 311 has connections 10.\n",
      "Hazard. Layer 1, neuron 312 has connections 10.\n",
      "Hazard. Layer 1, neuron 313 has connections 10.\n",
      "Hazard. Layer 1, neuron 314 has connections 10.\n",
      "Hazard. Layer 1, neuron 315 has connections 10.\n",
      "Hazard. Layer 1, neuron 316 has connections 10.\n",
      "Hazard. Layer 1, neuron 317 has connections 10.\n",
      "Hazard. Layer 1, neuron 318 has connections 10.\n",
      "Hazard. Layer 1, neuron 319 has connections 10.\n",
      "Hazard. Layer 1, neuron 320 has connections 10.\n",
      "Hazard. Layer 1, neuron 321 has connections 10.\n",
      "Hazard. Layer 1, neuron 322 has connections 10.\n",
      "Hazard. Layer 1, neuron 323 has connections 10.\n",
      "Hazard. Layer 1, neuron 324 has connections 10.\n",
      "Hazard. Layer 1, neuron 325 has connections 10.\n",
      "Hazard. Layer 1, neuron 326 has connections 10.\n",
      "Hazard. Layer 1, neuron 327 has connections 10.\n",
      "Hazard. Layer 1, neuron 328 has connections 10.\n",
      "Hazard. Layer 1, neuron 329 has connections 10.\n",
      "Hazard. Layer 1, neuron 330 has connections 10.\n",
      "Hazard. Layer 1, neuron 331 has connections 10.\n",
      "Hazard. Layer 1, neuron 332 has connections 10.\n",
      "Hazard. Layer 1, neuron 333 has connections 10.\n",
      "Hazard. Layer 1, neuron 334 has connections 10.\n",
      "Hazard. Layer 1, neuron 335 has connections 10.\n",
      "Hazard. Layer 1, neuron 336 has connections 10.\n",
      "Hazard. Layer 1, neuron 337 has connections 10.\n",
      "Hazard. Layer 1, neuron 338 has connections 10.\n",
      "Hazard. Layer 1, neuron 339 has connections 10.\n",
      "Hazard. Layer 1, neuron 340 has connections 10.\n",
      "Hazard. Layer 1, neuron 341 has connections 10.\n",
      "Hazard. Layer 1, neuron 342 has connections 10.\n",
      "Hazard. Layer 1, neuron 343 has connections 10.\n",
      "Hazard. Layer 1, neuron 344 has connections 10.\n",
      "Hazard. Layer 1, neuron 345 has connections 10.\n",
      "Hazard. Layer 1, neuron 346 has connections 10.\n",
      "Hazard. Layer 1, neuron 347 has connections 10.\n",
      "Hazard. Layer 1, neuron 348 has connections 10.\n",
      "Hazard. Layer 1, neuron 349 has connections 10.\n",
      "Hazard. Layer 1, neuron 350 has connections 10.\n",
      "Hazard. Layer 1, neuron 351 has connections 10.\n",
      "Hazard. Layer 1, neuron 352 has connections 10.\n",
      "Hazard. Layer 1, neuron 353 has connections 10.\n",
      "Hazard. Layer 1, neuron 354 has connections 10.\n",
      "Hazard. Layer 1, neuron 355 has connections 10.\n",
      "Hazard. Layer 1, neuron 356 has connections 10.\n",
      "Hazard. Layer 1, neuron 357 has connections 10.\n",
      "Hazard. Layer 1, neuron 358 has connections 10.\n",
      "Hazard. Layer 1, neuron 359 has connections 10.\n",
      "Hazard. Layer 1, neuron 360 has connections 10.\n",
      "Hazard. Layer 1, neuron 361 has connections 10.\n",
      "Hazard. Layer 1, neuron 362 has connections 10.\n",
      "Hazard. Layer 1, neuron 363 has connections 10.\n",
      "Hazard. Layer 1, neuron 364 has connections 10.\n",
      "Hazard. Layer 1, neuron 365 has connections 10.\n",
      "Hazard. Layer 1, neuron 366 has connections 10.\n",
      "Hazard. Layer 1, neuron 367 has connections 10.\n",
      "Hazard. Layer 1, neuron 368 has connections 10.\n",
      "Hazard. Layer 1, neuron 369 has connections 10.\n",
      "Hazard. Layer 1, neuron 370 has connections 10.\n",
      "Hazard. Layer 1, neuron 371 has connections 10.\n",
      "Hazard. Layer 1, neuron 372 has connections 10.\n",
      "Hazard. Layer 1, neuron 373 has connections 10.\n",
      "Hazard. Layer 1, neuron 374 has connections 10.\n",
      "Hazard. Layer 1, neuron 375 has connections 10.\n",
      "Hazard. Layer 1, neuron 376 has connections 10.\n",
      "Hazard. Layer 1, neuron 377 has connections 10.\n",
      "Hazard. Layer 1, neuron 378 has connections 10.\n",
      "Hazard. Layer 1, neuron 379 has connections 10.\n",
      "Hazard. Layer 1, neuron 380 has connections 10.\n",
      "Hazard. Layer 1, neuron 381 has connections 10.\n",
      "Hazard. Layer 1, neuron 382 has connections 10.\n",
      "Hazard. Layer 1, neuron 383 has connections 10.\n",
      "Hazard. Layer 1, neuron 384 has connections 10.\n",
      "Hazard. Layer 1, neuron 385 has connections 10.\n",
      "Hazard. Layer 1, neuron 386 has connections 10.\n",
      "Hazard. Layer 1, neuron 387 has connections 10.\n",
      "Hazard. Layer 1, neuron 388 has connections 10.\n",
      "Hazard. Layer 1, neuron 389 has connections 10.\n",
      "Hazard. Layer 1, neuron 390 has connections 10.\n",
      "Hazard. Layer 1, neuron 391 has connections 10.\n",
      "Hazard. Layer 1, neuron 392 has connections 10.\n",
      "Hazard. Layer 1, neuron 393 has connections 10.\n",
      "Hazard. Layer 1, neuron 394 has connections 10.\n",
      "Hazard. Layer 1, neuron 395 has connections 10.\n",
      "Hazard. Layer 1, neuron 396 has connections 10.\n",
      "Hazard. Layer 1, neuron 397 has connections 10.\n",
      "Hazard. Layer 1, neuron 398 has connections 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hazard. Layer 1, neuron 399 has connections 10.\n",
      "Hazard. Layer 1, neuron 400 has connections 10.\n",
      "Hazard. Layer 1, neuron 401 has connections 10.\n",
      "Hazard. Layer 1, neuron 402 has connections 10.\n",
      "Hazard. Layer 1, neuron 403 has connections 10.\n",
      "Hazard. Layer 1, neuron 404 has connections 10.\n",
      "Hazard. Layer 1, neuron 405 has connections 10.\n",
      "Hazard. Layer 1, neuron 406 has connections 10.\n",
      "Hazard. Layer 1, neuron 407 has connections 10.\n",
      "Hazard. Layer 1, neuron 408 has connections 10.\n",
      "Hazard. Layer 1, neuron 409 has connections 10.\n",
      "Hazard. Layer 1, neuron 410 has connections 10.\n",
      "Hazard. Layer 1, neuron 411 has connections 10.\n",
      "Hazard. Layer 1, neuron 412 has connections 10.\n",
      "Hazard. Layer 1, neuron 413 has connections 10.\n",
      "Hazard. Layer 1, neuron 414 has connections 10.\n",
      "Hazard. Layer 1, neuron 415 has connections 10.\n",
      "Hazard. Layer 1, neuron 416 has connections 10.\n",
      "Hazard. Layer 1, neuron 417 has connections 10.\n",
      "Hazard. Layer 1, neuron 418 has connections 10.\n",
      "Hazard. Layer 1, neuron 419 has connections 10.\n",
      "Hazard. Layer 1, neuron 420 has connections 10.\n",
      "Hazard. Layer 1, neuron 421 has connections 10.\n",
      "Hazard. Layer 1, neuron 422 has connections 10.\n",
      "Hazard. Layer 1, neuron 423 has connections 10.\n",
      "Hazard. Layer 1, neuron 424 has connections 10.\n",
      "Hazard. Layer 1, neuron 425 has connections 10.\n",
      "Hazard. Layer 1, neuron 426 has connections 10.\n",
      "Hazard. Layer 1, neuron 427 has connections 10.\n",
      "Hazard. Layer 1, neuron 428 has connections 10.\n",
      "Hazard. Layer 1, neuron 429 has connections 10.\n",
      "Hazard. Layer 1, neuron 430 has connections 10.\n",
      "Hazard. Layer 1, neuron 431 has connections 10.\n",
      "Hazard. Layer 1, neuron 432 has connections 10.\n",
      "Hazard. Layer 1, neuron 433 has connections 10.\n",
      "Hazard. Layer 1, neuron 434 has connections 10.\n",
      "Hazard. Layer 1, neuron 435 has connections 10.\n",
      "Hazard. Layer 1, neuron 436 has connections 10.\n",
      "Hazard. Layer 1, neuron 437 has connections 10.\n",
      "Hazard. Layer 1, neuron 438 has connections 10.\n",
      "Hazard. Layer 1, neuron 439 has connections 10.\n",
      "Hazard. Layer 1, neuron 440 has connections 10.\n",
      "Hazard. Layer 1, neuron 441 has connections 10.\n",
      "Hazard. Layer 1, neuron 442 has connections 10.\n",
      "Hazard. Layer 1, neuron 443 has connections 10.\n",
      "Hazard. Layer 1, neuron 444 has connections 10.\n",
      "Hazard. Layer 1, neuron 445 has connections 10.\n",
      "Hazard. Layer 1, neuron 446 has connections 10.\n",
      "Hazard. Layer 1, neuron 447 has connections 10.\n",
      "Hazard. Layer 1, neuron 448 has connections 10.\n",
      "Hazard. Layer 1, neuron 449 has connections 10.\n",
      "Hazard. Layer 1, neuron 450 has connections 10.\n",
      "Hazard. Layer 1, neuron 451 has connections 10.\n",
      "Hazard. Layer 1, neuron 452 has connections 10.\n",
      "Hazard. Layer 1, neuron 453 has connections 10.\n",
      "Hazard. Layer 1, neuron 454 has connections 10.\n",
      "Hazard. Layer 1, neuron 455 has connections 10.\n",
      "Hazard. Layer 1, neuron 456 has connections 10.\n",
      "Hazard. Layer 1, neuron 457 has connections 10.\n",
      "Hazard. Layer 1, neuron 458 has connections 10.\n",
      "Hazard. Layer 1, neuron 459 has connections 10.\n",
      "Hazard. Layer 1, neuron 460 has connections 10.\n",
      "Hazard. Layer 1, neuron 461 has connections 10.\n",
      "Hazard. Layer 1, neuron 462 has connections 10.\n",
      "Hazard. Layer 1, neuron 463 has connections 10.\n",
      "Hazard. Layer 1, neuron 464 has connections 10.\n",
      "Hazard. Layer 1, neuron 465 has connections 10.\n",
      "Hazard. Layer 1, neuron 466 has connections 10.\n",
      "Hazard. Layer 1, neuron 467 has connections 10.\n",
      "Hazard. Layer 1, neuron 468 has connections 10.\n",
      "Hazard. Layer 1, neuron 469 has connections 10.\n",
      "Hazard. Layer 1, neuron 470 has connections 10.\n",
      "Hazard. Layer 1, neuron 471 has connections 10.\n",
      "Hazard. Layer 1, neuron 472 has connections 10.\n",
      "Hazard. Layer 1, neuron 473 has connections 10.\n",
      "Hazard. Layer 1, neuron 474 has connections 10.\n",
      "Hazard. Layer 1, neuron 475 has connections 10.\n",
      "Hazard. Layer 1, neuron 476 has connections 10.\n",
      "Hazard. Layer 1, neuron 477 has connections 10.\n",
      "Hazard. Layer 1, neuron 478 has connections 10.\n",
      "Hazard. Layer 1, neuron 479 has connections 10.\n",
      "Hazard. Layer 1, neuron 480 has connections 10.\n",
      "Hazard. Layer 1, neuron 481 has connections 10.\n",
      "Hazard. Layer 1, neuron 482 has connections 10.\n",
      "Hazard. Layer 1, neuron 483 has connections 10.\n",
      "Hazard. Layer 1, neuron 484 has connections 10.\n",
      "Hazard. Layer 1, neuron 485 has connections 10.\n",
      "Hazard. Layer 1, neuron 486 has connections 10.\n",
      "Hazard. Layer 1, neuron 487 has connections 10.\n",
      "Hazard. Layer 1, neuron 488 has connections 10.\n",
      "Hazard. Layer 1, neuron 489 has connections 10.\n",
      "Hazard. Layer 1, neuron 490 has connections 10.\n",
      "Hazard. Layer 1, neuron 491 has connections 10.\n",
      "Hazard. Layer 1, neuron 492 has connections 10.\n",
      "Hazard. Layer 1, neuron 493 has connections 10.\n",
      "Hazard. Layer 1, neuron 494 has connections 10.\n",
      "Hazard. Layer 1, neuron 495 has connections 10.\n",
      "Hazard. Layer 1, neuron 496 has connections 10.\n",
      "Hazard. Layer 1, neuron 497 has connections 10.\n",
      "Hazard. Layer 1, neuron 498 has connections 10.\n",
      "Hazard. Layer 1, neuron 499 has connections 10.\n",
      "Hazard. Layer 1, neuron 500 has connections 10.\n",
      "Hazard. Layer 1, neuron 501 has connections 10.\n",
      "Hazard. Layer 1, neuron 502 has connections 10.\n",
      "Hazard. Layer 1, neuron 503 has connections 10.\n",
      "Hazard. Layer 1, neuron 504 has connections 10.\n",
      "Hazard. Layer 1, neuron 505 has connections 10.\n",
      "Hazard. Layer 1, neuron 506 has connections 10.\n",
      "Hazard. Layer 1, neuron 507 has connections 10.\n",
      "Hazard. Layer 1, neuron 508 has connections 10.\n",
      "Hazard. Layer 1, neuron 509 has connections 10.\n",
      "Hazard. Layer 1, neuron 510 has connections 10.\n",
      "Hazard. Layer 1, neuron 511 has connections 10.\n",
      "Hazard. Layer 1, neuron 512 has connections 10.\n",
      "Hazard. Layer 3, neuron 1 has connections 10.\n",
      "Hazard. Layer 3, neuron 2 has connections 10.\n",
      "Hazard. Layer 3, neuron 3 has connections 10.\n",
      "Hazard. Layer 3, neuron 4 has connections 10.\n",
      "Hazard. Layer 3, neuron 5 has connections 10.\n",
      "Hazard. Layer 3, neuron 6 has connections 10.\n",
      "Hazard. Layer 3, neuron 7 has connections 10.\n",
      "Hazard. Layer 3, neuron 8 has connections 10.\n",
      "Hazard. Layer 3, neuron 9 has connections 10.\n",
      "Hazard. Layer 3, neuron 10 has connections 10.\n",
      "Hazard. Layer 3, neuron 11 has connections 10.\n",
      "Hazard. Layer 3, neuron 12 has connections 10.\n",
      "Hazard. Layer 3, neuron 13 has connections 10.\n",
      "Hazard. Layer 3, neuron 14 has connections 10.\n",
      "Hazard. Layer 3, neuron 15 has connections 10.\n",
      "Hazard. Layer 3, neuron 16 has connections 10.\n",
      "Hazard. Layer 3, neuron 17 has connections 10.\n",
      "Hazard. Layer 3, neuron 18 has connections 10.\n",
      "Hazard. Layer 3, neuron 19 has connections 10.\n",
      "Hazard. Layer 3, neuron 20 has connections 10.\n",
      "Hazard. Layer 3, neuron 21 has connections 10.\n",
      "Hazard. Layer 3, neuron 22 has connections 10.\n",
      "Hazard. Layer 3, neuron 23 has connections 10.\n",
      "Hazard. Layer 3, neuron 24 has connections 10.\n",
      "Hazard. Layer 3, neuron 25 has connections 10.\n",
      "Hazard. Layer 3, neuron 26 has connections 10.\n",
      "Hazard. Layer 3, neuron 27 has connections 10.\n",
      "Hazard. Layer 3, neuron 28 has connections 10.\n",
      "Hazard. Layer 3, neuron 29 has connections 10.\n",
      "Hazard. Layer 3, neuron 30 has connections 10.\n",
      "Hazard. Layer 3, neuron 31 has connections 10.\n",
      "Hazard. Layer 3, neuron 32 has connections 10.\n",
      "Hazard. Layer 3, neuron 33 has connections 10.\n",
      "Hazard. Layer 3, neuron 34 has connections 10.\n",
      "Hazard. Layer 3, neuron 35 has connections 10.\n",
      "Hazard. Layer 3, neuron 36 has connections 10.\n",
      "Hazard. Layer 3, neuron 37 has connections 10.\n",
      "Hazard. Layer 3, neuron 38 has connections 10.\n",
      "Hazard. Layer 3, neuron 39 has connections 10.\n",
      "Hazard. Layer 3, neuron 40 has connections 10.\n",
      "Hazard. Layer 3, neuron 41 has connections 10.\n",
      "Hazard. Layer 3, neuron 42 has connections 10.\n",
      "Hazard. Layer 3, neuron 43 has connections 10.\n",
      "Hazard. Layer 3, neuron 44 has connections 10.\n",
      "Hazard. Layer 3, neuron 45 has connections 10.\n",
      "Hazard. Layer 3, neuron 46 has connections 10.\n",
      "Hazard. Layer 3, neuron 47 has connections 10.\n",
      "Hazard. Layer 3, neuron 48 has connections 10.\n",
      "Hazard. Layer 3, neuron 49 has connections 10.\n",
      "Hazard. Layer 3, neuron 50 has connections 10.\n",
      "Hazard. Layer 3, neuron 51 has connections 10.\n",
      "Hazard. Layer 3, neuron 52 has connections 10.\n",
      "Hazard. Layer 3, neuron 53 has connections 10.\n",
      "Hazard. Layer 3, neuron 54 has connections 10.\n",
      "Hazard. Layer 3, neuron 55 has connections 10.\n",
      "Hazard. Layer 3, neuron 56 has connections 10.\n",
      "Hazard. Layer 3, neuron 57 has connections 10.\n",
      "Hazard. Layer 3, neuron 58 has connections 10.\n",
      "Hazard. Layer 3, neuron 59 has connections 10.\n",
      "Hazard. Layer 3, neuron 60 has connections 10.\n",
      "Hazard. Layer 3, neuron 61 has connections 10.\n",
      "Hazard. Layer 3, neuron 62 has connections 10.\n",
      "Hazard. Layer 3, neuron 63 has connections 10.\n",
      "Hazard. Layer 3, neuron 64 has connections 10.\n",
      "Hazard. Layer 3, neuron 65 has connections 10.\n",
      "Hazard. Layer 3, neuron 66 has connections 10.\n",
      "Hazard. Layer 3, neuron 67 has connections 10.\n",
      "Hazard. Layer 3, neuron 68 has connections 10.\n",
      "Hazard. Layer 3, neuron 69 has connections 10.\n",
      "Hazard. Layer 3, neuron 70 has connections 10.\n",
      "Hazard. Layer 3, neuron 71 has connections 10.\n",
      "Hazard. Layer 3, neuron 72 has connections 10.\n",
      "Hazard. Layer 3, neuron 73 has connections 10.\n",
      "Hazard. Layer 3, neuron 74 has connections 10.\n",
      "Hazard. Layer 3, neuron 75 has connections 10.\n",
      "Hazard. Layer 3, neuron 76 has connections 10.\n",
      "Hazard. Layer 3, neuron 77 has connections 10.\n",
      "Hazard. Layer 3, neuron 78 has connections 10.\n",
      "Hazard. Layer 3, neuron 79 has connections 10.\n",
      "Hazard. Layer 3, neuron 80 has connections 10.\n",
      "Hazard. Layer 3, neuron 81 has connections 10.\n",
      "Hazard. Layer 3, neuron 82 has connections 10.\n",
      "Hazard. Layer 3, neuron 83 has connections 10.\n",
      "Hazard. Layer 3, neuron 84 has connections 10.\n",
      "Hazard. Layer 3, neuron 85 has connections 10.\n",
      "Hazard. Layer 3, neuron 86 has connections 10.\n",
      "Hazard. Layer 3, neuron 87 has connections 10.\n",
      "Hazard. Layer 3, neuron 88 has connections 10.\n",
      "Hazard. Layer 3, neuron 89 has connections 10.\n",
      "Hazard. Layer 3, neuron 90 has connections 10.\n",
      "Hazard. Layer 3, neuron 91 has connections 10.\n",
      "Hazard. Layer 3, neuron 92 has connections 10.\n",
      "Hazard. Layer 3, neuron 93 has connections 10.\n",
      "Hazard. Layer 3, neuron 94 has connections 10.\n",
      "Hazard. Layer 3, neuron 95 has connections 10.\n",
      "Hazard. Layer 3, neuron 96 has connections 10.\n",
      "Hazard. Layer 3, neuron 97 has connections 10.\n",
      "Hazard. Layer 3, neuron 98 has connections 10.\n",
      "Hazard. Layer 3, neuron 99 has connections 10.\n",
      "Hazard. Layer 3, neuron 100 has connections 10.\n",
      "Hazard. Layer 3, neuron 101 has connections 10.\n",
      "Hazard. Layer 3, neuron 102 has connections 10.\n",
      "Hazard. Layer 3, neuron 103 has connections 10.\n",
      "Hazard. Layer 3, neuron 104 has connections 10.\n",
      "Hazard. Layer 3, neuron 105 has connections 10.\n",
      "Hazard. Layer 3, neuron 106 has connections 10.\n",
      "Hazard. Layer 3, neuron 107 has connections 10.\n",
      "Hazard. Layer 3, neuron 108 has connections 10.\n",
      "Hazard. Layer 3, neuron 109 has connections 10.\n",
      "Hazard. Layer 3, neuron 110 has connections 10.\n",
      "Hazard. Layer 3, neuron 111 has connections 10.\n",
      "Hazard. Layer 3, neuron 112 has connections 10.\n",
      "Hazard. Layer 3, neuron 113 has connections 10.\n",
      "Hazard. Layer 3, neuron 114 has connections 10.\n",
      "Hazard. Layer 3, neuron 115 has connections 10.\n",
      "Hazard. Layer 3, neuron 116 has connections 10.\n",
      "Hazard. Layer 3, neuron 117 has connections 10.\n",
      "Hazard. Layer 3, neuron 118 has connections 10.\n",
      "Hazard. Layer 3, neuron 119 has connections 10.\n",
      "Hazard. Layer 3, neuron 120 has connections 10.\n",
      "Hazard. Layer 3, neuron 121 has connections 10.\n",
      "Hazard. Layer 3, neuron 122 has connections 10.\n",
      "Hazard. Layer 3, neuron 123 has connections 10.\n",
      "Hazard. Layer 3, neuron 124 has connections 10.\n",
      "Hazard. Layer 3, neuron 125 has connections 10.\n",
      "Hazard. Layer 3, neuron 126 has connections 10.\n",
      "Hazard. Layer 3, neuron 127 has connections 10.\n",
      "Hazard. Layer 3, neuron 128 has connections 10.\n",
      "Hazard. Layer 3, neuron 129 has connections 10.\n",
      "Hazard. Layer 3, neuron 130 has connections 10.\n",
      "Hazard. Layer 3, neuron 131 has connections 10.\n",
      "Hazard. Layer 3, neuron 132 has connections 10.\n",
      "Hazard. Layer 3, neuron 133 has connections 10.\n",
      "Hazard. Layer 3, neuron 134 has connections 10.\n",
      "Hazard. Layer 3, neuron 135 has connections 10.\n",
      "Hazard. Layer 3, neuron 136 has connections 10.\n",
      "Hazard. Layer 3, neuron 137 has connections 10.\n",
      "Hazard. Layer 3, neuron 138 has connections 10.\n",
      "Hazard. Layer 3, neuron 139 has connections 10.\n",
      "Hazard. Layer 3, neuron 140 has connections 10.\n",
      "Hazard. Layer 3, neuron 141 has connections 10.\n",
      "Hazard. Layer 3, neuron 142 has connections 10.\n",
      "Hazard. Layer 3, neuron 143 has connections 10.\n",
      "Hazard. Layer 3, neuron 144 has connections 10.\n",
      "Hazard. Layer 3, neuron 145 has connections 10.\n",
      "Hazard. Layer 3, neuron 146 has connections 10.\n",
      "Hazard. Layer 3, neuron 147 has connections 10.\n",
      "Hazard. Layer 3, neuron 148 has connections 10.\n",
      "Hazard. Layer 3, neuron 149 has connections 10.\n",
      "Hazard. Layer 3, neuron 150 has connections 10.\n",
      "Hazard. Layer 3, neuron 151 has connections 10.\n",
      "Hazard. Layer 3, neuron 152 has connections 10.\n",
      "Hazard. Layer 3, neuron 153 has connections 10.\n",
      "Hazard. Layer 3, neuron 154 has connections 10.\n",
      "Hazard. Layer 3, neuron 155 has connections 10.\n",
      "Hazard. Layer 3, neuron 156 has connections 10.\n",
      "Hazard. Layer 3, neuron 157 has connections 10.\n",
      "Hazard. Layer 3, neuron 158 has connections 10.\n",
      "Hazard. Layer 3, neuron 159 has connections 10.\n",
      "Hazard. Layer 3, neuron 160 has connections 10.\n",
      "Hazard. Layer 3, neuron 161 has connections 10.\n",
      "Hazard. Layer 3, neuron 162 has connections 10.\n",
      "Hazard. Layer 3, neuron 163 has connections 10.\n",
      "Hazard. Layer 3, neuron 164 has connections 10.\n",
      "Hazard. Layer 3, neuron 165 has connections 10.\n",
      "Hazard. Layer 3, neuron 166 has connections 10.\n",
      "Hazard. Layer 3, neuron 167 has connections 10.\n",
      "Hazard. Layer 3, neuron 168 has connections 10.\n",
      "Hazard. Layer 3, neuron 169 has connections 10.\n",
      "Hazard. Layer 3, neuron 170 has connections 10.\n",
      "Hazard. Layer 3, neuron 171 has connections 10.\n",
      "Hazard. Layer 3, neuron 172 has connections 10.\n",
      "Hazard. Layer 3, neuron 173 has connections 10.\n",
      "Hazard. Layer 3, neuron 174 has connections 10.\n",
      "Hazard. Layer 3, neuron 175 has connections 10.\n",
      "Hazard. Layer 3, neuron 176 has connections 10.\n",
      "Hazard. Layer 3, neuron 177 has connections 10.\n",
      "Hazard. Layer 3, neuron 178 has connections 10.\n",
      "Hazard. Layer 3, neuron 179 has connections 10.\n",
      "Hazard. Layer 3, neuron 180 has connections 10.\n",
      "Hazard. Layer 3, neuron 181 has connections 10.\n",
      "Hazard. Layer 3, neuron 182 has connections 10.\n",
      "Hazard. Layer 3, neuron 183 has connections 10.\n",
      "Hazard. Layer 3, neuron 184 has connections 10.\n",
      "Hazard. Layer 3, neuron 185 has connections 10.\n",
      "Hazard. Layer 3, neuron 186 has connections 10.\n",
      "Hazard. Layer 3, neuron 187 has connections 10.\n",
      "Hazard. Layer 3, neuron 188 has connections 10.\n",
      "Hazard. Layer 3, neuron 189 has connections 10.\n",
      "Hazard. Layer 3, neuron 190 has connections 10.\n",
      "Hazard. Layer 3, neuron 191 has connections 10.\n",
      "Hazard. Layer 3, neuron 192 has connections 10.\n",
      "Hazard. Layer 3, neuron 193 has connections 10.\n",
      "Hazard. Layer 3, neuron 194 has connections 10.\n",
      "Hazard. Layer 3, neuron 195 has connections 10.\n",
      "Hazard. Layer 3, neuron 196 has connections 10.\n",
      "Hazard. Layer 3, neuron 197 has connections 10.\n",
      "Hazard. Layer 3, neuron 198 has connections 10.\n",
      "Hazard. Layer 3, neuron 199 has connections 10.\n",
      "Hazard. Layer 3, neuron 200 has connections 10.\n",
      "Hazard. Layer 3, neuron 201 has connections 10.\n",
      "Hazard. Layer 3, neuron 202 has connections 10.\n",
      "Hazard. Layer 3, neuron 203 has connections 10.\n",
      "Hazard. Layer 3, neuron 204 has connections 10.\n",
      "Hazard. Layer 3, neuron 205 has connections 10.\n",
      "Hazard. Layer 3, neuron 206 has connections 10.\n",
      "Hazard. Layer 3, neuron 207 has connections 10.\n",
      "Hazard. Layer 3, neuron 208 has connections 10.\n",
      "Hazard. Layer 3, neuron 209 has connections 10.\n",
      "Hazard. Layer 3, neuron 210 has connections 10.\n",
      "Hazard. Layer 3, neuron 211 has connections 10.\n",
      "Hazard. Layer 3, neuron 212 has connections 10.\n",
      "Hazard. Layer 3, neuron 213 has connections 10.\n",
      "Hazard. Layer 3, neuron 214 has connections 10.\n",
      "Hazard. Layer 3, neuron 215 has connections 10.\n",
      "Hazard. Layer 3, neuron 216 has connections 10.\n",
      "Hazard. Layer 3, neuron 217 has connections 10.\n",
      "Hazard. Layer 3, neuron 218 has connections 10.\n",
      "Hazard. Layer 3, neuron 219 has connections 10.\n",
      "Hazard. Layer 3, neuron 220 has connections 10.\n",
      "Hazard. Layer 3, neuron 221 has connections 10.\n",
      "Hazard. Layer 3, neuron 222 has connections 10.\n",
      "Hazard. Layer 3, neuron 223 has connections 10.\n",
      "Hazard. Layer 3, neuron 224 has connections 10.\n",
      "Hazard. Layer 3, neuron 225 has connections 10.\n",
      "Hazard. Layer 3, neuron 226 has connections 10.\n",
      "Hazard. Layer 3, neuron 227 has connections 10.\n",
      "Hazard. Layer 3, neuron 228 has connections 10.\n",
      "Hazard. Layer 3, neuron 229 has connections 10.\n",
      "Hazard. Layer 3, neuron 230 has connections 10.\n",
      "Hazard. Layer 3, neuron 231 has connections 10.\n",
      "Hazard. Layer 3, neuron 232 has connections 10.\n",
      "Hazard. Layer 3, neuron 233 has connections 10.\n",
      "Hazard. Layer 3, neuron 234 has connections 10.\n",
      "Hazard. Layer 3, neuron 235 has connections 10.\n",
      "Hazard. Layer 3, neuron 236 has connections 10.\n",
      "Hazard. Layer 3, neuron 237 has connections 10.\n",
      "Hazard. Layer 3, neuron 238 has connections 10.\n",
      "Hazard. Layer 3, neuron 239 has connections 10.\n",
      "Hazard. Layer 3, neuron 240 has connections 10.\n",
      "Hazard. Layer 3, neuron 241 has connections 10.\n",
      "Hazard. Layer 3, neuron 242 has connections 10.\n",
      "Hazard. Layer 3, neuron 243 has connections 10.\n",
      "Hazard. Layer 3, neuron 244 has connections 10.\n",
      "Hazard. Layer 3, neuron 245 has connections 10.\n",
      "Hazard. Layer 3, neuron 246 has connections 10.\n",
      "Hazard. Layer 3, neuron 247 has connections 10.\n",
      "Hazard. Layer 3, neuron 248 has connections 10.\n",
      "Hazard. Layer 3, neuron 249 has connections 10.\n",
      "Hazard. Layer 3, neuron 250 has connections 10.\n",
      "Hazard. Layer 3, neuron 251 has connections 10.\n",
      "Hazard. Layer 3, neuron 252 has connections 10.\n",
      "Hazard. Layer 3, neuron 253 has connections 10.\n",
      "Hazard. Layer 3, neuron 254 has connections 10.\n",
      "Hazard. Layer 3, neuron 255 has connections 10.\n",
      "Hazard. Layer 3, neuron 256 has connections 10.\n",
      "Hazard. Layer 5, neuron 1 has connections 9.\n",
      "Hazard. Layer 5, neuron 2 has connections 9.\n",
      "Hazard. Layer 5, neuron 3 has connections 9.\n",
      "Hazard. Layer 5, neuron 4 has connections 9.\n",
      "Hazard. Layer 5, neuron 5 has connections 9.\n",
      "Hazard. Layer 5, neuron 6 has connections 9.\n",
      "Hazard. Layer 5, neuron 7 has connections 9.\n",
      "Hazard. Layer 5, neuron 8 has connections 9.\n",
      "Hazard. Layer 5, neuron 9 has connections 9.\n",
      "Hazard. Layer 5, neuron 10 has connections 9.\n"
     ]
    }
   ],
   "source": [
    "#verifying the two input only neurons architecture.\n",
    "weight_masks = l_history.weight_masks\n",
    "for i in range(len(weight_masks)):\n",
    "    for j in range(weight_masks[i].shape[1]):\n",
    "        num_nonz_conns = np.count_nonzero(weight_masks[i][:,j])\n",
    "        if num_nonz_conns != 2:\n",
    "            print('Hazard. Layer {}, neuron {} has connections {}.'.format(i+1, j+1, num_nonz_conns))\n",
    "            \n",
    "trained_weights = model.get_weights()\n",
    "for i in range(len(trained_weights)):\n",
    "    if i%2 == 0:\n",
    "        for j in range(trained_weights[i].shape[1]):\n",
    "            num_nonz_conns = np.count_nonzero(trained_weights[i][:,j])\n",
    "            if num_nonz_conns != 2:\n",
    "                print('Hazard. Layer {}, neuron {} has connections {}.'.format(i+1, j+1, num_nonz_conns))\n",
    "    else:\n",
    "        num_nonz_conns = np.count_nonzero(trained_weights[i])\n",
    "        if num_nonz_conns != 0:\n",
    "                print('Hazard. Layer {} has {} non-zero biases.'.format(i/2+1, num_nonz_conns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.99609375, 0.9921875], [0.0, 0.994140625, 0.98828125], [0.0, 0.9921875, 0.984375], [0.0, 0.990234375, 0.98046875], [0.0, 0.98828125, 0.9765625], [2.491230867374128e-06, 0.986328125, 0.97265625], [0.0, 0.984375, 0.96875], [0.0, 0.982421875, 0.96484375]]\n",
      "[0.098, 0.7844, 0.8547, 0.8731, 0.9114, 0.919, 0.922, 0.9347]\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(model_sparsity)\n",
    "print(model_accs)\n",
    "print(len(l_history.losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(784, 512)\n",
      "(512,)\n",
      "(512, 256)\n",
      "(256,)\n",
      "(256, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "for i in range(len(model.get_weights())):\n",
    "    print(model.get_weights()[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity =1-{non_zero_elements(matrix)}/{matrix.size}\n",
      "Each neuron of respective hidden layer has equal constrained sparsity. \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEYCAYAAAATRII7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzsnXeYVNX5xz/vFlhYll16r4I0aYtC\nUFEsCIgdiQJqQNSIEk1MYvmpGUaDUaOJSIzGWFAEsUXFWEDRlYCoSFEQC71Jl4Vd2GXb+/vj3Jmd\nLTN7t8zOzu75PM99Zu497Xvre8857z1HVBWLxWKxWGoaMZEWYLFYLBZLaVgDZbFYLJYaiTVQFovF\nYqmRWANlsVgslhqJNVAWi8ViqZFYA2WxWCyWGknEDJSIdBYRFZE4F3EnicjS6tBV2xGR2SLy5xDh\nmSLSNUhYyPMgImkicl1V6CyW73QReamq841GRGSiiCyKtI6agIh0E5Gg38mISG8R+VpEMkTkJhF5\nRkT+L0xazhWRrSHCg5YtInHOs7BzkPDrRCStKnQWyzfk8asJuDJQIrJVRHJEpHmx7atDHVhL9KGq\njVR1c6R1WEpHVeeq6nm+def+61YVeYvIn0Uk13lJ8S23VUXeEeIOYJGqJqnqP1X1OlV9oCIZichO\nERleUSGVKbsuU54a1BZgvG9FRPoCDatcUZThpgZoiT4idV7FEMmm97nOS4pv+VvxCCISE2GNbukE\nfOsmor2Pq4fyXjvlucjmANcErP8KeLFY4cki8qKI7BeRbSJyj0+MiMSKyCMickBENgNjSkn7rIjs\nFpFdzttcrBthIvKaiOwRkcMiskRE+gSENRCRRx09h0VkqYg0cMJOF5HPRCRdRHaIyCRne5GmquJN\nW85b680isgHY4Gyb6eRxRERWisiwgPixIvJ/IrLJaW5YKSIdROQJEXm02L4sEJHflbKPT4rII8W2\nve17wxWRO5zjliEiP4jIOSEOWRMRedeJ+4WInFBs37o5/5s5eo6IyJfACYGZiMgIEfneOa7/AKRY\n+LUi8p2IHBKRhSLSqVg5N4rIBuf4PyEiRdIHI9j5FpFTRGRv4HUjIpeJyNfO/xgRudM5DwdF5FUR\naeqE+Zqcp4jIduDjUsptLiL/dfT+LCL/C7i+t4rIXSKy3tnf50UkwQlr4qTb74T9V0TaB+SbJiIz\nRGQZcAzo6lxzm51ztEVEJjpx/deiiCxxsvhaTG3nChFZJyIXBuQdL+aeG+jm2IY45ktF5H4RWQ4c\nBTqKSIqzn7vF1DDuk4CHj5imqe+dfX5fRDo42/9PitbSckXkGScsaJ7OffR359xtBkaF0LsEGAY8\n5ZTRVUReEpHpTvi5zjn7PxHZA/xbRFqKyHsB53eJE/dloC3wvpRRqxSR253z/JOIXBOw3V+2s36n\ncw3vwjxLA/No4VwjR0Tkc6BLsfDeIvKRo/F7ERlbrJzHneOdISLLRaRI+hDarxNzv2Y490jgM/B7\nERkdsF7fOa99nfXTRORz59itEZEzAuKWuHbc6AFAVctcgK3AucAPQC8gFtiJeUNRoLMT70XgbSAJ\n6Az8CExxwm4Evgc6AE2BT5y0cU74m8C/gESgJfAl8GsnbBKwNIS+a50y6wOPAWsCwp4A0oB2ju5T\nnXidgAxMrTAeaAYMcNKkAdcF5FGkfEf3h85+NHC2XeXkEQf8HtgDJDhhfwTWAj0wD/H+TtzBwE9A\njBOvOeYB1aqUfTwD2AGIs94EyMLcOD2csLZOWGfghCDHajZw0Ck7DpgLzC+2b92c//OBV51zchKw\ny3ccHK0ZwOXO8fsdkOc7bsDFwEbM9RIH3AN8Vqyc/wIpmAt2PzAqiObpwEsuz/d6YHTA+pvA753/\ntwKfA+2dtP8CXg44Zoq5hhN957WYjr8ATzn7G495APrOx1ZgHYXX9zLgz05YM2AspsUhCXgNeCsg\n3zRgO9DHOVbJwBGghxPeBugT4lrsFrB+O/BKwPrFwFqX9/mfgdlBwpY6+9jL2fc44B3gn85+tQJW\nUni/j8U8L3o4cacD/ysl307AbuA8Zz1UntMwNaL2zjFdAmiI/VkKTApYfwmY7vw/F3O9PgDUAxoA\nfwX+4exfPeCMgLQ7geEhyvLl53HSX4R5GDcupewLnH3u7Vxrr1L0Ofo68LJzDPo5cdOcsEaY+/Aa\n57gOwtzPPQLKOQCc7Oh4hYB7p5jmboHHD7gQ6Ip5Rp2Neb70c8L+D1O79sUdC6x2/ndwNIzEVHpG\nORqaBbt23FyPqlpuA3UP5iYdhXlAx/kOLObhnwP0Dkj364AD+zFwY0DYeU7aOOdCPE7AQwFjOD4p\n7aYsQ2uKk2+yc7CygP6lxLsLeDNIHmmUbaDOLkPHIV+5mBv14iDxvgNGBNyA7wWJJ5iH2BnO+vXA\nxwEX2j7nHMWXoWs28EzA+vnA98X2rZtzPnOBngFhD1BooK4BPi+mbyeFBup9nAeLsx6DMb6dAso5\nPSD8VeDOIJqnE/wm859vZ/0OnBsJYyiOAW0CjvU5AWnbOPsYR6GB6hri2N2HeQHrVkrYVope3+cD\nm4LkMwA4VOx6uy9gPRFIxzwEGhRLW9q1GGig2mJeHHwPxteB213eO3/G3MPpAUtLJ2wp8KeAuO0w\n91b9gG1XAx86/z8EfhUQFoe5x9sFbGsIrKHwBaKsPJdQ9L48n8oZqGygXrHr+z+U8nKHOwOVCcQG\nbPsZOLmUsl/EeXlx1ntT+ByNxxi6wHP6MIXP0Yk4z8WA8GeBuwPKeSog7CJgXRDNRQxUKeH/BW52\n/nfAvDQlOutvAbc5/+8Gni+WdjEwsbRrpzxLeduR5wATMDfJi8XCmjsHd1vAtm2Yiw7MjbOjWJiP\nTk7a3U4VMR3zdtuyLEFOtf9Bp0p6BPOg8OlpDiQAm0pJ2iHIdrcE7gsi8genenzY0Z/slF9WWS9g\nal84v3NKi6TmTM+nsB9wAqb2g6puBH6LeZDvE5H5ItI2hPY9Af+PYd7KitMC81AJds6KnE9HX2Dc\nTsDMgPP5M8aItQuI40ZHEco432Bu0AtFJBH4JeatfXeApjcDNH0H5GNekHwUOa/F+CumVrjIaX67\ns1h48WPV1tHcUET+JaaZ+QjmQZsiRZuwA4/lUeAKTKvDbjHNsT1D6PKjqj9ham9jRSQFGI1znbhk\nnqqmBCz7guxfJ0wtdG/A8XyCwmPZCXgiIOwAUICp/fiYDXyjqo+6zDPUM6Qi7FXVnID1B508FzvX\n1x/Lmd8BVc0PWA92TYfaj1aYl8NQz8rTfMfHOUZXYF62fJT7vgIQkQvENPn/7OR7Hs59pao7MK1a\nl4lpFj8PmBegaXwxTb9w9tNHqPsqKOUyUKq6DeMscT7mTSOQA5i30U4B2zpiqqNgqqkdioX52IF5\nu2oecGM0VtU+lM0ETDPGuRij0NnZLo6mbIr1nQSUWdp2MFXzQAeQ1qXEUd8fMf1Nt2MeiE1UNQU4\nTGGfTKiyXgIuFpH+mCrwW0Higan2Xy6mL2cI8IZfjOo8VT2dwmbXh0Lk44b9mDe5YOesyPkUESkW\ndwemiTbwYddAVT+rpK5Q5xtV3QUsBy7DvH0HGvwdmOa/QE0JThofShBUNUNVf6+qXTFvprdJ0b6+\n4sfqJ+f/7zFNXUNUtTGmudavubRyVXWhqo7APHi+B/4dTFcp+F56xgHLi+1fZQjUuAPz8Gta7J7t\nFxA+pZTz/wWAiNyDOXc3lCPPUM+Qyu4PqnpEVX+nqp2BS4A7ROTM0uJWklD7sRdjyEM9KxcXO66N\nVHVaZQSJ6Zd/HdNC1sp5hi2i6DXqu66uAJaoqs8Q7sDUoAI1JarqXwPSVuj4VcQTZwqmeeto4Ebn\nzeFVYIaIJDkP0dswD2CcsFtEpL2INAHuDEi7G3MwHhWRxmI6s08IuDhCkYQxbgcxRsXvyqmqBcBz\nwN9EpK3z9j1UROpj3irPFZFfivkOoZmIDHCSrsG8KTQU4zAwxYWGPMxDPU5E/gQ0Dgh/BrhfRLqL\noZ+INHM07gRWYB6kb6hqVrBCVHU1xug+AyxU1XQAEekhImc7+5WNaSYpKENzSJzz+R9gunMcelO0\nM/ddoI8YJ4Q44BaKGvKngLuk0IEhWUTGVUaTQ9DzHcCLmBeGvhR9kXoKc312cjS1EJGL3RbsvGF2\nc4zxYUztK/A43+xc300xzR6vBGjOAtKdME8Z5bQSkYudWuBxTNNRsPO5F9NvEMhbQCqmz614S0eV\n4LxRfwo8EnDPdgvoHH8KuFtEeoHf+eFy5/+FmNrhJaqaXY48XwV+KyLtnPvnjqrcJxG50HnulHZ+\nSzvOFeVV4FoR6emcY//1oKq5mPPnFePgdRLmRcvHAsx9N0GMA0y8iAwWkR6V1FQf0++2H8gXkQuA\n4o5W/8G8GE+j6HU1B7hUjNNUrIgkiMhZEroVxxXlNlCquklVvwoS/BtM7WMzpt1xHsZAgHkDXAh8\nDayiZA3sGswBWo/pv3mdotXWYLyIqQLvctJ+Xiz8DxgHhRWYZqaHME4J2zE1wd8729dgnBcA/o5p\ni9+LeWsoq4lkIfABxilkG8ZIBFZp/4a5KBdh2nGfxXTK+ngB8zAttXmvGPMwtYd5AdvqY5onDmCq\n9y0xfWyVZRqmeWAPpjnmeV+Aqh7AvKE/iDEW3TFNS77wNzHHer7TrLUO09xUWco632AcIzph+hiP\nBWyfibnBF4lIhpN2SDnK7g58hDEYy4F/quonAeHzMOd4M6ZJ1/dB9GOY833AKfODMsqJwbzc/YS5\nNs8EpgaJOx14wWla+SWA85LzBsb7y3+fOQ+PTBEZ6mZnXXAVpr/Md8++hvOSoqqvYa7715zz/w2m\nEx3MG3hL4Ecp9OT7R1l5Ak9i+jZ89/PrVbQfPnpg+sozMdfyTFX9nxP2AMZopIvIbytTiKq+g2m6\n/BTzzPiwWJSpGCeovZhnReB9dxhzHK/C1MT2YGo99SupKR3j6PQm5pq7HNMHFRjnKMZ4diSgpUdV\ntwKXAvdiDNx2zHM1qH0RM2DAY2Xp8nkgWSKI84b4EsaBwJ6QKkBENmGaGD+qpvK2Yjrwq6W8snBq\n8Seq6lVlRrZYXCIi9wEdVXVSdZRnP06LMCISj2mKecYap6pBzHchSinfMtUFnGbEKRRtGrJYKoXT\nrDoZUwOuFsJmoMQrz2H8/fepR08qJVwwTS7nYzpGJ6lHV4VLT03EaZ//CtPsOTnCcmoFYsYs6w1c\n7fRB1ilE5HpMk+IcVV1SVnyLxQ0iMhV4BOMMUVlHJ/flhuulXbxyBqYt98UgBup8TJ/V+Zh+gJnq\n0fL0B1gsFoulFhO28bTUo0swnW3BuBhjvFQ9+jmQIl5x4xRhsVgsljpAJPug2lHU022ns2138Yji\nlRvwfS8xnUH161fKYcVisVjqJMePH1dVjYaBfoEocZJQjz4NPA2Q+HCiHj16tIwUpZOWlsbw4cOr\nUFl4sXrDRzRpBas3nESTVqicXhEJ+p1lTSSSlnQXRb+Wbk/hqBMWi8ViqeNEsga1AJgmXpmPcZI4\nrB4t0bxnsVgslrpJON3MXwaGA83FKzspHIYe9ehTwHsYD76NGDdz62ZtsVgsFj9hM1Dq0fFlhCtw\nc7jKt1gsFkt0EzXeHBaLxWKpBkSeQ2QfIuuChAsijyOyEZFvEEkNl5S6Y6B272bArbfCnj1lx7VY\nLJa6y2zMpLTBGI0ZOLk75vOfJ8MlpPYbqIEDQQTatiXlm2+gTRuzPnBgpJWFJtoMarTptVgspaPu\nBllwpr39HEhBwjPIQlR8B1Uphg6F9eshJ2DizJgYOH4cbrzR/I+NNb9u/1d1vMD/U6fCJjP5bgoY\ngwpw4onw8svu8i5vmIhZKsP995O8di3cfz888UTl8go3PmO6cCG0Lm0uyhqG1WupWbgeZKGyRN10\nG4mJ5fxQd/du6NAB8vPLjluXEamY4d29u6jx95GQAL16Vc5wVjYsWLy330a/+AI59VQYN67m6AoW\ndvvt6OzZyJQp8NhjJeNWxQtGVXLTTehTTyFTp0bFy0r6qFGkRIsxraTeFiI5+818Wj6eRvXpEhFF\nOgP/RUuOo4rIf4EHUV3qrC8G7iD4PIEVxhqoaCPwYVT81/dftfA3EN964G+UnX9LEAKNWiSNak4O\nfPghFBSY7RdeCImJkdcVLOzJJ9EPPkDGjIFbb60aHeVNU56Xi0oafxE5pqqJLiJ2JriB+heQhurL\nzvoPwHC06r9jrf0G6qab4JlnIDe3cFtsLFxyCdx2m9memwt5eUV/S9sWKqyq8sjJMUa1LhITA3Fx\nhTevbwlcD3bj+250X5Olbz2whrF9O+zbV7Lc1q2hW7eShj4UoYx9QUHR7QUFhdvy8wvXCwqKrhcP\nS0+H7OySZfv2OzBfS3TjwpBl5WWTcCgDAbLj4N1F/2DsWeX7UqeKDNQYzGzbvpkoHkd1cLmEuKT2\nG6iBA2HNmpLbBwyA1aurTlhVUZpBjY+H666Dxx+veiNa2bxWrIAtW0ruR8uW0K6d+7xqSw03FHFx\n5lwW/w22LTsbvv66ZD4jR0Lz5oXxfWliYwsNfFxcUSNf3NAHrpdW+/IZ90BD71ugqOH3GfO//x0+\n+aSk3tNPh+uvD26My2O0qyrs44/hyJGSWhMTYdCgipdVkXgV4HgMzD4llkb/foGJfSe6TufKQEnh\nIAuYaef9gyyg+hQiAvwD4+lnBlkIQ/Me1AUD5SNa2pqjzaBWlV5VY7DCaVTnz4fly0uW3b8/DBsW\nOo/K1qQr+CCKKkprVgbz0G/b1r1hLo8hr2heM2bAe++V1PrLX8I//lEyfmxshQ9LTn4Oh7IOkZ6d\nTnp2OoeyA/5nHeLwsUMczjpEetYhjhw7REZWOkecJTPrMAX5+fzvOeh1sGTeP7SOp8fuUvqAg+C6\nBlVDqDsGiigbtThaDKqPaOgYj6TxLygoaoDdGLerr4aNG0vm1amT8Zasrlqym7BaboBVBOLiKIiL\nRWNjyY+LIT82hvxYyI0V8kTJiYGcmAJypIDsmAKOk0cW+eRIAbmxkBsDeTGQG+v8FvsfLEzi4/nt\nklzaZEJgw7MC61pA333un+HRZqBqv5t5tNKmDWtmzmR4NBgngHvv5fCyZaTce2+klQTHZ4QiYUxj\nYqBePbO4ZcMG8xsNxr+ggJfXzGHG3BtZ8Xg2DfIgKw7Oub4+d1z4Fy7uen5QY1eQc5z8nOMU5Byn\nIDfH/M911p1tmpuD5uZSkOP7bxZy88z/vDzTfxuYf34e5OYheYW/Zsmn6Y87qJ9Tslk5LwYyE+OJ\nzcsnNl+JzVfi8yFGFXJziXWa3qv3wZlb6lYB6sXW7kd47d47S/URTQY1Goypw9y1c3ms1QIe6aj8\nodUCfrv21HL1OZRFbn4ux3KPcSz3GFl5Wf7/x3KPkZVbbL1YePFtizcv5njj4zw/AH69Ep4bAMtb\nHWfsij/SYv3D5BXkkVeQR35Bvv9/XkEeSjlaceo5SxXQOgM2z4QGeXAsDrreCnuToDSDkBLfmJb1\nm9AsrjFN4xvTPD6ZJnFJNIlrRJO4JFJiE0mJTSQ5LpHGMQ1IimlA45gGNJL6JBBnjGQlarI/7l1P\n/scf0X2/eWgfj4XZJ8fS6N+z6VE1h6NGYg2Upe4RJcZ07tq53PDODRzjGMMnA7qTKW9PYfXu1Qxp\nNyS0Yck75srY5BXkVbnu+8+EPvvNL0C+5rMnM/QII/Ex8cTGxBIXE1fqEivBw+Ji4sqddtaXs9iT\nhN+YPj/AGCdBWHzNYlISUkhJSKFJgyYk1UsiNqbifVBVwYnAGx//gy4jbyEuTymIEZr/ZSZjq/Bl\npSZiDZTFUkNQVXZl7GL17tWs3rOaB5c+SFZe0QlQj+cf59Hlj1ZZmbESS8P4hv6lQXyDoutxodcD\n00xZMIV9R/exJwljUB3aJbVjxfUrghqRGImpsv1xy4IfFrDt8LYSxrRjckfO6nJWtetxw9izp8H1\n69GnnqLB9VPL7WIejVgDZbFEgAItYNPPm1i1exWr9xiDtGr3Kg4cO+Aq/dheY10bjqDGJr4B8THx\njtdw5fnbyL+ZGl/uMf+2hvENeWjEQ7RJCstQbRVmxjkzuOGdG9iTdMxvTBvGN2TGOTMiK6wsoqh5\nuiqwBspiCTO5+bms37/eb4RW71nN13u+JiMno0TclIQUBrYeSGqbVGavmc3BrJK+xZ2SO/H6L1+v\nDunlwtc3dvfiu9l+eDsdkzsy45wZVdpnVlVEk9YiREnzdFVhDZTFUoUczTnKN3u/KWKM1u1bR05+\nyW9V2ia19Rujga0HMrDNQDold/LXaAa2GVhqjaQmv+VP7DuRiX0nRsUnHdGkta5iDZTFUkF+zvrZ\n31/kM0g/HvyRAi35TVC3pt2MEfIZpDYDaZnYMmT+UfuWb7FUEdZAWSxloKr8lPFTkf6i1btXs+3w\nthJxYyWWfq36+Y3RwDYDGdB6AI3rN65Q2fYt31KXsQbKUqeYu3ZuYY1kTckaic95IbCJbvXu1ew/\ntr9EXg3iGtCvVb8iTXQntTyJhLiE6twli6XWYg2Upc7g/67I6dPZdngb1y24jiVbl5AQl8CqPatc\nOS/4jNGJzU4kLsbeQhZLuLB3l6VWkpWbxZ7MPew9upc9mXvYk7mHOz+6s4jDAUB2XjZPryo6X5vP\neSGwvyjQecFisVQP1kBZooac/BzzIahjcPZmFhqfQEO09+hejhwvZSqFEDxw9gMMbGOMUqtGrcK0\nBxaLpTxYA2WpNGX164QivyCf/cf2lzA6xQ3Onsw9/Jz1s2tN8THxtG7UmlaNWtG6UWtaJ7bm1fWv\nlmq4OiV34q5hd7nO22KxVA/WQFkqRWn9Oje8cwMZxzM4vePpIWs5ezL3sP/ofteDhcZIDK0SWxUa\nnUataZVY8n+rRq1oktCkRJPc8C7Do+67IoulLmMNlKVS3PXRXSX6dY7lHmPqu1Nd59GiYQu/0SnN\n4PiMTrMGzSo1aKf9rshiiS6sgbKUm9z8XBZtWsS8dfPYcWRH0Hg9m/cs0+i0aNiC+Nj4atNuvyuy\nWKIHa6AsrijQAj7b8Rnz1s7j1W9fLXWMuEA6JXfiu5u/qyZ1FoulNmINlCUk6/atY+43c3l53ctF\nRk7o3aI3E/tOpGF8Q+7++G7br2OxWKoca6AsJdiWvo2X173MvLXzWLtvrX97h8YdGH/SeCb0nUC/\nVv38TggtElvYfh2LxVLlWANlAeDAsQO89u1rzFs3j6Xbl/q3N23QlHG9xzGh7wRO73h6qZPL2X4d\ni8USDqyBqsMczTnKgh8WMHftXBZuWuif/rtBXAMu7nkxE06awMhuI6kXWy/CSi0WS13EGqg6Rm5+\nLh9u/pC5a+fy1vdv+fuOYiWWUd1GMbHvRC7ucTFJ9ZMirNRisdR1rIGqAxRoAct3LGfu2rklPPCG\nth/KxL4TGddnXJnzE1ksFkt1Yg1ULSaYB16v5r2Y2Hci4/uOp2uTrhFUaLFYLMEJq4ESr4wCZgKx\nwDPq0QeLhXcEXgBSnDh3qkffC6em2s629G3MXzefuWvnFvHAa9+4PeNPGs/EvhOLeOBZLBZLTSVs\nBkq8Egs8AYwAdgIrxCsL1KPrA6LdA7yqHn1SvNIbeA/oHC5NtZUDxw7w+vrXmbt2bhEPvCYJTfwe\neMM6DSvVA89isVhqKuGsQQ0GNqpHNwOIV+YDFwOBBkoB31zYycBPYdRTq/B54M1bN48PNn5QxAPv\noh4XMaHvBEZ1G2U98CwWS9Qiqu5Gki53xl65HBilHr3OWb8aGKIenRYQpw2wCGgCJALnqkdXlpLX\nDcANAHEPxA36cOGHFdKUmZlJo0aNKpQ2EhTXm1eQx1eHvmLxvsUsPbCU7IJsAGKI4eQmJ3NOq3M4\nvdnpNIxrWCP01mSiSStYveEkmrRC5fSeddZZx1Q1sYolhY1IO0mMB2arRx8VrwwF5ohXTlKPFgRG\nUo8+DTwNkPhwolb0Y9Bo+ZA0cH6lDskdmNR/EgeOHeDV9a9y4NgBf7yh7Ycyoe8EftnnlzXCAy9a\nji9El1awesNJNGmF6NNbGcJpoHYBHQLW2zvbApkCjAJQjy4XryQAzYF9YdRVoyk+v9L2w9u5b8l9\n/nDrgWexWMKKFHVuQ4s6tyElndvQ8Di3hdNArQC6i1e6YAzTlcCEYnG2A+cAs8UrvYAEYH8YNdV4\n7l58d4n5lQAa12/Mp5M+pX+r/tYDz2KxhAcp6dyGyAK0pHMbqk8i4XVuC5tbl3o0D5gGLAS+w3jr\nfSteuU+8cpET7ffA9eKVr4GXgUnqCVOnWJSw/fD2UrdnHM9gQOsB1jhZLJZwMhjYiOpmVHMAn3Nb\nINXm3BbWPijnm6b3im37U8D/9cBp4dQQbXRM7ljko9rA7RaLxRJm2gGBs5DuBIYUizMdWITIb3Cc\n28Ilxn4YU8OYcc4MhKK1JDu/ksViqQqaQxwiXwUsN1Qgm/HAbFTbA+cDc5DwfGRZZg1KvNISU8tp\nC2QB64CvinvaWaqG4Z2Go6jfSNn5lSwWS1VxAPJQPTlElHI5t6G6HAmfc1tQAyVeOQu4E2gKrHYK\nTwAuAU4Qr7wOPKoePVLVouoyH2z8AIALTryA29rcVmfcSS0WS41gBdAdcefchoTXuS1UDep84Hr1\naIlee/FKHHABxtPjjXAIq6u8u+FdAMZ0HwOZERZjsVjqFqp5iPic22KB51D9FpH7gK9QXYBxbvs3\nIr/DOExMIkwjPgQ1UOrRP4YIywPeCoegukxOfg4fbjajZIzuPprNqzdHWJHFYqlzaEnnNrTQuc1x\nOa8W5zbXXnzilV9gvDcSgJnq0TfDJaqusnT7UjJzMjmp5Ul0TO7IZqyBslgsdZegnhfildbFNt0G\nXIpp+ruvZApLZXlvg3lpOb/b+RFWYrFYLJEnVA3qKfHKKuBh9Wg2kA5cDhQA1jEiDPj7n04cE2El\nFovFEnmC1qDUo5dgvPf+K165BvgtUB9ohvHks1Qhmw9t5vsD35NcP5mh7YdGWo7FYrFEnJAfV6lH\n3wFGYoazeBP4UT36uHq0To+XFw7e3/A+AOedcB7xsfERVmOxWCyRJ1Qf1EXilU+ADzAf514BXCxe\nmS9eOaG6BNYV3tvo9D91t/02aChUAAAgAElEQVRPFovFAqH7oP6MGTiwAbBQPToY+L14pTswA/MB\nl6UKOJZ7jI+3fAzA6G6jI6zGYrFYagahDNRh4DLMYID+ISzUoxuwxqlKSduaRnZeNie3PZlWjVpF\nWo7FYrHUCEL1QV2KcYiIoeRQF5YqxLqXWywWS0lCjSRxQLzyD0wz37niFTBjM31Z1+dsqkpU1e9e\nbvufLBaLpZBQg8WeB/wT2EDhaLbtgW7ilZvUo4uqQV+t5/sD37M1fSstGrbglHanRFqOxWKx1BhC\n9UHNBM5Vj24N3OhM4f4e0CuMuuoMvua9Ud1GEROeKVUsFoslKgn1RIzDzKZYnF2A/VCnirDu5RaL\nxVI6oWpQzwErxCvzKZwCuAPGg+/ZcAurCxw5foQl25YQIzGcd8J5kZZjsVgsNYpQQx39BZgICDDU\nWQSY6IRZKslHmz8iryCPUzucStMGTSMtx2KxWGoUIafbUI+uB9aLV5o66z9Xi6o6gnUvt1gsluCE\n8uLrCDwMnI35aFfEK42Bj4E7iztPWMqHqhYaKNv/ZLFYLCUI5STxCmaA2Dbq0e7q0W5AG8xMuvOr\nQ1xtZs2eNezO3E27pHb0a9Uv0nIsFoulxhGqia+5evSVwA3q0Xxgvnjl/vDKqv0E1p5EJMJqLBaL\npeYRykCtFK/8E3iBol58v8LME2WpBNa93GKxWEITykBdA0wBvEA7Z9tO4B2sm3mlOHjsIJ/v/Jz4\nmHjO6XJOpOVYLBZLjSTUWHw5wJPOYqlCFm5aSIEWcFbns0iqnxRpORaLxVIjqdDYOuKVP1W1kLqE\nr/9pTPcxEVZisVgsNZeKDv52XZWqqEPkF+TzwcYPANv/ZLFYLKEI9R3UkWBBmFl2LRVgxU8rOJh1\nkK5NunJisxMjLcdisVhqLKGcJNKBU9Sje4sHiFd2lBLf4oJ3f3Tmfupm3cstFoslFKGa+F4EOgUJ\nmxcGLXUCn3v5mBNt/5PFYrGEIpQX3z0hwu4Ij5zaze6M3azavYoGcQ04s9OZkZZjsVgsNZoynSTE\nK1OKrceKVzzhk1R78TlHnN3lbBrE2248i8ViCUXI0cwdzhGvjMV8tNsUmA18Gk5RtZV3Nzj9T9Z7\nz2KxWMqkTAOlHp0gXrkCWAscBSaoR5e5yVy8MgozdXws8Ix69MFS4vwSmA4o8LV6dIJ7+dFDbn4u\nizYtAqyBslgsFjeUaaDEK92BW4E3gF7A1eKV1erRY2WkiwWeAEZghkhaIV5Z4MwxFZj3XcBp6tFD\n4pWWFd+Vms2yHcvIyMmgd4vedE7pHGk5ljpObm4uO3fuJDs7Oyz5Jycn891334Ul76ommrSCO70J\nCQm0b9+e+Pj48hcgRSsWaMmKBVK0YoGGp2LhponvHeBm9ehi8YoAtwErgD5lpBsMbFSPbgZwpo6/\nGFgfEOd64An16CEA9ei+cuqPGuzkhJaaxM6dO0lKSqJz585h+dwhIyODpKToGMYrmrRC2XpVlYMH\nD7Jz5066dOlSvsylZMUCkQVoYcUCKaxYoHoICV/Fwo2BGqwePQKgHlXgUfHKOy7StaNwFHQwOzuk\nWJwTAcQryzDWerp69IPiGYlXbgBuAIjLiyMtLc1F8SXJzMyscNrK8uqaVwFoe7Staw2R1FsRoklv\nNGmFqtebnJxMs2bNyMzMrLI8A8nPzycjIyMseVc10aQV3OmtV68e6enpFblmBgMbUVOxQIJXLFBT\nsUDDV7EINZLE6erRpT7jFIh69Edndt2O6tF1lSy/OzAcaA8sEa/0VY+mFyvvaeBpgMSHE3X48OEV\nKiwtLY2Kpq0MW9O3su3TbSTVS2LaRdOIj3VX7Y6U3ooSTXqjSStUvd7vvvuOxo0bV1l+xYmmWkk0\naQX3ehMSEhg4cGB5s3ddsUAKKxZoyYpFVRCqBjVWvPIw8AGwEtgPJADdgLMwH/H+PkT6XZj5o3y0\nd7YFshP4Qj2aC2wRr/yIMVgryrMTNZ33N7wPwHknnOfaOFksFktV0xziEPkqYNPTqD5dzmxKVCwQ\n6YsWrVhUBUG/g1KP/g64ANgNjAPux/Q/dQf+pR49Qz0aypCsALqLV7qIV+oBVwILisV5C7OTiFea\nYyzz5ortSs3FTk5osZTk2muvpWXLlpx00kllxk1LS+Ozzz4rNWz69Ok88sgjpYadeuqppW6fNGkS\nr7/+eqnlXHDBBWXqccPw4cP56quvyo5YjRyAPFRPDliKGye3FYsFqOaiugXwVSyqnJAf6qpHf1aP\n/ls9Okk9OlI9eol69C716NKyMlaP5gHTgIXAd8Cr6tFvxSv3iVcucqItBA6KV9YDnwB/VI8erNwu\n1SyycrNYvHkxAKO7jY6wGoulgsydC507Q0yM+Z07t9JZTpo0iQ8+cNcyFMpAhaIiaaKF/Pz8cGS7\nAuiOSBek7IoF4qJiIdK3omIqOt2GK9Sj76lHT1SPnqAeneFs+5N6dIHzX9Wjt6lHe6tH+6pH54dT\nTyT4dNunZOVlkdomlTZJbSItx2IpP3Pnwg03wLZtoGp+b7ih0kbqjDPOoGnTpiW2P/744/Tu3Zt+\n/fpx5ZVXsnXrVp566in+/ve/M2DAAP73v/+VSLN+/XqGDx9O165defzxx/3bGzVqBBjPtmnTptGj\nRw/OPfdc9u0r7Nf/8MMP6dmzJ6mpqfznP//xbz969CjXXnstgwcPZuDAgbz99tsAzJ49m8suu4xR\no0bRvXt3br/99jL3derUqZx88sn06dMHj8cMxPPxxx9zySWXFNFx6aWXArBo0SKGDh1Kamoq48aN\n8zuzdO7cmT/96U+kpqby2muvlVluudGSFQtUv0XkPqRoxQIprFigISsW/0TkS0RuQiS5nHo0qpaG\nDRtqRfnkk08qnLai/Oa93yjT0XsW31PutJHQWxmiSW80aVWter3r168vXDFmp+oXF2zZskX79OlT\nZFubNm00OztbVVUPHTqkqqoej0f/+te/lpqHx+PRoUOHanZ2tu7fv1+bNm2qOTk5qqqamJioqqpv\nvPGGnnvuuZqXl6e7du3S5ORkfe211zQrK0vbtWunP/74oxYUFOi4ceN0zJgxqqp611136Zw5c/w6\nunfvrpmZmfr8889rly5dND09XbOysrRjx466ffv2ErrOPPNMXbFihaqqHjx4UFVV8/Ly9Mwzz9Sv\nv/5aCwoKtEePHrpv3z5VVR0/frwuWLBA9+/fr8OGDdPMzExVVX3wwQfV6/WqqmqnTp30vvvuc3Vs\ni5xjB+CoRuLZDd0V/qKwUWGewgg36cJag6rrqKod3shiKSf9+vVj4sSJvPTSS8TFufkSBsaMGUP9\n+vVp3rw5LVu2ZO/eorMELVmyhPHjxxMbG0vbtm05++yzAfj+++/p1KkT3bt3R0S46qqr/GkWLVrE\ngw8+yIABAxg+fDjZ2dls374dgHPOOYfk5GQSEhLo3bs327ZtC6nv1VdfJTU1lYEDB/Ltt9+yfv16\nRISrr76al156ifT0dJYvX87o0aP5/PPPWb9+PaeddhoDBgzghRdeKJL/ZZdd5uqY1ChUNwD3AHcA\nZwKPI/I9IiF3xs1IEiuB54B5vg9qLe748eCPbD60mWYNmjG43eBIy7FYSkc1dHjnzqZZrzidOsHW\nraUmycjIoKKO2++++y5LlizhnXfeYcaMGaxdu7bMNPXr1/f/j42NJS8vr4KlF6KqvPHGG/To0aPI\n9i+++KJc5W3ZsoVHHnmEFStW0KRJEyZNmuQfwWPy5MlceOGFJCQkMG7cOOLi4lBVRowYwcsvv1xq\nfomJiZXet2pFpB8wGRgDfAhciOoqRNoCy4H/BEvqpgZ1BdAWM1TRfPHKSGdECUsZ+EaPGNVtFLEx\nsRFWY7FUkBkzoGHDotsaNjTbq5iCggJ27NjBWWedxUMPPcThw4fJzMwkKSmpUh/TnnHGGbzyyivk\n5+eze/duPvnkEwB69uzJ9u3b2bRpE0ARozBy5EhmzZqFOgZ89erVFSr7yJEjJCYmkpyczN69e3n/\n/ff9YW3btqVt27b8+c9/ZvLkyQD84he/YNmyZWzcuBEwfWE//vhjhcquIcwCVgH9Ub0Z1VUAqP6E\nqVUFpUwDpR7dqB69G+OpMQ9Tm9omXvGKV0r2cFr8WPdyS61g4kR4+mlTYxIxv08/bbZXgvHjxzN0\n6FB++OEH2rdvz7PPPkt+fj5XXXUVffv2ZeDAgdxyyy2kpKRw4YUX8uabbwZ1kiiLSy+9lO7du9O7\nd2+uueYahg4dCpiPWWfOnMmYMWNITU2lZcvCUXvuvfdecnNz6devH3369OHee++t0H7279+fgQMH\n0rNnTyZMmMBpp51WJHzixIl06NCBXr16AdCiRQtmz57N+PHj6devH0OHDuX777+vUNk1hDdRnYNq\nln+LyK0AqM4JlVC0rOo9IF5/Fe18jAfHXOB04Gr16IAKy64AiYmJevTo0Qqlrc7RAzKOZ9Ds4Wbk\naz77/rCPZg2blTuPuj7aQTiJJq0QnpEkfA/EcBBNozNEWuu0adMYOHAgU6ZMKTsy7vWWdo5F5Jiq\nVm8bocgqVFOLbVuNapnDXLjtg0oHngXuVI8ed4K+EK+cFjxl3WbxlsXkFuRyaodTK2ScLBZL7WfQ\noEEkJiby6KOPRlpK1SMyHpgAdEEk8FuqJOBnN1m4cZEZ5xuRvDjq0Sh0J6ke7OjlFoulLFauXBlp\nCeHkM8xIRM2BQAucAXzjJgM3ThLXiVdSfCvilSbilT+XR2VdQ1ULDZTtf7JYLHUR1W2opqE6FNVP\nA5ZVzgfBZeLGQI0OHF3ccTW3T90QfLP3G3Zl7KJNozYMaF2tXXQWi8VSMxBZ6vxmIHIkYDHrLnBj\noGLFK36nf/FKA6B+iPh1nsDaUzgmg7NYLJYaj+rpzm8Sqo0DFrPuAjd9UHOBxeKV5531ycALFRJc\nR7Du5RaLxeIgcgKwE9XjiAwH+gEv4mJ6DjffQT0EzAB6Ocv96tGHK6e49nIo6xCf7fiMuJg4zu16\nbqTlWCw1Et/HuL1796ZPnz7MnDmz3HkEm86ic+fOHDhwoMT2BQsW8OCDD5aal29Q2eIEm5ajvGzd\nutXVtCK1lDeAfES6YSae7YD5prZMXA10pR59H3i/zIgWFm5aSIEWcGanM2lcP3wzllos1c7u3XDl\nlfDKK9C6daWyiouL49FHHyU1NZWMjAwGDRrEiBEj6N27dxWJLclFF13ERRddVHbEKCQvL8/1uIUR\noADVPEQuBWahOgsRV8NylFmDEq/8QryyQrySKV7JEa/ki9ddB1ddxNf/NKb7mAgrsViqmPvvh6VL\nzW8ladOmDamp5tvNpKQkevXqxa5dZl684cOHc8cddzB48GBOPPFE/8gRWVlZXHnllfTq1YtLL72U\nrKysoPnPmjWL1NRU+vbt6x+FYfbs2UybNg0w4+MNHTqUvn37ct999/nTaYhpOVauXMmZZ57JoEGD\nGDlyJLt37w6pNxhbt25l2LBhpKamkpqa6p+z6pprruGtt97yx5s4cSJvv/02+fn5/PGPf+SUU06h\nX79+PPfcc4D5eHvYsGFcdNFFYTXsVUCu803Ur4D/OttcTS3uxkniH8B4YAPQALgOeKICIms9BVrA\n+xtNRdP2P1miBhF3y5NPQkEB/POfZcZNauy+9WDr1q2sXr2aIUOG+Lfl5eXx5Zdf8thjj+H1egF4\n8sknadiwId999x1erzfkN0TNmzdn1apVTJ06tdTZdm+99VamTp3K2rVraR1QG3zzzTf54YcfWL9+\nPS+++KLfeOTm5vKb3/yG119/nZUrV3Lttddy9913h9QbjJYtW/Lhhx+yatUqXnnlFW655RYApkyZ\nwuzZswE4fPgwn332GWPGjOHZZ58lOTmZFStWsGLFCl544QW2bNkCwKpVq5g5c2ZNH6tvMjAUmIHq\nFkS6ACGHOPLhtolvo3glVj2aDzwvXlkN3FVhubWUr376igPHDtA5pTM9m/eMtByLpcaTmZnJ2LFj\neeyxx2gcYNR8U0oMGjSIrc6I6UuWLPE/zPv160e/fv2C5huYPnASQh/Lli3jjTfeAODKK6/0TyIY\nbFqOH374gXXr1jFixAjAzGbbpk3hBKSl6Q1Gbm4u06ZNY82aNcTGxvqNy5lnnslNN93E/v37eeON\nNxg7dixxcXEsWrSIb775xt8Xlp6ezoYNG6hXrx6DBw+mS5cuIcuLKCKxwN2oFg7caKaJf8hNcjcG\n6ph4pR6wRrzyMObLYDuPVCm8+6Mz91M3615uiSLKGo9z927o2hWcKSIAaNAANm8O2hflZrqN3Nxc\nxo4dy8SJE0vMceSbzqKiU2e4SV+ee1RV6dOnD8uXL69weT7+/ve/06pVK77++msKCgpISEjwh11z\nzTW89NJLzJ8/n+eff95f9qxZsxg5ciRQOBZfWlpazZ96QzUfkU6I1EM1p7zJ3Riaq51404CjGA+M\nseUtqC7gcy8fc6Ltf7LUIu6/3zTtBZKfX6m+KFVlypQp9OrVi9tuu81VmjPOOIN584zz17p16/jm\nG1ej5ZTKaaedxvz58wEzmWBgGaVNy9GjRw/279/vN1C5ubl8++23FSr78OHDtGnThpiYGObMmUN+\nfr4/bNKkSTz22GMA/n6lkSNH8uSTT5KbmwvAhg0bqOiA2RFiM7AMkXsRuc2/uCBkDUq8Egs8oB6d\nCGQDoRtX6zB7M/fy1U9fkRCXwPDOwyMtx2KpOpYvh5xiL785OeD0z1SEZcuWMWfOHPr27cuAAWa0\nlQceeIDzzw/edzt16lQmT55Mr1696NWrF4MGDapw+TNnzmTChAk89NBDjBo1yr/90ksv5eOPP6Z3\n79507NjRPy1HvXr1eP3117nllls4fPgweXl5/Pa3v6VPnz7lLvumm25i7NixvPjii4waNapILahV\nq1b06tWLSy65xL/tuuuuY+vWraSmpqKqNG3alHfeeafC+x4BNjlLDJRzHsuy5oRnOkuZTr2IzGNf\nytKwYUOtKJ988kmF05bF7NWzleno6JdGV1me4dQbDqJJbzRpVa16vevXr6/S/Ipz5MiRsOZfldQk\nrUePHtWuXbtqenp60Dhu9ZZ2joGjWgOe424XN31Qm4Fl4pUFmCY+Y9g8+rdyWcJazrsbnP4n671n\nsVgqwEcffcSUKVP43e9+R3JycqTlVB0iLYDbgT5AYYeb6tllJXVjoCpePasj5ObnsmjTIsAaKIvF\nUjHOPfdctm3bFmkZ4WAu8ApwAXAj5nuo/W4Slmmg1KO236kMlu9czuHjh+nZvCddm3SNtByLxWKp\nSTRD9VlEbkX1U+BTRFa4SehmRt1PgBJ+qOopu3pWV7CTE1osFktQcp3f3YiMAX4CmrpJ6KaJ7w8B\n/xMwLubl/zChFmP7nywWiyUof0YkGfg9MAtoDPzOTUI3TXzFxxNZJl75stwSaynbD29n3b51NKrX\niGGdhkVajsVisdQsVH3j7x0GzipPUjeDxTYNWJqLV0YCtcjFpHK8v8GMvTei6wjqxdaLsBqLJTrI\nzs5m8ODB9O/fnz59+viHGiqLYNNWpKWlccEFF5Sa5rrrrmP9+vUltgcOHlucYNNvlJfp06eXOhZg\nnUKkKyLvIHIAkX2IvI2Iq856N018KzF9UIJp2tsCTKm42tqFnZzQUusZOBDWrCm5fcAAWO1q1oQS\n1K9fn48//phGjRqRm5vL6aefzujRo/nFL35RJF5+fj6xsbEVKsPHM888U6n0NZkaPs2Gj3mYAcYv\nddavBF4GhgRN4eBmwsIu6tGuzm939eh56tGllZJbS8jOy+ajzR8BMLrb6AirsVjCxNChUK9Y60C9\nenDqqRXOUkT8tZTc3Fxyc3P9Y+N17tyZO+64g9TUVF577TVWrlxJ//796d+/P088EXwihczMTC6/\n/HJ69uzJxIkTzUADFJ3Y8Pnnn+fEE09k8ODBLFu2zJ82cPqNe+65p0i+f/3rX/1TXfhqelu3bqVX\nr15cf/319OnTh/POOy/k9B8A//73vznllFPo378/Y8eO5dixY2RkZNClSxf/MEZHjhzxr2/atIlR\no0YxaNAghg0b5p825MYbb+TGG29kyJAh3H777a6PeQRpiOocVPOc5SUCv4cKgZsmvpvFKykB603E\nKzdVQmytYcm2JRzLPcaA1gNo17hdpOVYLBXDzTQbpQ11FGLaDTfTbeTn5zNgwABatmzJiBEjiky3\n0axZM1atWsWVV17J5MmTmTVrFl9//XXI/FavXs1jjz3G+vXr2bx5cxEDBLB79248Hg/Lli1j6dKl\nRZr9AqffCBylfNGiRWzYsIEvv/ySNWvWsHLlSpYsWQKYMfFuvvlmvv32W1JSUvyjowfjsssuY8WK\nFXz99df06tWLZ599lqSkJIYPH8677xpHq/nz53PZZZcRHx/PDTfcwKxZs1i5ciWPPPIIN91U+Njd\nuXMnn332GX/7W1SMl/A+Inci0tkZOPZ24D1EmiIS0pvPzWCx16uncO549egh4PrK6a0dWPdyi6Xi\nxMbGsmbNGnbu3MmXX37JunXr/GFXXHEFYKaWSE9P54wzzgDg6quvDprf4MGDad++PTExMQwYMKDE\ntBdffPEFw4cPp0WLFtSrV89fBpixAcePH1+ijEWLFrFo0SIGDhxIamoq33//PRs2bACgS5cu/nEE\n3UyzsW7dOoYNG0bfvn2ZO3euf7DZ6667zj9y+fPPP8/kyZPJzMzks88+Y9y4cQwYMIBf//rX/gkS\nAcaNG1fpps9q5JfAr4GPgU+AqZhmvpXAV6ESumm8jBWviHpMfdkZQNZ6AxBgoGz/kyWaKWu6DSg6\n5UYZU22Au+k2fKSkpHDWWWfxwQcf+B0gKjKNhG/KC6jYNB2lTb+hqtx11138+te/LrJ969atJcor\nq4lv0qRJvPXWW/Tv35/Zs2eTlpYGmJHVt27dSlpaGvn5+Zx00kkcOXKElJQU1pTW90fFjk+1I3IK\nsAPVLs76rzCfKW0FpqP6c1lZuKlBfQC8Il45R7xyDqZz64OKaq4tbDi4gQ0/b6BJQhOGtC+zr89i\niW7atIHJkyEmxvyGME5u2L9/P+nppmEmKyuLDz/8kJ49S07ymZKSQkpKCkuXmm7vuXPnVrjMIUOG\n8Omnn3Lw4EFyc3N57bXX/GGB028EljFy5Eiee+45MjMzAdi1a1eRaeDLQ0ZGBm3atCE3N7fEflxz\nzTVMmDCByZMnA9C4cWO6dOni16iqZTZxVhkioxD5AZGNiNwZIt5YRBSRk4PE+BeQ48Q9A/gL8ALG\n3fxpN1LcGKg7MFWzqc6yGDPwX53GV3sa1W0UcTE13ovGYqk8994Lp59ufivJ7t27Oeuss+jXrx+n\nnHIKI0aMCOom/vzzz3PzzTczYMAAv+NDRWjTpg3Tp09n6NChnHbaafTq1csfNnPmTJ544gn69u3L\nrl27/NvPO+88JkyY4HeguPzyy8nIyKhQ+ffffz9DhgzhtNNOK2GMJ06cyKFDh/zNjGAM5bPPPut3\nxX/77bcrVG65MDPgPgGMBnoD4xHpXUq8JOBW4IsQucUG1JKuAJ5G9Q1U7wW6udJT1nDnTCeR6cQG\nrMcynYZuhkpnOqOYzg9MZyPTuTNEvLFMR5nOyWXlWVOm2zhvznnKdHTO13OqLM/i1PUpIcJJNGlV\ntdNthJOaoPW1117Tq666ylXcsE63AUMVFgas36VwVynxHlMYo5CmBHluwzqFOOf/9wpnFAlzYUPc\n1KAWAw0C1hsAH5WVyOmrKmKJxVvSEovXlSWuURzNOUra1jQEYeQJIyMtx2KxRDG/+c1vuPPOO7m3\nCmqmZdEc4hD5KmC5oViUdsCOgPWdzrZCRFKBDqi+W0ZxL2MGhn0byAL+56TvhmnmKxM3bVMJ6tFM\n34p6NFO80tBFusHARvXoZgDxynzgYqD4J933Aw8Bf3QjuCaweMticvJzGNJuCC0SW0RajsViiWJm\nzZpVbWUdgDxUg/UZlY1IDPA3YFKZcVVnILIYaAMsCmifjQF+46Y4NwbqqHglVT26CkC8MghjDcui\nNEtcxJtAvMYSq0ffFa8ENVDilRuAGwDi8uL83i/lJTMzs8JpA3nmR/Nleu/43lWSXzCqSm91EU16\no0krVL3e5ORkjhw5UqrnWlWQn59f4b6a6iaatII7vapKdnZ2Ra6ZXUCHgPX2zjYfScBJQBrm2mkN\nLEDkIlRLuoyrfl7Kth/dinFjoH4LvCZe+Qkz3FFrTIdXpRCve0usHn0ax+sj8eFEHT58eIXKTEtL\no6Jp/VpUuWb1NQDcPOJmBrUdVKn8QlEVequTaNIbTVqh6vVu2bKFnJwcmjVrFhYjlZGRQVJSdMxv\nGk1aoWy9qsrBgwdJSUlh4MCB5c1+BdAdkS4Yw3QlMCEg88NAc/+6SBrwh1KNUxXgZjTzFeKVnkAP\nZ9MP6tHcUGkcXFti8RZaYvHKReoJz85WBev2rWPHkR20SmzFwDblPvkWS42gffv27Ny5k/37XU1s\nWm6ys7NJSHA1mk3EiSat4E5vQkIC7du3L3/mqnmITAMWArHAc6h+i8h9wFeoLqiA5Arj1j+6B8bR\nIQFIFa+gHn2xjDQrgO7iLd0Sq6eoJRavscQ12ThBoXv56O6jiRE3PiYWS80jPj6eLl26hC3/tLS0\niry9R4Ro0grVoFf1PeC9Ytv+FCTu8PAJcTcWnwczydQszFweDwMXlZVOPZoH+Czxd8Cr6tFvxSv3\niVfKTF9T8Y1ePqb7mAgrsVgsltqNmxrU5UB/YLV6dLJ4pRXwkpvM1VPSEqundEusnvBa4qogPTud\nZduXESuxjOg6ItJyLBaLpVbjpo0qSz1aAOSJVxoD+yjat1RnWLRpEfmaz+kdTyc5wc7ZaLFYLOHE\nTQ3qK2e6jX9jRp/NBJaHVVUNxQ4Oa7FYLNWHGy8+3yQkT4lXPgAaq0e/Ca+smkeBFvD+RjO9u+1/\nslgslvBTrlFO1aNbwxtHijQAABT6SURBVKSjxrNq9yr2Hd1Hx+SO9G5RcuxEi8VisVQt1k/aJe/+\naIadOr/b+WH7+t5isVgshVgD5RKfe7ntf7JYLJbqIWgTn3hDzxWvnrJnQ6wt7Du6jxW7VlA/tj5n\ndzk70nIsFoulThCqD2oloJjx94qjQNewKKqBLNy4EEUZ3nk4ifWiYKpli8ViqQUENVDq0fCNgxJl\nvLvB6X+yzXsWi8VSbZTpxSdeEWAi0EU9er94pSPQWj36ZdjV1QDyCvJYuGkhYA2UxWKxVCdunCT+\nCQylcKDXDMxMuXWCz3d+Tnp2Oic2O5FuTbtFWo7FYrHUGdwYqCHq0ZuBbAD16CGgXlhV1SD8o0d0\ns7Uni8ViqU7cGKhc8UosxjEC8UoLoCCsqmoQtv/JYrFYIoMbA/U48CbQUrwyA1gKPBBWVTWEnUd2\n8s3eb0iMT+SMTmdEWo7FYrHUKdyMxTdXvLISOAfjcn6JevS7sCurAby/wYy9d27Xc6kfVz/CaiwW\ni6Vu4fZD3X3Ay4FhdeFDXTt6hMVisUQOtx/qdgQOOf9TgO1Arf5O6njecT7c9CEAo7uNjrAai8Vi\nqXsE7YNSj3ZRj3YFPgIuVI82V482Ay4AFlWXwEjxv+3/42juUfq27EuH5Do5P6PFYrFEFDdOEr9w\npm4HQD36PnBq+CTVDHzu5XbuJ4vFYokMbuaD+km8cg/wkrM+EfgpfJJqBnb2XIvFYoksbmpQ44EW\nGFfzN4GWzrZay6afN/HDwR9Irp/M0A5DIy3HYrFY6iRu3Mx/Bm4VryQBqh7NDL+syOKrPY3sNpK4\nmHJNOmyxWCyWKsLNYLF9gReBps76AeBX6tF1YdYWMXzu5bb/yWKxWCKHmya+fwG3qUc7qUc7Ab8H\nng6vrMhxLPcYn2z5BIBR3UZFWI3FYrHUXdwYqET16Ce+FfVoGlBrZ+37eMvHHM8/ziltT6FlYstI\ny7FYLJY6i5sOls3ilXuBOc76VcDm8EmKLNZ7z2KxWGoGbmpQ12K8+P7jLC2cbbUOVbXfP1ksFksN\nwY0X3yHglmrQEnG+O/Ad2w5vo0XDFgxqOyjSciwWi6VOE2qw2AWhEqpHL6p6OZHl3R/N3E+ju48m\nRtxULi0Wi8USLkLVoIYCOzCjmH+BGSi2VuMfvdzOnmuxWOoqIqOAmUAs8AyqDxYLvw24DsgD9gPX\norotHFJCGajWwAjMqBETgHeBl9Wj34ZDSKQ5nH2YpduXEiuxnHfCeZGWY7FYLNWPSCzwBObZvxNY\ngcgCVNcHxFoNnIzqMUSmAg8DV4RDTqjRzPPVox+oR38F/ALYCKSJV6aFQ0ik+WjzR+QV5HFqh1Np\n0qBJpOVYLBZLJBgMbER1M6o5wHzg4iIxVD9B9Ziz9jnQPlxiQjpJiFfqA2MwtajOFE7/Xut4d4Pp\nf7Lu5RaLpbbSHOIQ+Spg09OoBg680A7TteNjJzAkRJZTgPerUGIRQjlJvAicBLwHeGvz0EYFWsD7\nG80xtgbKYrHUVg5AHqonV0lmIlcBJwNnVkl+pRCqBnUVcBS4FbhFvH4fCcEMGts4XKKqmzV71rAn\ncw/tG7enb8u+kZZjsVgskWIXEDhDa3tnW1FEzgXuBs5E9Xi4xAQ1UOrR/2/v3oOkKs88jn9/MiDM\nsGIEN/HGgEiShdXVwGqiRklQI5poktJSM67rlrXEjYm3Sm0l65ZtU8Wul1zcqmxtMsGKZp1cwFsu\nKuoKY2nMIlcFJDEoSLzFuPEGBGHg2T/ed6AZ+jaXM+ec6edT1cXpc06f/vXpoZ8+57z9vv1uZ63i\n3q1BrLB3axAV920NYoVkWoNU0928/KyjzkIa8o0VnXOukqXAZKSJhMJ0IaGR3B7ScYQ+Ws/E7PUk\nwyT2Yx8Vd7cGmQVMAS5SUVN6rLYSmG4FOwa4i9AaZNDtbl7up/ecc43MrAv4MvAQsA6Yj9lapDlI\n3b99vQUYDSxAWoWq/2a2P5Ic7Oh4YL0V7AUAFdXdGmR3c8XSTmgJrUEuTjBPWW9sfYMlLy1hxLAR\nzDxy5mA/vXPOZYvZA4S2B6Xzri+ZPm2woiRZoAasNYiKmg3MBmjqaqKzs7NPgTZv3rzPYx/5wyMY\nxjEHHMOyJ5eVf2BKyuXNsjzlzVNW8LxJylNWyF/e/sjEcLEqVm8NYgVrJ45B1XJzi82YMaNPz9PZ\n2UnPx7bfHVpYth3fxoyP9m27SSmXN8vylDdPWcHzJilPWSF/efsjyQJVV2sQFfe0BrFCcq1Bytm5\naycL1y8E/PqTc85lTZIFaikwWcXKrUFU3NMaxArJtgYpZ8nLS3hz25tMet8kJh80ebCf3jnnXBWJ\nteKzwr6tQaxga1XUHBX3bQ2iolbV6kF9oJWO/eTNy51zLlsSvQZlhX1bg1hhT2sQKwxea5ByvHsj\n55zLroYd9Ojld15m1WurGNU0ilMnJNZTh3POuT5q2ALV3Thi5pEzGdk0MuU0zjnnemrYAtXde8TZ\nk89OOYlzzrlyGrJAbd+5nUeefwSAWUfNSjmNc865chqyQD2x6Qne3f4uUw+eSuuBrWnHcc45V0ZD\nFqju5uXees8557KroQuUX39yzrnsargCteHNDax7Yx0H7H8AJx5xYtpxnHPOVdBwBar76OmMSWcw\nfNjwlNM455yrpPEKVPfghEf59SfnnMuyhipQ7+18j0UbFgEwa7I3L3fOuSxrqAK18q2VbOvaxrRD\npvGB0R9IO45zzrkqGqpALfnTEsCblzvnXB40RIHqWN1B662t3PfKfQAM07CUEznnnKtlyBeojtUd\nzP7FbDa9vWn3vJufvJmO1R0ppnLOOVfLkC9Q1z16HVt3bN1r3tYdW7nu0etSSuScc64eQ75AlR45\n1TPfOedcNgz5AjV+zPhezXfOOZcNQ75AzZ05l+bhzXvNax7ezNyZc1NK5Jxzrh5DvkC1Hd1G+2fa\naR3TihCtY1pp/0w7bUe3pR3NOedcFU1pBxgMbUe30XZ0G52dncyYMSPtOM455+ow5I+gnHPO5ZMX\nKOecc5nkBco551wmeYFyzjmXTWaWq1tzc7P12p13mrW22i7JrLU13M8yz5ucPGU187xJylNWswHJ\nC2yxWp+zcKbBbw3WG3ytzPL9DX4aly8xmFBzm328KWTOj5aWFtuyZUv9D+jogNmzYWtJd0cjR8IN\nN8DZZw94vn67//6Qbdu2PfM878DIU1bwvEnKU1Yon7e5Gdrboa3+n8xI2mpmLVVWGAY8B5wOvAQs\nBS7C7NmSdb4EHIPZ5UgXAp/D7IJevZ568w75AjVhArz4YmJ5nHMuNa2tsHFj3avXUaA+BtyA2afi\n/a8DYPbvJes8FNf5NVIT8BpwMAkUk6H/O6hNVfrcmzp18HLUa+3ayss8b//kKSt43iTlKStUzlvt\n862McdCEtKxkVjtm7SX3DwN+X3L/JeCEHpvZs45ZF9LbwFjgjV6FqcPQL1Djx5c/gmpthTVrBj9P\nLZWO+Dxv/+UpK3jeJOUpK1TOO753fYq+AV2YTR+YUMkb+q345s4N52pLNTeH+VnkeZOTp6zgeZOU\np6wwmHlfBo4ouX94nFd+nXCKbwzwfwMdBPBWfJnkeZOTp6xmnjdJecpqNjit+KDJ4AWDiQYjDJ42\nmNpjnSsMvhunLzSYX3Wb/bilXnB6e+tTgYoWL17c58emwfMmJ09ZzTxvkvKU1ax/eWsWqFB0zjJ4\nzuB5g+vivDkG58TpkQYLYjPzpwyOrLnNPt6G/jUo55xz9TN7AHigx7zrS6a3AecPRpShfw3KOedc\nLiV6BKWizgT+AxgGzLOC3dhj+f7AD4FphItsF1jBNiaZyTnnXD4kdgSlooYB/wnMAqYAF6moKT1W\nuwx40wp2FPBt4Kak8jjnnMuXJE/xHQ+st4K9YAXbDvwEOLfHOucCd8Tpu4CZKkoJZnLOOZcTSZ7i\n69Uvkq1gXSqW/0WyipoNzAZgKybpz33M1AR09fGxafC8yclTVvC8ScpTVuhf3lEDGSRpuWjFZwVr\nB0J3HIW+b0fSMsvRr6g9b3LylBU8b5LylBXyl7c/kjzF16tfJKuY8C+SnXPO5UqSR1BLgckqaiKh\nEF0IfKHHOj8H/h74NXAesMgKlq/u1Z1zziUisSMoK1gX8GXgIWAdMN8KtlZFzVFR58TVbgPGqqj1\nwLXA15LKE7XXXiVTPG9y8pQVPG+S8pQV8pe3z3I3HpRzzrnG4D1JOOecyyQvUM455zKpIQqUpCMk\nLZb0rKS1kq5KO1M1kkZKekrS0zFvMe1MtUgaJmmlpF+mnaUWSRslrZa0SnuPLppJkg6UdJek30ha\npzAsd+ZI+lDcp923dyRdnXauaiRdE/+PrZH0Y0kj085UjaSrYta1Wd+3A6EhrkFJOgQ4xMxWSPoL\nYDnwWTN7NuVoZUkS0GJmmyUNB54ArjKz/005WkWSrgWmAweY2afTzlONpI3AdDMb8CGqkyDpDuBx\nM5snaQTQbGZvpZ2rGknDCK13TzCzMkPBpk/SYYT/W1PM7M+S5gMPmNnt6SYrT9JfE3rkOR7YDiwE\nLjez9akGS1BDHEGZ2atmtiJOv0toVXhYuqkqi0O3bI53h8dbZr9JSDocOBuYl3aWoUbSGOAUQotX\nzGx71otTNBN4PqvFqUQTMEphZNhm4JWU81TzV8ASM9tqZl3AY8DnU86UqIYoUKUkTQCOA5akm6S6\neMpsFfA68IiZZTnvrcA/A7vSDlInAx6WtFzS7LTD1DAR+CPwg3gKdZ6klrRD1eFC4Mdph6jGzF4G\nvgFsAl4F3jazh9NNVdUa4OOSxkpqBs5i784QhpyGKlCSRgN3A1eb2Ttp56nGzHaa2bGEHjiOj4f3\nmSPp08DrZrY87Sy9cLKZfYTQ0/4Vkk5JO1AVTcBHgP8ys+OALST/e8F+iachzwEWpJ2lGknvI3RY\nPRE4FGiRdHG6qSozs3WEER8eJpzeWwXsTDVUwhqmQMVrOXcDHWZ2T9p56hVP5ywGzkw7SwUnAefE\n6zo/AT4p6c50I1UXvzljZq8D9xLO6WfVS8BLJUfQdxEKVpbNAlaY2R/SDlLDacAGM/ujme0A7gFO\nTDlTVWZ2m5lNM7NTgDeB59LOlKSGKFCx0cFtwDoz+1baeWqRdLCkA+P0KOB04DfppirPzL5uZoeb\n2QTCaZ1FZpbZb6GSWmJDGeKpsjMIp04yycxeA34v6UNx1kwgk417SlxExk/vRZuAj0pqjp8RMwnX\npzNL0l/Gf8cTrj/9KN1EycpFb+YD4CTg74DV8boOwL+Y2QMpZqrmEOCO2BJqP2C+mWW++XZOvB+4\nN3we0QT8yMwWphuppq8AHfHU2QvAP6Scp6JY9E8Hvph2llrMbImku4AVhOErVpL9boTuljQW2AFc\nkZMGM33WEM3MnXPO5U9DnOJzzjmXP16gnHPOZZIXKOecc5nkBco551wmeYFyzjmXSV6gHJJM0jdL\n7n9V0g0DtO3bJZ03ENuq8Tznx56+F/eYf2hsSjzQz3espLMGaFtXxuwdA7G9MtsflPfAuYHmBcoB\nvAd8XtK4tIOUih141usy4B/N7BOlM83sFTNL4sP5WEJfaAPhS8DpZtY2QNvLjF6+h87txQuUg/Aj\nxXbgmp4Len77lrQ5/jtD0mOSfibpBUk3SmqL41itljSpZDOnSVom6bnYd193Z7i3SFoq6RlJXyzZ\n7uOSfk6ZHhMkXRS3v0bSTXHe9cDJwG2Sbumx/gRJa+L0pZLukbRQ0u8k3Vz6uiR9O46z86ikg+P8\nTknT4/Q4hbGkRgBzgAsUxj26QNKp2jMO0sru3ip6ZLk25l6jOJaPpO8CRwIPSrqmx/qV9tHomHFF\n3Bfnljzmkrju05L+u2Rzp0h6Mr5X+xTsuJ/WSfp+3AcPx15MkDQp7rPl8b35cB1/G3u9hxVee8Xn\ndA4AM/Nbg9+AzcABwEZgDPBV4Ia47HbgvNJ1478zgLcIvV7sTxj7pxiXXQXcWvL4hYQvQ5MJfcuN\nBGYD/xrX2R9YRui0cwahQ9SJZXIeSuie5mBCLxCLCON6AXQSxnjq+ZgJwJo4fSmhJ4YxMcOLwBFx\nmQFtcfp64Ds9twuMAzaWbOs7Jc/zC+CkOD0aaOqRYxqwGmiJy9cCx8VlG4FxZbJX2kdNhHG3ujOt\nBwRMJfTNNi4uO6jkPVgQ34MpwPoK+6kLODbenw9cHKcfBSbH6RMI3Vl1b7fS38bu97DSa6/2nH7z\nm5n5EZQLLPTu/kPgyl48bKmFsbbeA54n9LIM4cNoQsl6881sl5n9jlAgPkzoA+8Sha6nlgBjCQUM\n4Ckz21Dm+f4W6LTQuWcX0EEYK6k3HjWzt81sG+HbfWucvwv4aZy+k3BE1hu/Ar4l6UrgwJiv1MnA\nvWa2xcJYX/cAH6+xzUr7SMC/SXoG+B/C2GbvBz4JLLA4EKOZ/alkW/fF9+DZuG45G8ysuyuw5cAE\nhREATgQWxBzfI3wpqaX0Paz22vd5zjq27RqEnx92pW4l9Ev2g5J5XcRTwZL2A0aULHuvZHpXyf1d\n7P231bM/LSN8yH7FzB4qXSBpBuHbd1JKM++k8v+B7sy7Xz/hqKv8ymY3SrqfcF3qV5I+ZWb97eC3\n0j66lHAUOc3Mdij0JF9rqPLS16061tkJjCK89rcsDP3SU7W/jXrfw3LP6Rzg16BcifiNez6hwUG3\njYRTNBDG+Bneh02fL2m/eF3qSOC3wEPAPykMg4KkD6r2QHxPAafGa0HDCL1mP9aHPOXsB3RfT/kC\nYShw2Pv1l167eRfYfZ1J0iQzW21mNwFLCUeJpR4HPqvQc3YL8Lk4r5pK+2gMYQyuHZI+wZ6jwEWE\nfT02rn9Q7ZddXTyy3iDp/LhNSfqbuHgj9f1t9OW1O+cFyu3jm4TrGt2+TygKTwMfo29HN5sIxeVB\n4PJ4em0e4RTbitiI4XvUOKI3s1cJg/UtBp4GlpvZz/qQp5wthIEh1xBOlc2J879BKBIr2Xu/LAam\ndDeSAK6ODQCeIfQ0/WCP7CsI12yeIpyum2dmK2tkqrSPOoDpklYDlxCHYjGztcBc4LH4fg3U0DJt\nwGVxm2sJg/xBnX8bfXntki6XdPnAxHd55b2ZO0dogWZmo9PO4Zzbw4+gnHPOZZIfQTnnnMskP4Jy\nzjmXSV6gnHPOZZIXKOecc5nkBco551wmeYFyzjmXSf8PdcTK7PnSZw8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "X_axis = num_neuron_ips\n",
    "\n",
    "ax1.plot(X_axis,model_accs,'go-', linewidth=2)\n",
    "ax1.set_xlabel('Number of inputs of each neuron.')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Model accuracy (x100%)')\n",
    "ax1.set_xticks(X_axis)\n",
    "ax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.title('Model accuracy vs neuron size. Freezed first hidden layer.')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(X_axis,model_accs,'go-', linewidth=2)\n",
    "ax1.set_xlabel('Number of inputs of each neuron.')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Model accuracy (x100%)', color='g')\n",
    "ax1.tick_params('y', colors='g')\n",
    "ax1.set_xticks(X_axis)\n",
    "ax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(X_axis,[elem[0] for elem in model_sparsity],'ro-', linewidth=2, label = '1st hidden layer' ,marker = \"o\")\n",
    "ax2.plot(X_axis,[elem[1] for elem in model_sparsity],'ro-', linewidth=2, label = '2nd hidden layer' ,marker = \"^\")\n",
    "ax2.plot(X_axis,[elem[2] for elem in model_sparsity],'ro-', linewidth=2, label = '3rd hidden layer' ,marker = \"v\")\n",
    "ax2.set_ylabel('Sparsity', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "plt.legend(loc='center right')\n",
    "plt.title('Model accuracy vs hidden layer sparsity. Freezed first hidden layer.')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(X_axis,model_losses,'go-', linewidth=2)\n",
    "ax1.set_xlabel('Number of inputs of each neuron.')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Model loss', color='g')\n",
    "ax1.tick_params('y', colors='g')\n",
    "ax1.set_xticks(X_axis)\n",
    "ax1.set_yticks([0, 0.5, 1.0, 1.5, 2.0, 2.5])\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(X_axis,[elem[0] for elem in model_sparsity],'ro-', linewidth=2, label = '1st hidden layer' ,marker = \"o\")\n",
    "ax2.plot(X_axis,[elem[1] for elem in model_sparsity],'ro-', linewidth=2, label = '2nd hidden layer' ,marker = \"^\")\n",
    "ax2.plot(X_axis,[elem[2] for elem in model_sparsity],'ro-', linewidth=2, label = '3rd hidden layer' ,marker = \"v\")\n",
    "ax2.set_ylabel('Sparsity ', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "plt.legend()\n",
    "plt.title('Model loss vs hidden layer sparsity. Freezed first hidden layer.')\n",
    "fig.tight_layout()\n",
    "print('Sparsity =1-{non_zero_elements(matrix)}/{matrix.size}')\n",
    "print('Each neuron of respective hidden layer has equal constrained sparsity. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
