{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 14:15:57.630483: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 14:15:57.680006: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 14:15:57.681110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 14:15:58.463859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "\n",
    "#os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsampling is made to get rid of host machine's memory and hls4ml vector multiplication size (4096) issue.\n",
    "#### At every layer, the number of weights per neuron x number of neuron should be less than 4096, i.e \n",
    "#### w = layer.get_weights();  layersize = numpy.prod(w.shape); layersize must be less than 4096 \n",
    "##### Taking every 3rd pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n",
      "(10000, 784)\n",
      "(10000,)\n",
      "Sumsampled shape:  (50000, 261) (10000, 261)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('MNIST_dataset.pkl', 'rb') as file:\n",
    "    mnist_dataset = pickle.load(file)\n",
    "\n",
    "for dataset in mnist_dataset:\n",
    "    print(dataset.shape)\n",
    "    \n",
    "X_train_val, X_test, y_train_val, y_test = mnist_dataset[0], mnist_dataset[2], mnist_dataset[1], mnist_dataset[3]\n",
    "\n",
    "#Subsampling every third pixel\n",
    "X_Train_val =  np.zeros((len(X_train_val), 261))\n",
    "X_Test = np.zeros((len(X_test), 261))\n",
    "for i in range(len(X_train_val)):\n",
    "    X_Train_val[i] = X_train_val[i][0:783:3]    \n",
    "for i in range(len(X_test)):\n",
    "    X_Test[i] = X_test[i][0:783:3]   \n",
    "X_train_val = X_Train_val\n",
    "X_test = X_Test\n",
    "print('Sumsampled shape: ',X_train_val.shape, X_test.shape)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_val = le.fit_transform(y_train_val)\n",
    "y_train_val = to_categorical(y_train_val, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_val = scaler.fit_transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/391 [..............................] - ETA: 1:54 - loss: 2.4780 - accuracy: 0.1016WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0012s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 4ms/step - loss: 1.4192 - accuracy: 0.5861 - val_loss: 0.9472 - val_accuracy: 0.7448\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.8575 - accuracy: 0.7575 - val_loss: 0.7593 - val_accuracy: 0.7836\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.7432 - accuracy: 0.7812 - val_loss: 0.6943 - val_accuracy: 0.7934\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6954 - accuracy: 0.7911 - val_loss: 0.6639 - val_accuracy: 0.8020\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6705 - accuracy: 0.7974 - val_loss: 0.6471 - val_accuracy: 0.8030\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.8004 - val_loss: 0.6379 - val_accuracy: 0.8059\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.8028 - val_loss: 0.6319 - val_accuracy: 0.8066\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6399 - accuracy: 0.8050 - val_loss: 0.6276 - val_accuracy: 0.8098\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6352 - accuracy: 0.8060 - val_loss: 0.6248 - val_accuracy: 0.8102\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.8073 - val_loss: 0.6231 - val_accuracy: 0.8103\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6291 - accuracy: 0.8070 - val_loss: 0.6214 - val_accuracy: 0.8107\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6269 - accuracy: 0.8092 - val_loss: 0.6202 - val_accuracy: 0.8118\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.8088 - val_loss: 0.6207 - val_accuracy: 0.8117\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6239 - accuracy: 0.8095 - val_loss: 0.6193 - val_accuracy: 0.8119\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6229 - accuracy: 0.8098 - val_loss: 0.6190 - val_accuracy: 0.8115\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.8093 - val_loss: 0.6183 - val_accuracy: 0.8127\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6211 - accuracy: 0.8106 - val_loss: 0.6193 - val_accuracy: 0.8127\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.8101 - val_loss: 0.6182 - val_accuracy: 0.8132\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.8105 - val_loss: 0.6184 - val_accuracy: 0.8122\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.8106 - val_loss: 0.6187 - val_accuracy: 0.8127\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.8106 - val_loss: 0.6188 - val_accuracy: 0.8135\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.8107 - val_loss: 0.6179 - val_accuracy: 0.8129\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.8105 - val_loss: 0.6183 - val_accuracy: 0.8132\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6178 - accuracy: 0.8113 - val_loss: 0.6183 - val_accuracy: 0.8123\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.8112 - val_loss: 0.6177 - val_accuracy: 0.8145\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.8109 - val_loss: 0.6181 - val_accuracy: 0.8131\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.8112 - val_loss: 0.6183 - val_accuracy: 0.8129\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.8109 - val_loss: 0.6186 - val_accuracy: 0.8144\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6165 - accuracy: 0.8116 - val_loss: 0.6190 - val_accuracy: 0.8141\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.8118 - val_loss: 0.6185 - val_accuracy: 0.8130\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6162 - accuracy: 0.8110 - val_loss: 0.6181 - val_accuracy: 0.8144\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6159 - accuracy: 0.8113 - val_loss: 0.6186 - val_accuracy: 0.8138\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6158 - accuracy: 0.8117 - val_loss: 0.6192 - val_accuracy: 0.8137\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.8111 - val_loss: 0.6199 - val_accuracy: 0.8132\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6156 - accuracy: 0.8111 - val_loss: 0.6195 - val_accuracy: 0.8134\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6153 - accuracy: 0.8114 - val_loss: 0.6194 - val_accuracy: 0.8147\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.8113 - val_loss: 0.6197 - val_accuracy: 0.8135\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.8112 - val_loss: 0.6191 - val_accuracy: 0.8135\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6150 - accuracy: 0.8112 - val_loss: 0.6203 - val_accuracy: 0.8140\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6149 - accuracy: 0.8112 - val_loss: 0.6194 - val_accuracy: 0.8141\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.8116 - val_loss: 0.6207 - val_accuracy: 0.8128\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.8116 - val_loss: 0.6202 - val_accuracy: 0.8125\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.8115 - val_loss: 0.6207 - val_accuracy: 0.8132\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6145 - accuracy: 0.8115 - val_loss: 0.6207 - val_accuracy: 0.8139\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.8118 - val_loss: 0.6207 - val_accuracy: 0.8143\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.8114 - val_loss: 0.6209 - val_accuracy: 0.8133\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6143 - accuracy: 0.8111 - val_loss: 0.6220 - val_accuracy: 0.8139\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6143 - accuracy: 0.8116 - val_loss: 0.6216 - val_accuracy: 0.8142\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6141 - accuracy: 0.8117 - val_loss: 0.6214 - val_accuracy: 0.8150\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.6141 - accuracy: 0.8113 - val_loss: 0.6211 - val_accuracy: 0.8143\n",
      "313/313 - 0s - loss: 0.6211 - accuracy: 0.8143 - 249ms/epoch - 795us/step\n",
      "[0.6211438179016113] [0.814300000667572]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(261,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model_losses = []\n",
    "model_accs = []\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "            self.batches = []\n",
    "            self.weight_save = []\n",
    "            self.model_weights = model.get_weights()\n",
    "            self.weight_masks = []\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i%2 == 0:\n",
    "                    self.random_mask = np.logical_and(np.random.randint(0,2,self.model_weights[i].shape), np.random.randint(0,2,self.model_weights[i].shape))\n",
    "                    for j in range(0):\n",
    "                        self.random_mask = np.logical_and(self.random_mask, np.random.randint(0,2,self.model_weights[i].shape))\n",
    "                    self.weight_masks.append(self.random_mask)\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i == 0:\n",
    "                    self.model_weights[i] = np.multiply(self.model_weights[i], self.weight_masks[int(i/2)])\n",
    "            model.set_weights(self.model_weights)\n",
    "            self.weight_save.append(model.get_weights)       \n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.batches.append(batch)\n",
    "            self.model_weights = model.get_weights()\n",
    "            for i in range(len(self.model_weights)):\n",
    "                if i %2 == 0:\n",
    "                    self.model_weights[i] = np.multiply(self.model_weights[i], self.weight_masks[int(i/2)])\n",
    "            model.set_weights(self.model_weights)\n",
    "            self.weight_save.append(model.get_weights) \n",
    "            \n",
    "l_history=LossHistory()\n",
    "history = model.fit(X_train_val, y_train_val,\n",
    "          batch_size=128, epochs=50, verbose=1,\n",
    "          validation_data=(X_test, y_test), callbacks = [l_history])\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, verbose=2)\n",
    "model_losses.append(loss_and_metrics[0])\n",
    "model_accs.append(loss_and_metrics[1])\n",
    "print(model_losses, model_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sparsity\n",
    "Make a quick check that the model was indeed trained sparse. We'll just make a histogram of the weights of the 1st layer, and hopefully observe a large peak in the bin containing '0'. Note logarithmic y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of zeros = 0.7352490421455938\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = model.layers[0].weights[0].numpy()\n",
    "h, b = np.histogram(w, bins=100)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(b[:-1], h, width=b[1] - b[0])\n",
    "plt.semilogy()\n",
    "print('% of zeros = {}'.format(np.sum(w == 0) / np.size(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsparsed Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.8560 - accuracy: 0.7517 - val_loss: 0.4438 - val_accuracy: 0.8808\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.4182 - accuracy: 0.8845 - val_loss: 0.3629 - val_accuracy: 0.8997\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.3584 - accuracy: 0.8997 - val_loss: 0.3356 - val_accuracy: 0.9086\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.3313 - accuracy: 0.9063 - val_loss: 0.3226 - val_accuracy: 0.9115\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.3159 - accuracy: 0.9105 - val_loss: 0.3171 - val_accuracy: 0.9129\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.3057 - accuracy: 0.9131 - val_loss: 0.3157 - val_accuracy: 0.9157\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2987 - accuracy: 0.9151 - val_loss: 0.3118 - val_accuracy: 0.9157\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2938 - accuracy: 0.9166 - val_loss: 0.3123 - val_accuracy: 0.9166\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2900 - accuracy: 0.9183 - val_loss: 0.3119 - val_accuracy: 0.9169\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2874 - accuracy: 0.9181 - val_loss: 0.3127 - val_accuracy: 0.9174\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2853 - accuracy: 0.9187 - val_loss: 0.3142 - val_accuracy: 0.9165\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2836 - accuracy: 0.9194 - val_loss: 0.3150 - val_accuracy: 0.9169\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2823 - accuracy: 0.9197 - val_loss: 0.3145 - val_accuracy: 0.9181\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2808 - accuracy: 0.9206 - val_loss: 0.3130 - val_accuracy: 0.9185\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2801 - accuracy: 0.9213 - val_loss: 0.3167 - val_accuracy: 0.9164\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2788 - accuracy: 0.9210 - val_loss: 0.3159 - val_accuracy: 0.9191\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2781 - accuracy: 0.9215 - val_loss: 0.3181 - val_accuracy: 0.9188\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2776 - accuracy: 0.9210 - val_loss: 0.3182 - val_accuracy: 0.9183\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2769 - accuracy: 0.9216 - val_loss: 0.3195 - val_accuracy: 0.9182\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2764 - accuracy: 0.9224 - val_loss: 0.3206 - val_accuracy: 0.9187\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2757 - accuracy: 0.9216 - val_loss: 0.3214 - val_accuracy: 0.9186\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2753 - accuracy: 0.9220 - val_loss: 0.3215 - val_accuracy: 0.9186\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2751 - accuracy: 0.9221 - val_loss: 0.3230 - val_accuracy: 0.9185\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2742 - accuracy: 0.9225 - val_loss: 0.3232 - val_accuracy: 0.9192\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.2744 - accuracy: 0.9224 - val_loss: 0.3235 - val_accuracy: 0.9186\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 1s 2ms/step - loss: 0.2740 - accuracy: 0.9218 - val_loss: 0.3256 - val_accuracy: 0.9181\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2737 - accuracy: 0.9223 - val_loss: 0.3269 - val_accuracy: 0.9184\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2732 - accuracy: 0.9227 - val_loss: 0.3273 - val_accuracy: 0.9178\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2730 - accuracy: 0.9224 - val_loss: 0.3287 - val_accuracy: 0.9182\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2729 - accuracy: 0.9226 - val_loss: 0.3289 - val_accuracy: 0.9190\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2728 - accuracy: 0.9222 - val_loss: 0.3292 - val_accuracy: 0.9183\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2724 - accuracy: 0.9224 - val_loss: 0.3317 - val_accuracy: 0.9186\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2723 - accuracy: 0.9224 - val_loss: 0.3306 - val_accuracy: 0.9189\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2720 - accuracy: 0.9227 - val_loss: 0.3307 - val_accuracy: 0.9187\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2720 - accuracy: 0.9230 - val_loss: 0.3313 - val_accuracy: 0.9184\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2720 - accuracy: 0.9231 - val_loss: 0.3332 - val_accuracy: 0.9185\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2716 - accuracy: 0.9232 - val_loss: 0.3332 - val_accuracy: 0.9192\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2717 - accuracy: 0.9229 - val_loss: 0.3343 - val_accuracy: 0.9178\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2717 - accuracy: 0.9228 - val_loss: 0.3353 - val_accuracy: 0.9191\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2713 - accuracy: 0.9227 - val_loss: 0.3347 - val_accuracy: 0.9189\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2712 - accuracy: 0.9227 - val_loss: 0.3370 - val_accuracy: 0.9183\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2712 - accuracy: 0.9232 - val_loss: 0.3379 - val_accuracy: 0.9179\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2711 - accuracy: 0.9228 - val_loss: 0.3379 - val_accuracy: 0.9173\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2710 - accuracy: 0.9234 - val_loss: 0.3387 - val_accuracy: 0.9183\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2708 - accuracy: 0.9229 - val_loss: 0.3393 - val_accuracy: 0.9182\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2709 - accuracy: 0.9225 - val_loss: 0.3383 - val_accuracy: 0.9176\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2707 - accuracy: 0.9235 - val_loss: 0.3419 - val_accuracy: 0.9177\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2706 - accuracy: 0.9229 - val_loss: 0.3417 - val_accuracy: 0.9193\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2710 - accuracy: 0.9226 - val_loss: 0.3403 - val_accuracy: 0.9183\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2706 - accuracy: 0.9232 - val_loss: 0.3410 - val_accuracy: 0.9185\n",
      "313/313 - 0s - loss: 0.3410 - accuracy: 0.9185 - 242ms/epoch - 773us/step\n",
      "[0.3410451114177704] [0.9185000061988831]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "Unsparsemodel = Sequential()\n",
    "Unsparsemodel.add(Dense(10, input_shape=(261,)))\n",
    "Unsparsemodel.add(Activation('softmax'))\n",
    "\n",
    "Unsparsemodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model_losses = []\n",
    "model_accs = []\n",
    "            \n",
    "history = Unsparsemodel.fit(X_train_val, y_train_val,\n",
    "          batch_size=128, epochs=50, verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "loss_and_metrics = Unsparsemodel.evaluate(X_test, y_test, verbose=2)\n",
    "model_losses.append(loss_and_metrics[0])\n",
    "model_accs.append(loss_and_metrics[1])\n",
    "print(model_losses, model_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check percentage of zeros in this unsparsed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of zeros = 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = Unsparsemodel.layers[0].weights[0].numpy()\n",
    "h, b = np.histogram(w, bins=100)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.bar(b[:-1], h, width=b[1] - b[0])\n",
    "plt.semilogy()\n",
    "print('% of zeros = {}'.format(np.sum(w == 0) / np.size(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the model to FPGA firmware with hls4ml\n",
    "Let's use the default configuration: `ap_fixed<16,6>` precision everywhere and `ReuseFactor=1`, so we can compare with the part 1 model. We need to use `strip_pruning` to change the layer types back to their originals.\n",
    "\n",
    "**The synthesis will take a while**\n",
    "\n",
    "While the C-Synthesis is running, we can monitor the progress looking at the log file by opening a terminal from the notebook home, and executing:\n",
    "\n",
    "`tail -f model_2/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: dense_48_input, layer type: InputLayer, input shapes: [[None, 261]], output shape: [None, 261]\n",
      "Layer name: dense_48, layer type: Dense, input shapes: [[None, 261]], output shape: [None, 10]\n",
      "Layer name: activation_46, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "{'Model': {'Precision': 'fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}}\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: dense_48_input, layer type: InputLayer, input shapes: [[None, 261]], output shape: [None, 261]\n",
      "Layer name: dense_48, layer type: Dense, input shapes: [[None, 261]], output shape: [None, 10]\n",
      "Layer name: activation_46, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2018.3 (64-bit)\n",
      "  **** SW Build 2405991 on Thu Dec  6 23:36:41 MST 2018\n",
      "  **** IP Build 2404404 on Fri Dec  7 01:43:56 MST 2018\n",
      "    ** Copyright 1986-2018 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /home/rrk307/Xilinx/Vivado/2018.3/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/home/rrk307/Xilinx/Vivado/2018.3/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'rrk307' on host 'ADUAED10599LPLX' (Linux_x86_64 version 5.4.0-150-generic) on Mon Aug 07 16:08:30 +04 2023\n",
      "INFO: [HLS 200-10] On os Ubuntu 18.04.6 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/rrk307/model_Sparse_MNIST/hls4ml_prj'\n",
      "INFO: [HLS 200-10] Opening project '/home/rrk307/model_Sparse_MNIST/hls4ml_prj/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Opening solution '/home/rrk307/model_Sparse_MNIST/hls4ml_prj/myproject_prj/solution1'.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "INFO: [HLS 200-10] Setting target device to 'xc7a200tfbg676-2'\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:17 ; elapsed = 00:00:18 . Memory (MB): peak = 437.949 ; gain = 0.078 ; free physical = 10398 ; free virtual = 11462\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:17 ; elapsed = 00:00:18 . Memory (MB): peak = 437.949 ; gain = 0.078 ; free physical = 10398 ; free virtual = 11462\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' into 'myproject' (firmware/myproject.cpp:35).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::softmax<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' into 'myproject' (firmware/myproject.cpp:37).\n",
      "INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:19 ; elapsed = 00:00:20 . Memory (MB): peak = 566.992 ; gain = 129.121 ; free physical = 10339 ; free virtual = 11426\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:66) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:239) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:254) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:262) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:265) automatically.\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:19 ; elapsed = 00:00:20 . Memory (MB): peak = 566.992 ; gain = 129.121 ; free physical = 10328 ; free virtual = 11416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:217:46).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:17:48).\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:243) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-2' (firmware/nnet_utils/nnet_activation.h:252) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-3' (firmware/nnet_utils/nnet_activation.h:266) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 261.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 261.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [XFORM 203-102] Partitioning array 'd_xi_xmax.V' (firmware/nnet_utils/nnet_activation.h:242) automatically.\n",
      "INFO: [XFORM 203-131] Reshaping array 'dense_48_input.V' (firmware/myproject.cpp:7) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer4_out.V' (firmware/myproject.cpp:8) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer2_out.V' (firmware/myproject.cpp:33) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b2.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:17) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:18) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'exp_res.V' (firmware/nnet_utils/nnet_activation.h:249) in dimension 1 completely.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:66) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:239) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:254) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:265) automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0' by setting 'weights.V' to 'w2.V'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0' by setting 'biases[0].V' to 'b2.V.0'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0' by setting 'biases[1].V' to 'b2.V.1'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0' by setting 'biases[2].V' to 'b2.V.2'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0' by setting 'biases[3].V' to 'b2.V.3'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0' by setting 'biases[4].V' to 'b2.V.4'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0' by setting 'biases[5].V' to 'b2.V.5'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0' by setting 'biases[6].V' to 'b2.V.6'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0' by setting 'biases[7].V' to 'b2.V.7'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0' by setting 'biases[8].V' to 'b2.V.8'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0.0' by setting 'biases[9].V' to 'b2.V.9'.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation.h:249) to (firmware/nnet_utils/nnet_activation.h:270:1) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>'... converting 41 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:45:44) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:43:16) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:45:44) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 13 basic blocks.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:262) automatically.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0.0' (firmware/nnet_utils/nnet_dense_latency.h:26:27)...691 expression(s) balanced.\n",
      "INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:01:59 ; elapsed = 00:02:03 . Memory (MB): peak = 7413.895 ; gain = 6976.023 ; free physical = 7265 ; free virtual = 8357\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' to 'softmax_stable<ap_fixed,ap_fixed<16,6,5,3,0>,softmax_config4>' (firmware/nnet_utils/nnet_activation.h:45:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' (firmware/nnet_utils/nnet_common.h:36:44)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0.0' to 'dense_latency<ap_fixed,ap_fixed<16,6,5,3,0>,config2>.0.0.0.0.0.0.0.0.0.0.0' (firmware/nnet_utils/nnet_mult.h:26:9)\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:02:04 ; elapsed = 00:02:08 . Memory (MB): peak = 7413.895 ; gain = 6976.023 ; free physical = 7273 ; free virtual = 8357\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_latency<ap_fixed,ap_fixed<16,6,5,3,0>,config2>.0.0.0.0.0.0.0.0.0.0.0' to 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' to 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'softmax_stable<ap_fixed,ap_fixed<16,6,5,3,0>,softmax_config4>' to 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [SCHED 204-61] Pipelining function 'dense_latency<ap_fixed,ap_fixed<16,6,5,3,0>,config2>.0.0.0.0.0.0.0.0.0.0.0'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 8.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 143.26 seconds; current allocated memory: 214.141 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 8.59 seconds; current allocated memory: 255.161 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 6.87 seconds; current allocated memory: 257.301 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.1 seconds; current allocated memory: 257.605 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'softmax_stable<ap_fixed,ap_fixed<16,6,5,3,0>,softmax_config4>'.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 5, Depth = 27.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.33 seconds; current allocated memory: 258.702 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.46 seconds; current allocated memory: 260.078 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'myproject'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 5, Depth = 36.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.68 seconds; current allocated memory: 260.514 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.07 seconds; current allocated memory: 261.896 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-104] Estimated max fanout for 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0' is 22222 from HDL expression: ((1'b0 == ap_block_pp0_stage0_11001) & (1'b1 == ap_ce_reg))\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_10ns_26_2_0': 60 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_10s_26_2_0': 62 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_11ns_26_2_0': 31 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_11s_26_2_0': 65 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_12ns_26_2_0': 3 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_12s_26_2_0': 47 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_5ns_21_2_0': 3 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_5s_21_2_0': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_6ns_22_2_0': 9 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_6s_22_2_0': 9 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_7ns_23_2_0': 21 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_7s_23_2_0': 23 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_8ns_24_2_0': 45 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_8s_24_2_0': 47 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_9ns_25_2_0': 64 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_16s_9s_25_2_0': 62 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.87 seconds; current allocated memory: 295.619 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mux_104_18_1_0': 4 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 11.88 seconds; current allocated memory: 359.119 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_17ns_18s_26_3_1': 10 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.97 seconds; current allocated memory: 363.086 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/dense_48_input_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_0_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_1_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_2_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_3_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_4_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_5_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_6_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_7_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_8_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_9_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject' to 'ap_ctrl_hs'.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.98 seconds; current allocated memory: 369.147 MB.\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_10ns_26_2_0_MulnS_0'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_8s_24_2_0_MulnS_1'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_10s_26_2_0_MulnS_2'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_9ns_25_2_0_MulnS_3'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_7ns_23_2_0_MulnS_4'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_8ns_24_2_0_MulnS_5'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_9s_25_2_0_MulnS_6'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_11s_26_2_0_MulnS_7'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_6ns_22_2_0_MulnS_8'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_12s_26_2_0_MulnS_9'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_7s_23_2_0_MulnS_10'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_11ns_26_2_0_MulnS_11'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_5ns_21_2_0_MulnS_12'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_6s_22_2_0_MulnS_13'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_12ns_26_2_0_MulnS_14'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_16s_5s_21_2_0_MulnS_15'\n",
      "INFO: [RTMG 210-279] Implementing memory 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_exp_table1_rom' using auto ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_invert_table2_rom' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:02:54 ; elapsed = 00:03:09 . Memory (MB): peak = 7413.895 ; gain = 6976.023 ; free physical = 7185 ; free virtual = 8351\n",
      "INFO: [SYSC 207-301] Generating SystemC RTL for myproject.\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 0h3m8s *****\n",
      "INFO: [HLS 200-112] Total elapsed time: 189.1 seconds; peak allocated memory: 369.147 MB.\n",
      "INFO: [Common 17-206] Exiting vivado_hls at Mon Aug  7 16:11:39 2023...\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CSynthesisReport': {'TargetClockPeriod': '5.00',\n",
       "  'EstimatedClockPeriod': '4.105',\n",
       "  'BestLatency': '35',\n",
       "  'WorstLatency': '35',\n",
       "  'IntervalMin': '5',\n",
       "  'IntervalMax': '5',\n",
       "  'BRAM_18K': '2',\n",
       "  'DSP': '562',\n",
       "  'FF': '71103',\n",
       "  'LUT': '21024',\n",
       "  'AvailableBRAM_18K': '730',\n",
       "  'AvailableDSP': '740',\n",
       "  'AvailableFF': '269200',\n",
       "  'AvailableLUT': '129000'}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hls4ml\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "print(config)\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_Sparse_MNIST/hls4ml_prj', part='xc7a200tfbg676-2'\n",
    ")\n",
    "hls_model.compile()\n",
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the reports\n",
    "Print out the reports generated by Vivado HLS. Pay attention to the Utilization Estimates' section in particular this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in model_Sparse_MNIST/hls4ml_prj//myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "SYNTHESIS REPORT:\n",
      "================================================================\n",
      "== Vivado HLS Report for 'myproject'\n",
      "================================================================\n",
      "* Date:           Mon Aug  7 16:11:33 2023\n",
      "\n",
      "* Version:        2018.3 (Build 2405991 on Thu Dec 06 23:56:15 MST 2018)\n",
      "* Project:        myproject_prj\n",
      "* Solution:       solution1\n",
      "* Product family: artix7\n",
      "* Target device:  xc7a200tfbg676-2\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Performance Estimates\n",
      "================================================================\n",
      "+ Timing (ns): \n",
      "    * Summary: \n",
      "    +--------+-------+----------+------------+\n",
      "    |  Clock | Target| Estimated| Uncertainty|\n",
      "    +--------+-------+----------+------------+\n",
      "    |ap_clk  |   5.00|     4.105|        0.62|\n",
      "    +--------+-------+----------+------------+\n",
      "\n",
      "+ Latency (clock cycles): \n",
      "    * Summary: \n",
      "    +-----+-----+-----+-----+----------+\n",
      "    |  Latency  |  Interval | Pipeline |\n",
      "    | min | max | min | max |   Type   |\n",
      "    +-----+-----+-----+-----+----------+\n",
      "    |   35|   35|    5|    5| function |\n",
      "    +-----+-----+-----+-----+----------+\n",
      "\n",
      "    + Detail: \n",
      "        * Instance: \n",
      "        +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+-----+-----+-----+----------+\n",
      "        |                                                                                     |                                                                          |  Latency  |  Interval | Pipeline |\n",
      "        |                                       Instance                                      |                                  Module                                  | min | max | min | max |   Type   |\n",
      "        +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+-----+-----+-----+----------+\n",
      "        |grp_dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0_fu_130  |dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0  |    7|    7|    1|    1| function |\n",
      "        |grp_softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_fu_136             |softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s             |   26|   26|    5|    5| function |\n",
      "        +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+-----+-----+-----+----------+\n",
      "\n",
      "        * Loop: \n",
      "        N/A\n",
      "\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Utilization Estimates\n",
      "================================================================\n",
      "* Summary: \n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|       Name      | BRAM_18K| DSP48E|   FF   |   LUT  |\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|DSP              |        -|      -|       -|       -|\n",
      "|Expression       |        -|      -|       0|       6|\n",
      "|FIFO             |        -|      -|       -|       -|\n",
      "|Instance         |        2|    562|   66752|   20931|\n",
      "|Memory           |        -|      -|       -|       -|\n",
      "|Multiplexer      |        -|      -|       -|      87|\n",
      "|Register         |        -|      -|    4351|       -|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|Total            |        2|    562|   71103|   21024|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|Available        |      730|    740|  269200|  129000|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|Utilization (%)  |    ~0   |     75|      26|      16|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "\n",
      "+ Detail: \n",
      "    * Instance: \n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+-------+-------+\n",
      "    |                                       Instance                                      |                                  Module                                  | BRAM_18K| DSP48E|   FF  |  LUT  |\n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+-------+-------+\n",
      "    |grp_dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0_fu_130  |dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0  |        0|    552|  64283|  18141|\n",
      "    |grp_softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_fu_136             |softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s             |        2|     10|   2469|   2790|\n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+-------+-------+\n",
      "    |Total                                                                                |                                                                          |        2|    562|  66752|  20931|\n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+-------+-------+\n",
      "\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('model_Sparse_MNIST/hls4ml_prj/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report for the model trained in part 1. Remember these models have the same architecture, but the model in this section was trained using the sparsity API from tensorflow_model_optimization. Notice how the resource usage had dramatically reduced (particularly the DSPs). When Vivado HLS notices an operation like `y = 0 * x` it can avoid placing a DSP for that operation. The impact of this is biggest when `ReuseFactor = 1`, but still applies at higher reuse as well. **Note you need to have trained and synthesized the model from part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare it with Unsparsed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: dense_49_input, layer type: InputLayer, input shapes: [[None, 261]], output shape: [None, 261]\n",
      "Layer name: dense_49, layer type: Dense, input shapes: [[None, 261]], output shape: [None, 10]\n",
      "Layer name: activation_47, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "{'Model': {'Precision': 'fixed<16,6>', 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False}}\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: dense_49_input, layer type: InputLayer, input shapes: [[None, 261]], output shape: [None, 261]\n",
      "Layer name: dense_49, layer type: Dense, input shapes: [[None, 261]], output shape: [None, 10]\n",
      "Layer name: activation_47, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2018.3 (64-bit)\n",
      "  **** SW Build 2405991 on Thu Dec  6 23:36:41 MST 2018\n",
      "  **** IP Build 2404404 on Fri Dec  7 01:43:56 MST 2018\n",
      "    ** Copyright 1986-2018 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /home/rrk307/Xilinx/Vivado/2018.3/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/home/rrk307/Xilinx/Vivado/2018.3/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "INFO: [HLS 200-10] For user 'rrk307' on host 'ADUAED10599LPLX' (Linux_x86_64 version 5.4.0-150-generic) on Mon Aug 07 16:14:46 +04 2023\n",
      "INFO: [HLS 200-10] On os Ubuntu 18.04.6 LTS\n",
      "INFO: [HLS 200-10] In directory '/home/rrk307/model_UnSparse_MNIST/hls4ml_prj'\n",
      "INFO: [HLS 200-10] Creating and opening project '/home/rrk307/model_UnSparse_MNIST/hls4ml_prj/myproject_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Creating and opening solution '/home/rrk307/model_UnSparse_MNIST/hls4ml_prj/myproject_prj/solution1'.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 80.\n",
      "INFO: [HLS 200-10] Setting target device to 'xc7a200tfbg676-2'\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.625ns.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... \n",
      "INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:18 ; elapsed = 00:00:19 . Memory (MB): peak = 437.957 ; gain = 0.078 ; free physical = 10259 ; free virtual = 11452\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:18 ; elapsed = 00:00:19 . Memory (MB): peak = 437.957 ; gain = 0.078 ; free physical = 10259 ; free virtual = 11452\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:42).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' into 'myproject' (firmware/myproject.cpp:35).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::softmax<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' into 'myproject' (firmware/myproject.cpp:37).\n",
      "INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:00:20 ; elapsed = 00:00:21 . Memory (MB): peak = 567.020 ; gain = 129.141 ; free physical = 10200 ; free virtual = 11414\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:66) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:239) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:254) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:262) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:265) automatically.\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:00:20 ; elapsed = 00:00:21 . Memory (MB): peak = 567.020 ; gain = 129.141 ; free physical = 10186 ; free virtual = 11402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:217:46).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:17:48).\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:243) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-2' (firmware/nnet_utils/nnet_activation.h:252) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-3' (firmware/nnet_utils/nnet_activation.h:266) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:37) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 261.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:40) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:48) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:54) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 261.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:56) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:64) in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' completely with a factor of 10.\n",
      "INFO: [XFORM 203-102] Partitioning array 'd_xi_xmax.V' (firmware/nnet_utils/nnet_activation.h:242) automatically.\n",
      "INFO: [XFORM 203-131] Reshaping array 'dense_49_input.V' (firmware/myproject.cpp:7) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer4_out.V' (firmware/myproject.cpp:8) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer2_out.V' (firmware/myproject.cpp:33) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b2.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:17) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:18) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'exp_res.V' (firmware/nnet_utils/nnet_activation.h:249) in dimension 1 completely.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:66) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 4, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 2, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::operator()' into 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:43) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10, nnet::Op_max<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:239) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:254) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config4>' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:265) automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0' by setting 'weights.V' to 'w2.V'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0' by setting 'biases[0].V' to 'b2.V.0'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0' by setting 'biases[1].V' to 'b2.V.1'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0' by setting 'biases[2].V' to 'b2.V.2'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0' by setting 'biases[3].V' to 'b2.V.3'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0' by setting 'biases[4].V' to 'b2.V.4'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0' by setting 'biases[5].V' to 'b2.V.5'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0' by setting 'biases[6].V' to 'b2.V.6'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0' by setting 'biases[7].V' to 'b2.V.7'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0' by setting 'biases[8].V' to 'b2.V.8'.\n",
      "INFO: [XFORM 203-622] Instantiating function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0'(firmware/nnet_utils/nnet_dense_latency.h:26:27) to 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0.0' by setting 'biases[9].V' to 'b2.V.9'.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation.h:249) to (firmware/nnet_utils/nnet_activation.h:270:1) in function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>'... converting 41 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:45:44) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:43:16) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:45:44) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 13 basic blocks.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:45) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' (firmware/nnet_utils/nnet_activation.h:262) automatically.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0.0' (firmware/nnet_utils/nnet_dense_latency.h:26:27)...2603 expression(s) balanced.\n",
      "INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:02:22 ; elapsed = 00:02:26 . Memory (MB): peak = 7413.902 ; gain = 6976.023 ; free physical = 7130 ; free virtual = 8333\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::softmax_stable<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, softmax_config4>' to 'softmax_stable<ap_fixed,ap_fixed<16,6,5,3,0>,softmax_config4>' (firmware/nnet_utils/nnet_activation.h:45:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' (firmware/nnet_utils/nnet_common.h:36:44)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2>.0.0.0.0.0.0.0.0.0.0.0' to 'dense_latency<ap_fixed,ap_fixed<16,6,5,3,0>,config2>.0.0.0.0.0.0.0.0.0.0.0' (firmware/nnet_utils/nnet_mult.h:26:9)\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:03:07 ; elapsed = 00:03:11 . Memory (MB): peak = 7413.902 ; gain = 6976.023 ; free physical = 7127 ; free virtual = 8332\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_latency<ap_fixed,ap_fixed<16,6,5,3,0>,config2>.0.0.0.0.0.0.0.0.0.0.0' to 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' to 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'softmax_stable<ap_fixed,ap_fixed<16,6,5,3,0>,softmax_config4>' to 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [SCHED 204-61] Pipelining function 'dense_latency<ap_fixed,ap_fixed<16,6,5,3,0>,config2>.0.0.0.0.0.0.0.0.0.0.0'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 9.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 249.05 seconds; current allocated memory: 350.176 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 76.9 seconds; current allocated memory: 500.067 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 33.48 seconds; current allocated memory: 507.347 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.17 seconds; current allocated memory: 507.652 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'softmax_stable<ap_fixed,ap_fixed<16,6,5,3,0>,softmax_config4>'.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "WARNING: [SCHED 204-68] The II Violation in module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s': Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 1)\n",
      "   between 'call' operation ('__Val2__', firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_common.h:45->firmware/nnet_utils/nnet_activation.h:262) to 'reduce<ap_fixed<18, 8, 0, 0, 0>, 4, Op_add<ap_fixed<18, 8, 0, 0, 0> > >' and 'store' operation (firmware/nnet_utils/nnet_activation.h:255) of variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:255 on local variable 'exp_res[0].V', firmware/nnet_utils/nnet_activation.h:249.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 5, Depth = 27.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.39 seconds; current allocated memory: 508.784 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.73 seconds; current allocated memory: 510.121 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'myproject'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 5, Depth = 37.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.94 seconds; current allocated memory: 510.585 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 6.29 seconds; current allocated memory: 514.378 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-104] Estimated max fanout for 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0' is 76960 from HDL expression: ((1'b1 == ap_ce_reg) & (1'b0 == ap_block_pp0_stage0_11001))\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_10ns_16s_26_2_0': 150 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_10s_16s_26_2_0': 225 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_11ns_16s_26_2_0': 21 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_11s_16s_26_2_0': 135 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_12ns_16s_26_2_0': 2 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_12s_16s_26_2_0': 120 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_13ns_16s_26_2_0': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_5ns_16s_21_2_0': 10 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_5s_16s_21_2_0': 9 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_6ns_16s_22_2_0': 36 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_6s_16s_22_2_0': 34 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_7ns_16s_23_2_0': 124 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_7s_16s_23_2_0': 113 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_8ns_16s_24_2_0': 261 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_8s_16s_24_2_0': 214 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_9ns_16s_25_2_0': 338 instance(s).\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_9s_16s_25_2_0': 264 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 12.13 seconds; current allocated memory: 627.332 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mux_104_18_1_0': 4 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'reduce_ap_fixed_18_8_0_0_0_4_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 63.21 seconds; current allocated memory: 842.906 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_mul_mul_17ns_18s_26_3_1': 10 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.87 seconds; current allocated memory: 846.890 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/dense_49_input_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_0_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_1_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_2_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_3_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_4_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_5_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_6_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_7_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_8_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject/layer4_out_9_V' to 'ap_vld'.\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject' to 'ap_ctrl_hs'.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.18 seconds; current allocated memory: 852.858 MB.\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_10s_16s_26_2_0_MulnS_0'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_9s_16s_25_2_0_MulnS_1'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_9ns_16s_25_2_0_MulnS_2'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_8s_16s_24_2_0_MulnS_3'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_11s_16s_26_2_0_MulnS_4'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_7ns_16s_23_2_0_MulnS_5'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_7s_16s_23_2_0_MulnS_6'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_10ns_16s_26_2_0_MulnS_7'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_8ns_16s_24_2_0_MulnS_8'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_12s_16s_26_2_0_MulnS_9'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_6s_16s_22_2_0_MulnS_10'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_11ns_16s_26_2_0_MulnS_11'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_5s_16s_21_2_0_MulnS_12'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_5ns_16s_21_2_0_MulnS_13'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_6ns_16s_22_2_0_MulnS_14'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_12ns_16s_26_2_0_MulnS_15'\n",
      "INFO: [RTMG 210-282] Generating pipelined core: 'myproject_mul_13ns_16s_26_2_0_MulnS_16'\n",
      "INFO: [RTMG 210-279] Implementing memory 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_exp_table1_rom' using auto ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_invert_table2_rom' using auto ROMs.\n",
      "INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:07:08 ; elapsed = 00:07:49 . Memory (MB): peak = 7413.902 ; gain = 6976.023 ; free physical = 6654 ; free virtual = 8109\n",
      "INFO: [SYSC 207-301] Generating SystemC RTL for myproject.\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 0h7m48s *****\n",
      "INFO: [HLS 200-112] Total elapsed time: 469.19 seconds; peak allocated memory: 852.858 MB.\n",
      "INFO: [Common 17-206] Exiting vivado_hls at Mon Aug  7 16:22:35 2023...\n",
      "Vivado synthesis report not found.\n",
      "Cosim report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CSynthesisReport': {'TargetClockPeriod': '5.00',\n",
       "  'EstimatedClockPeriod': '4.105',\n",
       "  'BestLatency': '36',\n",
       "  'WorstLatency': '36',\n",
       "  'IntervalMin': '5',\n",
       "  'IntervalMax': '5',\n",
       "  'BRAM_18K': '2',\n",
       "  'DSP': '2067',\n",
       "  'FF': '228552',\n",
       "  'LUT': '71763',\n",
       "  'AvailableBRAM_18K': '730',\n",
       "  'AvailableDSP': '740',\n",
       "  'AvailableFF': '269200',\n",
       "  'AvailableLUT': '129000'}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_keras_model(Unsparsemodel, granularity='model')\n",
    "print(config)\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    Unsparsemodel, hls_config=config, output_dir='model_UnSparse_MNIST/hls4ml_prj', part='xc7a200tfbg676-2'\n",
    ")\n",
    "hls_model.compile()\n",
    "hls_model.build(csim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in model_UnSparse_MNIST/hls4ml_prj//myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "SYNTHESIS REPORT:\n",
      "================================================================\n",
      "== Vivado HLS Report for 'myproject'\n",
      "================================================================\n",
      "* Date:           Mon Aug  7 16:22:22 2023\n",
      "\n",
      "* Version:        2018.3 (Build 2405991 on Thu Dec 06 23:56:15 MST 2018)\n",
      "* Project:        myproject_prj\n",
      "* Solution:       solution1\n",
      "* Product family: artix7\n",
      "* Target device:  xc7a200tfbg676-2\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Performance Estimates\n",
      "================================================================\n",
      "+ Timing (ns): \n",
      "    * Summary: \n",
      "    +--------+-------+----------+------------+\n",
      "    |  Clock | Target| Estimated| Uncertainty|\n",
      "    +--------+-------+----------+------------+\n",
      "    |ap_clk  |   5.00|     4.105|        0.62|\n",
      "    +--------+-------+----------+------------+\n",
      "\n",
      "+ Latency (clock cycles): \n",
      "    * Summary: \n",
      "    +-----+-----+-----+-----+----------+\n",
      "    |  Latency  |  Interval | Pipeline |\n",
      "    | min | max | min | max |   Type   |\n",
      "    +-----+-----+-----+-----+----------+\n",
      "    |   36|   36|    5|    5| function |\n",
      "    +-----+-----+-----+-----+----------+\n",
      "\n",
      "    + Detail: \n",
      "        * Instance: \n",
      "        +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+-----+-----+-----+----------+\n",
      "        |                                                                                     |                                                                          |  Latency  |  Interval | Pipeline |\n",
      "        |                                       Instance                                      |                                  Module                                  | min | max | min | max |   Type   |\n",
      "        +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+-----+-----+-----+----------+\n",
      "        |grp_dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0_fu_130  |dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0  |    8|    8|    1|    1| function |\n",
      "        |grp_softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_fu_136             |softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s             |   26|   26|    5|    5| function |\n",
      "        +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+-----+-----+-----+-----+----------+\n",
      "\n",
      "        * Loop: \n",
      "        N/A\n",
      "\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Utilization Estimates\n",
      "================================================================\n",
      "* Summary: \n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|       Name      | BRAM_18K| DSP48E|   FF   |   LUT  |\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|DSP              |        -|      -|       -|       -|\n",
      "|Expression       |        -|      -|       0|       6|\n",
      "|FIFO             |        -|      -|       -|       -|\n",
      "|Instance         |        2|   2067|  224201|   71670|\n",
      "|Memory           |        -|      -|       -|       -|\n",
      "|Multiplexer      |        -|      -|       -|      87|\n",
      "|Register         |        -|      -|    4351|       -|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|Total            |        2|   2067|  228552|   71763|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|Available        |      730|    740|  269200|  129000|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "|Utilization (%)  |    ~0   |    279|      84|      55|\n",
      "+-----------------+---------+-------+--------+--------+\n",
      "\n",
      "+ Detail: \n",
      "    * Instance: \n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+--------+-------+\n",
      "    |                                       Instance                                      |                                  Module                                  | BRAM_18K| DSP48E|   FF   |  LUT  |\n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+--------+-------+\n",
      "    |grp_dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0_fu_130  |dense_latency_ap_fixed_ap_fixed_16_6_5_3_0_config2_0_0_0_0_0_0_0_0_0_0_0  |        0|   2057|  221732|  68880|\n",
      "    |grp_softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s_fu_136             |softmax_stable_ap_fixed_ap_fixed_16_6_5_3_0_softmax_config4_s             |        2|     10|    2469|   2790|\n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+--------+-------+\n",
      "    |Total                                                                                |                                                                          |        2|   2067|  224201|  71670|\n",
      "    +-------------------------------------------------------------------------------------+--------------------------------------------------------------------------+---------+-------+--------+-------+\n",
      "\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('model_UnSparse_MNIST/hls4ml_prj/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}